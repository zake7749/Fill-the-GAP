{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission_stage_1.csv', 'test_stage_1.tsv', 'trees']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"data\"))\n",
    "import zipfile\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from keras import backend, models, layers, initializers, regularizers, constraints, optimizers\n",
    "from keras import callbacks as kc\n",
    "from keras import optimizers as ko\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the OOFs and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oof_filenames = os.listdir('oof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bnli-mh-bert-base-uncased-seq512-8.csv',\n",
       " 'bnli-mh-bert-large-cased-seq300-18.csv',\n",
       " 'bnli-mh-bert-large-uncased-seq300-19.csv',\n",
       " 'HBnli-mh-bert-base-uncased-seq512-8.csv',\n",
       " 'HBnli-mh-bert-large-cased-seq300-18.csv',\n",
       " 'HBnli-mh-bert-large-uncased-seq300-19.csv',\n",
       " 'Hnaive-bert-base-uncased-seq512-8.csv',\n",
       " 'Hnaive-bert-large-cased-seq300-18.csv',\n",
       " 'Hnaive-bert-large-uncased-seq300-19.csv',\n",
       " 'Hnli-mh-bert-base-uncased-seq512-8.csv',\n",
       " 'Hnli-mh-bert-large-cased-seq300-18.csv',\n",
       " 'Hnli-mh-bert-large-uncased-seq300-19.csv',\n",
       " 'naive-bert-base-uncased-seq512-8.csv',\n",
       " 'naive-bert-large-cased-seq300-18.csv',\n",
       " 'naive-bert-large-uncased-seq300-19.csv',\n",
       " 'nli-mh-bert-base-uncased-seq512-8.csv',\n",
       " 'nli-mh-bert-large-cased-seq300-18.csv',\n",
       " 'nli-mh-bert-large-uncased-seq300-19.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v for v in oof_filenames if 'bert-base-cased' not in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_filenames = os.listdir('outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bnli-mh-bert-base-cased-seq512-8.csv',\n",
       " 'bnli-mh-bert-base-uncased-seq512-8.csv',\n",
       " 'bnli-mh-bert-large-cased-seq300-18.csv',\n",
       " 'bnli-mh-bert-large-uncased-seq300-19.csv',\n",
       " 'HBnli-mh-bert-base-cased-seq512-8.csv',\n",
       " 'HBnli-mh-bert-base-uncased-seq512-8.csv',\n",
       " 'HBnli-mh-bert-large-cased-seq300-18.csv',\n",
       " 'HBnli-mh-bert-large-uncased-seq300-19.csv',\n",
       " 'Hnaive-bert-base-cased-seq512-8.csv',\n",
       " 'Hnaive-bert-base-uncased-seq512-8.csv',\n",
       " 'Hnaive-bert-large-cased-seq300-18.csv',\n",
       " 'Hnaive-bert-large-uncased-seq300-19.csv',\n",
       " 'Hnli-mh-bert-base-cased-seq512-8.csv',\n",
       " 'Hnli-mh-bert-base-uncased-seq512-8.csv',\n",
       " 'Hnli-mh-bert-large-cased-seq300-18.csv',\n",
       " 'Hnli-mh-bert-large-uncased-seq300-19.csv',\n",
       " 'naive-bert-base-cased-seq512-8.csv',\n",
       " 'naive-bert-base-uncased-seq512-8.csv',\n",
       " 'naive-bert-large-cased-seq300-18.csv',\n",
       " 'naive-bert-large-uncased-seq300-19.csv',\n",
       " 'nli-mh-bert-base-cased-seq512-8.csv',\n",
       " 'nli-mh-bert-base-uncased-seq512-8.csv',\n",
       " 'nli-mh-bert-large-cased-seq300-18.csv',\n",
       " 'nli-mh-bert-large-uncased-seq300-19.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tag(oof_name):\n",
    "    if oof_name.startswith('naive'):\n",
    "        return '-'.join(oof_name.split('-')[1:4])\n",
    "    elif oof_name.startswith('nli'):\n",
    "        return '-'.join(oof_name.split('-')[2:5])\n",
    "    else:\n",
    "        pritn(\"[WTF?]\", oof_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oofs = [pd.read_csv(os.path.join('oof', v)).values for v in oof_filenames]\n",
    "preds = [pd.read_csv(os.path.join('outputs', v)).values[:, 1:] for v in pred_filenames]\n",
    "\n",
    "dev_filename = \"contextual_embeddings_gap_development.json\"\n",
    "val_filename = \"contextual_embeddings_gap_validation.json\"\n",
    "test_filename = \"contextual_embeddings_gap_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_test = []\n",
    "remove_validation = []\n",
    "remove_development = [209, 1506, 1988]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_labels():\n",
    "    dev = pd.read_csv('gap-development.tsv', sep = '\\t')\n",
    "    test =  pd.read_csv('gap-test.tsv', sep = '\\t')\n",
    "    validation =  pd.read_csv('gap-validation.tsv', sep = '\\t')\n",
    "    \n",
    "    def _get_labels(df):\n",
    "        labels = df[['A-coref', 'B-coref']].values\n",
    "        ys = []\n",
    "        for label in labels:\n",
    "            y = np.zeros(3)\n",
    "            if label[0]:\n",
    "                y[0] = 1\n",
    "            elif label[1]:\n",
    "                y[1] = 1\n",
    "            else:\n",
    "                y[2] = 1\n",
    "            ys.append(y)\n",
    "        return np.array(ys)\n",
    "        \n",
    "    dev_labels, test_labels, valid_labels = _get_labels(dev), _get_labels(test), _get_labels(validation)  \n",
    "    #dev_labels = np.delete(dev_labels, remove_development, 0) Should be change in stage 2\n",
    "\n",
    "    test_labels = np.delete(test_labels, remove_test, 0)\n",
    "    valid_labels = np.delete(valid_labels, remove_validation, 0)\n",
    "    print(\"Shape of test, valid, dev samples\", test_labels.shape, valid_labels.shape, dev_labels.shape)\n",
    "    return test_labels, valid_labels, dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test, valid, dev samples (2000, 3) (454, 3) (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "test_labels, valid_labels, dev_labels = get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n",
      "(2454, 3)\n"
     ]
    }
   ],
   "source": [
    "for oof in oofs:\n",
    "    print(oof.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "for pred in preds:\n",
    "    print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (2454, 72) train_y shape: (2454, 3)\n",
      "train_x shape: (2000, 72) train_y shape: (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "train_x = np.concatenate(oofs, axis=1)\n",
    "test_x = np.concatenate(preds, axis=1)\n",
    "train_y = np.concatenate([test_labels, valid_labels], axis=0)\n",
    "test_y = dev_labels\n",
    "\n",
    "print(\"train_x shape:\", train_x.shape, \"train_y shape:\", train_y.shape)\n",
    "print(\"train_x shape:\", test_x.shape, \"train_y shape:\", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math \n",
    "import keras.backend as K\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + K.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * K.pow(x, 3))))\n",
    "class Gelu(Layer):\n",
    "    def __init__(self, accurate: bool = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.accurate = accurate\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sigmoid(inputs)\n",
    "        if not self.accurate:\n",
    "            return gelu(inputs)\n",
    "        if K.backend() == 'tensorflow':\n",
    "            erf = K.tf.erf\n",
    "        else:\n",
    "            erf = K.T.erf\n",
    "        return inputs * 0.5 * (1.0 + erf(inputs / math.sqrt(2.0)))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'accurate': self.accurate,\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp_model(feature_nums):\n",
    "    features_inputs = Input(shape=feature_nums, name='mata-features', dtype=\"float32\")\n",
    "    features = features_inputs\n",
    "    \n",
    "    depth = 5\n",
    "    for i in range(depth):\n",
    "        new_features = Dense(24, activation='relu')(features)\n",
    "        new_features = Dropout(0.5)(new_features)\n",
    "        features = Concatenate()([features, new_features])\n",
    "\n",
    "    out_ = Dense(3, activation='softmax')(features)\n",
    "    \n",
    "    model = Model(inputs=[features_inputs], outputs=out_)\n",
    "    model.compile(optimizer=Adam(lr=1e-3, decay=1e-6,), loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def build_logistic_regression(feature_nums):\n",
    "    features_inputs = Input(shape=feature_nums, name='mata-features', dtype=\"float32\")\n",
    "    out_ = Dense(3, activation='softmax')(features_inputs)\n",
    "    model = Model(inputs=[features_inputs], outputs=out_)\n",
    "    model.compile(optimizer=Adam(lr=1e-3, decay=1e-6,), loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 24)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 24)           2328        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 24)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 120)          0           concatenate_1[0][0]              \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 24)           2904        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 24)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 144)          0           concatenate_2[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 24)           3480        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 24)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 168)          0           concatenate_3[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 24)           4056        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 24)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 192)          0           concatenate_4[0][0]              \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            579         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1963 samples, validate on 491 samples\n",
      "Epoch 1/200\n",
      "1963/1963 [==============================] - 2s 882us/step - loss: 0.4523 - val_loss: 0.3035\n",
      "Epoch 2/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.3450 - val_loss: 0.3027\n",
      "Epoch 3/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.3352 - val_loss: 0.2944\n",
      "Epoch 4/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.3279 - val_loss: 0.2906\n",
      "Epoch 5/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.3166 - val_loss: 0.2908\n",
      "Epoch 6/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.3126 - val_loss: 0.2920\n",
      "Epoch 7/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.3174 - val_loss: 0.2885\n",
      "Epoch 8/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.3089 - val_loss: 0.2893\n",
      "Epoch 9/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.3075 - val_loss: 0.2840\n",
      "Epoch 10/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.3065 - val_loss: 0.2898\n",
      "Epoch 11/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.3004 - val_loss: 0.2857\n",
      "Epoch 12/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2952 - val_loss: 0.2911\n",
      "Epoch 13/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2966 - val_loss: 0.2864\n",
      "Epoch 14/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2981 - val_loss: 0.2892\n",
      "Epoch 15/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2941 - val_loss: 0.2863\n",
      "Epoch 16/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2931 - val_loss: 0.2865\n",
      "Epoch 17/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2915 - val_loss: 0.2879\n",
      "Epoch 18/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2930 - val_loss: 0.2856\n",
      "Epoch 19/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2832 - val_loss: 0.2858\n",
      "Epoch 20/200\n",
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.2869 - val_loss: 0.2853\n",
      "Epoch 21/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2880 - val_loss: 0.2854\n",
      "Epoch 22/200\n",
      "1963/1963 [==============================] - 0s 158us/step - loss: 0.2873 - val_loss: 0.2862\n",
      "Epoch 23/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2811 - val_loss: 0.2868\n",
      "Epoch 24/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2817 - val_loss: 0.2865\n",
      "Epoch 25/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2826 - val_loss: 0.2863\n",
      "Epoch 26/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2806 - val_loss: 0.2861\n",
      "Epoch 27/200\n",
      "1963/1963 [==============================] - 0s 166us/step - loss: 0.2801 - val_loss: 0.2858\n",
      "Epoch 28/200\n",
      "1963/1963 [==============================] - 0s 158us/step - loss: 0.2806 - val_loss: 0.2864\n",
      "Epoch 29/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2817 - val_loss: 0.2862\n",
      "Epoch 30/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2772 - val_loss: 0.2864\n",
      "Epoch 31/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2809 - val_loss: 0.2868\n",
      "Epoch 32/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2779 - val_loss: 0.2869\n",
      "Epoch 33/200\n",
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.2747 - val_loss: 0.2874\n",
      "Epoch 34/200\n",
      "1963/1963 [==============================] - 0s 168us/step - loss: 0.2826 - val_loss: 0.2868\n",
      "Epoch 35/200\n",
      "1963/1963 [==============================] - 0s 164us/step - loss: 0.2794 - val_loss: 0.2868\n",
      "Epoch 36/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2740 - val_loss: 0.2871\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2800 - val_loss: 0.2868\n",
      "Epoch 38/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2848 - val_loss: 0.2871\n",
      "Epoch 39/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2759 - val_loss: 0.2871\n",
      "Epoch 40/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2804 - val_loss: 0.2873\n",
      "Epoch 41/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2819 - val_loss: 0.2872\n",
      "Epoch 42/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2761 - val_loss: 0.2872\n",
      "Epoch 43/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2748 - val_loss: 0.2871\n",
      "Epoch 44/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2808 - val_loss: 0.2873\n",
      "Epoch 45/200\n",
      "1963/1963 [==============================] - 0s 166us/step - loss: 0.2772 - val_loss: 0.2870\n",
      "Epoch 46/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2779 - val_loss: 0.2871\n",
      "Epoch 47/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2809 - val_loss: 0.2870\n",
      "Epoch 48/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2740 - val_loss: 0.2869\n",
      "Epoch 49/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2827 - val_loss: 0.2868\n",
      "Epoch 50/200\n",
      "1963/1963 [==============================] - 0s 158us/step - loss: 0.2807 - val_loss: 0.2867\n",
      "Epoch 51/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2756 - val_loss: 0.2866\n",
      "Epoch 52/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2786 - val_loss: 0.2867\n",
      "Epoch 53/200\n",
      "1963/1963 [==============================] - 0s 166us/step - loss: 0.2756 - val_loss: 0.2868\n",
      "Epoch 54/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2768 - val_loss: 0.2869\n",
      "Epoch 55/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2791 - val_loss: 0.2869\n",
      "Epoch 56/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2814 - val_loss: 0.2865\n",
      "Epoch 57/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2796 - val_loss: 0.2868\n",
      "Epoch 58/200\n",
      "1963/1963 [==============================] - 0s 158us/step - loss: 0.2795 - val_loss: 0.2866\n",
      "Epoch 59/200\n",
      "1963/1963 [==============================] - 0s 158us/step - loss: 0.2748 - val_loss: 0.2867\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 24)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 24)           2328        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 24)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 120)          0           concatenate_6[0][0]              \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 24)           2904        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 24)           0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 144)          0           concatenate_7[0][0]              \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 24)           3480        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 24)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 168)          0           concatenate_8[0][0]              \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 24)           4056        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 24)           0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 192)          0           concatenate_9[0][0]              \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 3)            579         concatenate_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1963 samples, validate on 491 samples\n",
      "Epoch 1/200\n",
      "1963/1963 [==============================] - 1s 374us/step - loss: 0.4577 - val_loss: 0.3014\n",
      "Epoch 2/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.3502 - val_loss: 0.2990\n",
      "Epoch 3/200\n",
      "1963/1963 [==============================] - 0s 168us/step - loss: 0.3350 - val_loss: 0.2915\n",
      "Epoch 4/200\n",
      "1963/1963 [==============================] - 0s 166us/step - loss: 0.3220 - val_loss: 0.2889\n",
      "Epoch 5/200\n",
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.3240 - val_loss: 0.2879\n",
      "Epoch 6/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.3222 - val_loss: 0.2828\n",
      "Epoch 7/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.3169 - val_loss: 0.2839\n",
      "Epoch 8/200\n",
      "1963/1963 [==============================] - 0s 168us/step - loss: 0.3181 - val_loss: 0.2847\n",
      "Epoch 9/200\n",
      "1963/1963 [==============================] - 0s 168us/step - loss: 0.3089 - val_loss: 0.2773\n",
      "Epoch 10/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.3083 - val_loss: 0.2821\n",
      "Epoch 11/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.3069 - val_loss: 0.2894\n",
      "Epoch 12/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.3069 - val_loss: 0.2761\n",
      "Epoch 13/200\n",
      "1963/1963 [==============================] - 0s 166us/step - loss: 0.3031 - val_loss: 0.2800\n",
      "Epoch 14/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.3021 - val_loss: 0.2778\n",
      "Epoch 15/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2980 - val_loss: 0.2787\n",
      "Epoch 16/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.3054 - val_loss: 0.2794\n",
      "Epoch 17/200\n",
      "1963/1963 [==============================] - 0s 164us/step - loss: 0.2993 - val_loss: 0.2754\n",
      "Epoch 18/200\n",
      "1963/1963 [==============================] - 0s 164us/step - loss: 0.3010 - val_loss: 0.2802\n",
      "Epoch 19/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2927 - val_loss: 0.2732\n",
      "Epoch 20/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2934 - val_loss: 0.2815\n",
      "Epoch 21/200\n",
      "1963/1963 [==============================] - 0s 166us/step - loss: 0.2981 - val_loss: 0.2753\n",
      "Epoch 22/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2949 - val_loss: 0.2694\n",
      "Epoch 23/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2914 - val_loss: 0.2722\n",
      "Epoch 24/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2920 - val_loss: 0.2739\n",
      "Epoch 25/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2911 - val_loss: 0.2728\n",
      "Epoch 26/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2875 - val_loss: 0.2865\n",
      "Epoch 27/200\n",
      "1963/1963 [==============================] - 0s 168us/step - loss: 0.2886 - val_loss: 0.2730\n",
      "Epoch 28/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2854 - val_loss: 0.2702\n",
      "Epoch 29/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2851 - val_loss: 0.2722\n",
      "Epoch 30/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2803 - val_loss: 0.2720\n",
      "Epoch 31/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2795 - val_loss: 0.2690\n",
      "Epoch 32/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2783 - val_loss: 0.2732\n",
      "Epoch 33/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2789 - val_loss: 0.2676\n",
      "Epoch 34/200\n",
      "1963/1963 [==============================] - 0s 164us/step - loss: 0.2800 - val_loss: 0.2757\n",
      "Epoch 35/200\n",
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.2800 - val_loss: 0.2718\n",
      "Epoch 36/200\n",
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.2802 - val_loss: 0.2722\n",
      "Epoch 37/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2755 - val_loss: 0.2721\n",
      "Epoch 38/200\n",
      "1963/1963 [==============================] - 0s 168us/step - loss: 0.2721 - val_loss: 0.2777\n",
      "Epoch 39/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2746 - val_loss: 0.2729\n",
      "Epoch 40/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2699 - val_loss: 0.2740\n",
      "Epoch 41/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2694 - val_loss: 0.2746\n",
      "Epoch 42/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2731 - val_loss: 0.2729\n",
      "Epoch 43/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2755 - val_loss: 0.2717\n",
      "Epoch 44/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2688 - val_loss: 0.2722\n",
      "Epoch 45/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2736 - val_loss: 0.2726\n",
      "Epoch 46/200\n",
      "1963/1963 [==============================] - 0s 148us/step - loss: 0.2691 - val_loss: 0.2732\n",
      "Epoch 47/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2708 - val_loss: 0.2726\n",
      "Epoch 48/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2754 - val_loss: 0.2721\n",
      "Epoch 49/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2671 - val_loss: 0.2721\n",
      "Epoch 50/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2721 - val_loss: 0.2721\n",
      "Epoch 51/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2706 - val_loss: 0.2725\n",
      "Epoch 52/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2701 - val_loss: 0.2724\n",
      "Epoch 53/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2723 - val_loss: 0.2724\n",
      "Epoch 54/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2683 - val_loss: 0.2728\n",
      "Epoch 55/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2688 - val_loss: 0.2731\n",
      "Epoch 56/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2716 - val_loss: 0.2727\n",
      "Epoch 57/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2712 - val_loss: 0.2727\n",
      "Epoch 58/200\n",
      "1963/1963 [==============================] - 0s 148us/step - loss: 0.2736 - val_loss: 0.2727\n",
      "Epoch 59/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2673 - val_loss: 0.2726\n",
      "Epoch 60/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2696 - val_loss: 0.2727\n",
      "Epoch 61/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2687 - val_loss: 0.2726\n",
      "Epoch 62/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2662 - val_loss: 0.2728\n",
      "Epoch 63/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2672 - val_loss: 0.2730\n",
      "Epoch 64/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2652 - val_loss: 0.2734\n",
      "Epoch 65/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2692 - val_loss: 0.2733\n",
      "Epoch 66/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2640 - val_loss: 0.2736\n",
      "Epoch 67/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2644 - val_loss: 0.2734\n",
      "Epoch 68/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2633 - val_loss: 0.2736\n",
      "Epoch 69/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2697 - val_loss: 0.2739\n",
      "Epoch 70/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2643 - val_loss: 0.2741\n",
      "Epoch 71/200\n",
      "1963/1963 [==============================] - 0s 148us/step - loss: 0.2660 - val_loss: 0.2740\n",
      "Epoch 72/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2684 - val_loss: 0.2737\n",
      "Epoch 73/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2658 - val_loss: 0.2735\n",
      "Epoch 74/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2676 - val_loss: 0.2738\n",
      "Epoch 75/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2691 - val_loss: 0.2741\n",
      "Epoch 76/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2649 - val_loss: 0.2742\n",
      "Epoch 77/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2711 - val_loss: 0.2742\n",
      "Epoch 78/200\n",
      "1963/1963 [==============================] - 0s 148us/step - loss: 0.2676 - val_loss: 0.2739\n",
      "Epoch 79/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2626 - val_loss: 0.2740\n",
      "Epoch 80/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2702 - val_loss: 0.2738\n",
      "Epoch 81/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2621 - val_loss: 0.2739\n",
      "Epoch 82/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2625 - val_loss: 0.2740\n",
      "Epoch 83/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2649 - val_loss: 0.2736\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 24)           0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 24)           2328        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 24)           0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 120)          0           concatenate_11[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 24)           2904        concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 24)           0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 144)          0           concatenate_12[0][0]             \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 24)           3480        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 24)           0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 168)          0           concatenate_13[0][0]             \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 24)           4056        concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 24)           0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 192)          0           concatenate_14[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 3)            579         concatenate_15[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1963 samples, validate on 491 samples\n",
      "Epoch 1/200\n",
      "1963/1963 [==============================] - 1s 387us/step - loss: 0.4346 - val_loss: 0.3455\n",
      "Epoch 2/200\n",
      "1963/1963 [==============================] - 0s 158us/step - loss: 0.3407 - val_loss: 0.3390\n",
      "Epoch 3/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.3218 - val_loss: 0.3344\n",
      "Epoch 4/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.3193 - val_loss: 0.3308\n",
      "Epoch 5/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.3210 - val_loss: 0.3297\n",
      "Epoch 6/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.3016 - val_loss: 0.3265\n",
      "Epoch 7/200\n",
      "1963/1963 [==============================] - 0s 158us/step - loss: 0.3089 - val_loss: 0.3246\n",
      "Epoch 8/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.3016 - val_loss: 0.3244\n",
      "Epoch 9/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2939 - val_loss: 0.3306\n",
      "Epoch 10/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2964 - val_loss: 0.3258\n",
      "Epoch 11/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2977 - val_loss: 0.3216\n",
      "Epoch 12/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2960 - val_loss: 0.3203\n",
      "Epoch 13/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2904 - val_loss: 0.3263\n",
      "Epoch 14/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2918 - val_loss: 0.3252\n",
      "Epoch 15/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2882 - val_loss: 0.3247\n",
      "Epoch 16/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2889 - val_loss: 0.3218\n",
      "Epoch 17/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2881 - val_loss: 0.3210\n",
      "Epoch 18/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2817 - val_loss: 0.3217\n",
      "Epoch 19/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2819 - val_loss: 0.3182\n",
      "Epoch 20/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2758 - val_loss: 0.3174\n",
      "Epoch 21/200\n",
      "1963/1963 [==============================] - 0s 158us/step - loss: 0.2841 - val_loss: 0.3176\n",
      "Epoch 22/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2763 - val_loss: 0.3178\n",
      "Epoch 23/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2794 - val_loss: 0.3175\n",
      "Epoch 24/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2767 - val_loss: 0.3179\n",
      "Epoch 25/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2753 - val_loss: 0.3181\n",
      "Epoch 26/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2750 - val_loss: 0.3176\n",
      "Epoch 27/200\n",
      "1963/1963 [==============================] - 0s 155us/step - loss: 0.2745 - val_loss: 0.3172\n",
      "Epoch 28/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2754 - val_loss: 0.3167\n",
      "Epoch 29/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2688 - val_loss: 0.3172\n",
      "Epoch 30/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2745 - val_loss: 0.3174\n",
      "Epoch 31/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2746 - val_loss: 0.3166\n",
      "Epoch 32/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2722 - val_loss: 0.3170\n",
      "Epoch 33/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2707 - val_loss: 0.3169\n",
      "Epoch 34/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2705 - val_loss: 0.3167\n",
      "Epoch 35/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2710 - val_loss: 0.3158\n",
      "Epoch 36/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2705 - val_loss: 0.3160\n",
      "Epoch 37/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2754 - val_loss: 0.3162\n",
      "Epoch 38/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2702 - val_loss: 0.3170\n",
      "Epoch 39/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2687 - val_loss: 0.3161\n",
      "Epoch 40/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2682 - val_loss: 0.3155\n",
      "Epoch 41/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2647 - val_loss: 0.3156\n",
      "Epoch 42/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2646 - val_loss: 0.3161\n",
      "Epoch 43/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2683 - val_loss: 0.3155\n",
      "Epoch 44/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2674 - val_loss: 0.3148\n",
      "Epoch 45/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2709 - val_loss: 0.3149\n",
      "Epoch 46/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2656 - val_loss: 0.3136\n",
      "Epoch 47/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2655 - val_loss: 0.3147\n",
      "Epoch 48/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2689 - val_loss: 0.3140\n",
      "Epoch 49/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2679 - val_loss: 0.3152\n",
      "Epoch 50/200\n",
      "1963/1963 [==============================] - 0s 158us/step - loss: 0.2671 - val_loss: 0.3142\n",
      "Epoch 51/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2644 - val_loss: 0.3166\n",
      "Epoch 52/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2564 - val_loss: 0.3154\n",
      "Epoch 53/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2613 - val_loss: 0.3156\n",
      "Epoch 54/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2628 - val_loss: 0.3150\n",
      "Epoch 55/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2616 - val_loss: 0.3152\n",
      "Epoch 56/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2614 - val_loss: 0.3149\n",
      "Epoch 57/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2646 - val_loss: 0.3150\n",
      "Epoch 58/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2610 - val_loss: 0.3153\n",
      "Epoch 59/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2576 - val_loss: 0.3158\n",
      "Epoch 60/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2599 - val_loss: 0.3154\n",
      "Epoch 61/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2613 - val_loss: 0.3152\n",
      "Epoch 62/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2572 - val_loss: 0.3151\n",
      "Epoch 63/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2666 - val_loss: 0.3151\n",
      "Epoch 64/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2617 - val_loss: 0.3152\n",
      "Epoch 65/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2564 - val_loss: 0.3151\n",
      "Epoch 66/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2606 - val_loss: 0.3151\n",
      "Epoch 67/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2579 - val_loss: 0.3148\n",
      "Epoch 68/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2608 - val_loss: 0.3147\n",
      "Epoch 69/200\n",
      "1963/1963 [==============================] - 0s 157us/step - loss: 0.2645 - val_loss: 0.3150\n",
      "Epoch 70/200\n",
      "1963/1963 [==============================] - 0s 155us/step - loss: 0.2587 - val_loss: 0.3150\n",
      "Epoch 71/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2569 - val_loss: 0.3150\n",
      "Epoch 72/200\n",
      "1963/1963 [==============================] - 0s 156us/step - loss: 0.2589 - val_loss: 0.3151\n",
      "Epoch 73/200\n",
      "1963/1963 [==============================] - 0s 169us/step - loss: 0.2624 - val_loss: 0.3154\n",
      "Epoch 74/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2648 - val_loss: 0.3151\n",
      "Epoch 75/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2628 - val_loss: 0.3152\n",
      "Epoch 76/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2583 - val_loss: 0.3151\n",
      "Epoch 77/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2564 - val_loss: 0.3150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2602 - val_loss: 0.3150\n",
      "Epoch 79/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2583 - val_loss: 0.3150\n",
      "Epoch 80/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2588 - val_loss: 0.3148\n",
      "Epoch 81/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2585 - val_loss: 0.3150\n",
      "Epoch 82/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2564 - val_loss: 0.3150\n",
      "Epoch 83/200\n",
      "1963/1963 [==============================] - 0s 159us/step - loss: 0.2597 - val_loss: 0.3151\n",
      "Epoch 84/200\n",
      "1963/1963 [==============================] - 0s 160us/step - loss: 0.2575 - val_loss: 0.3155\n",
      "Epoch 85/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2579 - val_loss: 0.3153\n",
      "Epoch 86/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2575 - val_loss: 0.3153\n",
      "Epoch 87/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2608 - val_loss: 0.3157\n",
      "Epoch 88/200\n",
      "1963/1963 [==============================] - 0s 164us/step - loss: 0.2605 - val_loss: 0.3156\n",
      "Epoch 89/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2539 - val_loss: 0.3153\n",
      "Epoch 90/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2558 - val_loss: 0.3153\n",
      "Epoch 91/200\n",
      "1963/1963 [==============================] - 0s 166us/step - loss: 0.2630 - val_loss: 0.3154\n",
      "Epoch 92/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2538 - val_loss: 0.3153\n",
      "Epoch 93/200\n",
      "1963/1963 [==============================] - 0s 168us/step - loss: 0.2590 - val_loss: 0.3153\n",
      "Epoch 94/200\n",
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.2544 - val_loss: 0.3155\n",
      "Epoch 95/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2573 - val_loss: 0.3156\n",
      "Epoch 96/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2591 - val_loss: 0.3158\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 24)           0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 24)           2328        concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 24)           0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 120)          0           concatenate_16[0][0]             \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 24)           2904        concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 24)           0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 144)          0           concatenate_17[0][0]             \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 24)           3480        concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 24)           0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 168)          0           concatenate_18[0][0]             \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 24)           4056        concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 24)           0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 192)          0           concatenate_19[0][0]             \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 3)            579         concatenate_20[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1963 samples, validate on 491 samples\n",
      "Epoch 1/200\n",
      "1963/1963 [==============================] - 1s 437us/step - loss: 0.5087 - val_loss: 0.3746\n",
      "Epoch 2/200\n",
      "1963/1963 [==============================] - 0s 166us/step - loss: 0.3273 - val_loss: 0.3585\n",
      "Epoch 3/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.3354 - val_loss: 0.3505\n",
      "Epoch 4/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.3166 - val_loss: 0.3462\n",
      "Epoch 5/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.3114 - val_loss: 0.3412\n",
      "Epoch 6/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2998 - val_loss: 0.3381\n",
      "Epoch 7/200\n",
      "1963/1963 [==============================] - 0s 168us/step - loss: 0.3009 - val_loss: 0.3370\n",
      "Epoch 8/200\n",
      "1963/1963 [==============================] - 0s 164us/step - loss: 0.3002 - val_loss: 0.3383\n",
      "Epoch 9/200\n",
      "1963/1963 [==============================] - 0s 168us/step - loss: 0.3001 - val_loss: 0.3340\n",
      "Epoch 10/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2950 - val_loss: 0.3367\n",
      "Epoch 11/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2969 - val_loss: 0.3341\n",
      "Epoch 12/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2905 - val_loss: 0.3334\n",
      "Epoch 13/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2895 - val_loss: 0.3318\n",
      "Epoch 14/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2898 - val_loss: 0.3300\n",
      "Epoch 15/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2871 - val_loss: 0.3309\n",
      "Epoch 16/200\n",
      "1963/1963 [==============================] - 0s 161us/step - loss: 0.2823 - val_loss: 0.3314\n",
      "Epoch 17/200\n",
      "1963/1963 [==============================] - 0s 154us/step - loss: 0.2838 - val_loss: 0.3310\n",
      "Epoch 18/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2808 - val_loss: 0.3334\n",
      "Epoch 19/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2838 - val_loss: 0.3328\n",
      "Epoch 20/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2753 - val_loss: 0.3290\n",
      "Epoch 21/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2777 - val_loss: 0.3299\n",
      "Epoch 22/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2761 - val_loss: 0.3326\n",
      "Epoch 23/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2677 - val_loss: 0.3292\n",
      "Epoch 24/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2774 - val_loss: 0.3322\n",
      "Epoch 25/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2718 - val_loss: 0.3295\n",
      "Epoch 26/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2681 - val_loss: 0.3293\n",
      "Epoch 27/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2670 - val_loss: 0.3285\n",
      "Epoch 28/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2721 - val_loss: 0.3294\n",
      "Epoch 29/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2682 - val_loss: 0.3288\n",
      "Epoch 30/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2657 - val_loss: 0.3305\n",
      "Epoch 31/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2709 - val_loss: 0.3293\n",
      "Epoch 32/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2630 - val_loss: 0.3290\n",
      "Epoch 33/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2665 - val_loss: 0.3291\n",
      "Epoch 34/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2665 - val_loss: 0.3294\n",
      "Epoch 35/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2633 - val_loss: 0.3302\n",
      "Epoch 36/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2656 - val_loss: 0.3299\n",
      "Epoch 37/200\n",
      "1963/1963 [==============================] - 0s 154us/step - loss: 0.2589 - val_loss: 0.3300\n",
      "Epoch 38/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2715 - val_loss: 0.3300\n",
      "Epoch 39/200\n",
      "1963/1963 [==============================] - 0s 149us/step - loss: 0.2650 - val_loss: 0.3300\n",
      "Epoch 40/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2646 - val_loss: 0.3302\n",
      "Epoch 41/200\n",
      "1963/1963 [==============================] - 0s 153us/step - loss: 0.2597 - val_loss: 0.3304\n",
      "Epoch 42/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2617 - val_loss: 0.3303\n",
      "Epoch 43/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2621 - val_loss: 0.3300\n",
      "Epoch 44/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2660 - val_loss: 0.3304\n",
      "Epoch 45/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2640 - val_loss: 0.3304\n",
      "Epoch 46/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2648 - val_loss: 0.3304\n",
      "Epoch 47/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2627 - val_loss: 0.3304\n",
      "Epoch 48/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2636 - val_loss: 0.3301\n",
      "Epoch 49/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2582 - val_loss: 0.3300\n",
      "Epoch 50/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2588 - val_loss: 0.3304\n",
      "Epoch 51/200\n",
      "1963/1963 [==============================] - 0s 151us/step - loss: 0.2624 - val_loss: 0.3305\n",
      "Epoch 52/200\n",
      "1963/1963 [==============================] - 0s 150us/step - loss: 0.2595 - val_loss: 0.3306\n",
      "Epoch 53/200\n",
      "1963/1963 [==============================] - 0s 152us/step - loss: 0.2615 - val_loss: 0.3309\n",
      "Epoch 54/200\n",
      "1963/1963 [==============================] - 0s 155us/step - loss: 0.2626 - val_loss: 0.3309\n",
      "Epoch 55/200\n",
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.2640 - val_loss: 0.3312\n",
      "Epoch 56/200\n",
      "1963/1963 [==============================] - 0s 162us/step - loss: 0.2581 - val_loss: 0.3308\n",
      "Epoch 57/200\n",
      "1963/1963 [==============================] - 0s 169us/step - loss: 0.2667 - val_loss: 0.3307\n",
      "Epoch 58/200\n",
      "1963/1963 [==============================] - 0s 166us/step - loss: 0.2604 - val_loss: 0.3306\n",
      "Epoch 59/200\n",
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.2636 - val_loss: 0.3304\n",
      "Epoch 60/200\n",
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.2663 - val_loss: 0.3302\n",
      "Epoch 61/200\n",
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.2587 - val_loss: 0.3305\n",
      "Epoch 62/200\n",
      "1963/1963 [==============================] - 0s 169us/step - loss: 0.2607 - val_loss: 0.3305\n",
      "Epoch 63/200\n",
      "1963/1963 [==============================] - 0s 166us/step - loss: 0.2644 - val_loss: 0.3307\n",
      "Epoch 64/200\n",
      "1963/1963 [==============================] - 0s 168us/step - loss: 0.2582 - val_loss: 0.3308\n",
      "Epoch 65/200\n",
      "1963/1963 [==============================] - 0s 168us/step - loss: 0.2600 - val_loss: 0.3305\n",
      "Epoch 66/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2575 - val_loss: 0.3307\n",
      "Epoch 67/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2622 - val_loss: 0.3304\n",
      "Epoch 68/200\n",
      "1963/1963 [==============================] - 0s 169us/step - loss: 0.2582 - val_loss: 0.3307\n",
      "Epoch 69/200\n",
      "1963/1963 [==============================] - 0s 163us/step - loss: 0.2570 - val_loss: 0.3309\n",
      "Epoch 70/200\n",
      "1963/1963 [==============================] - 0s 165us/step - loss: 0.2592 - val_loss: 0.3303\n",
      "Epoch 71/200\n",
      "1963/1963 [==============================] - 0s 166us/step - loss: 0.2593 - val_loss: 0.3308\n",
      "Epoch 72/200\n",
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.2574 - val_loss: 0.3309\n",
      "Epoch 73/200\n",
      "1963/1963 [==============================] - 0s 168us/step - loss: 0.2616 - val_loss: 0.3306\n",
      "Epoch 74/200\n",
      "1963/1963 [==============================] - 0s 166us/step - loss: 0.2579 - val_loss: 0.3306\n",
      "Epoch 75/200\n",
      "1963/1963 [==============================] - 0s 167us/step - loss: 0.2588 - val_loss: 0.3308\n",
      "Epoch 76/200\n",
      "1963/1963 [==============================] - 0s 171us/step - loss: 0.2552 - val_loss: 0.3314\n",
      "Epoch 77/200\n",
      "1963/1963 [==============================] - 0s 169us/step - loss: 0.2596 - val_loss: 0.3311\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 24)           0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 24)           2328        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 24)           0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 120)          0           concatenate_21[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 24)           2904        concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 24)           0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 144)          0           concatenate_22[0][0]             \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 24)           3480        concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 24)           0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 168)          0           concatenate_23[0][0]             \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 24)           4056        concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 24)           0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 192)          0           concatenate_24[0][0]             \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 3)            579         concatenate_25[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1964 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1964/1964 [==============================] - 1s 469us/step - loss: 0.5038 - val_loss: 0.3199\n",
      "Epoch 2/200\n",
      "1964/1964 [==============================] - 0s 169us/step - loss: 0.3506 - val_loss: 0.2975\n",
      "Epoch 3/200\n",
      "1964/1964 [==============================] - 0s 167us/step - loss: 0.3481 - val_loss: 0.2896\n",
      "Epoch 4/200\n",
      "1964/1964 [==============================] - 0s 166us/step - loss: 0.3339 - val_loss: 0.2956\n",
      "Epoch 5/200\n",
      "1964/1964 [==============================] - 0s 166us/step - loss: 0.3225 - val_loss: 0.2929\n",
      "Epoch 6/200\n",
      "1964/1964 [==============================] - 0s 168us/step - loss: 0.3221 - val_loss: 0.2914\n",
      "Epoch 7/200\n",
      "1964/1964 [==============================] - 0s 169us/step - loss: 0.3184 - val_loss: 0.2902\n",
      "Epoch 8/200\n",
      "1964/1964 [==============================] - 0s 164us/step - loss: 0.3147 - val_loss: 0.2918\n",
      "Epoch 9/200\n",
      "1964/1964 [==============================] - 0s 163us/step - loss: 0.3125 - val_loss: 0.2898\n",
      "Epoch 10/200\n",
      "1964/1964 [==============================] - 0s 166us/step - loss: 0.3050 - val_loss: 0.2876\n",
      "Epoch 11/200\n",
      "1964/1964 [==============================] - 0s 165us/step - loss: 0.3028 - val_loss: 0.2872\n",
      "Epoch 12/200\n",
      "1964/1964 [==============================] - 0s 167us/step - loss: 0.3060 - val_loss: 0.2871\n",
      "Epoch 13/200\n",
      "1964/1964 [==============================] - 0s 164us/step - loss: 0.3050 - val_loss: 0.2879\n",
      "Epoch 14/200\n",
      "1964/1964 [==============================] - 0s 170us/step - loss: 0.3028 - val_loss: 0.2863\n",
      "Epoch 15/200\n",
      "1964/1964 [==============================] - 0s 165us/step - loss: 0.2982 - val_loss: 0.2893\n",
      "Epoch 16/200\n",
      "1964/1964 [==============================] - 0s 165us/step - loss: 0.2983 - val_loss: 0.2858\n",
      "Epoch 17/200\n",
      "1964/1964 [==============================] - 0s 167us/step - loss: 0.2961 - val_loss: 0.2889\n",
      "Epoch 18/200\n",
      "1964/1964 [==============================] - 0s 165us/step - loss: 0.2963 - val_loss: 0.2882\n",
      "Epoch 19/200\n",
      "1964/1964 [==============================] - 0s 168us/step - loss: 0.3003 - val_loss: 0.2897\n",
      "Epoch 20/200\n",
      "1964/1964 [==============================] - 0s 175us/step - loss: 0.2972 - val_loss: 0.2881\n",
      "Epoch 21/200\n",
      "1964/1964 [==============================] - 0s 164us/step - loss: 0.2964 - val_loss: 0.2897\n",
      "Epoch 22/200\n",
      "1964/1964 [==============================] - 0s 154us/step - loss: 0.2910 - val_loss: 0.2889\n",
      "Epoch 23/200\n",
      "1964/1964 [==============================] - 0s 153us/step - loss: 0.2933 - val_loss: 0.2881\n",
      "Epoch 24/200\n",
      "1964/1964 [==============================] - 0s 153us/step - loss: 0.2934 - val_loss: 0.2879\n",
      "Epoch 25/200\n",
      "1964/1964 [==============================] - 0s 153us/step - loss: 0.2895 - val_loss: 0.2878\n",
      "Epoch 26/200\n",
      "1964/1964 [==============================] - 0s 154us/step - loss: 0.2874 - val_loss: 0.2885\n",
      "Epoch 27/200\n",
      "1964/1964 [==============================] - 0s 154us/step - loss: 0.2890 - val_loss: 0.2888\n",
      "Epoch 28/200\n",
      "1964/1964 [==============================] - 0s 153us/step - loss: 0.2882 - val_loss: 0.2887\n",
      "Epoch 29/200\n",
      "1964/1964 [==============================] - 0s 151us/step - loss: 0.2881 - val_loss: 0.2883\n",
      "Epoch 30/200\n",
      "1964/1964 [==============================] - 0s 152us/step - loss: 0.2880 - val_loss: 0.2886\n",
      "Epoch 31/200\n",
      "1964/1964 [==============================] - 0s 153us/step - loss: 0.2852 - val_loss: 0.2892\n",
      "Epoch 32/200\n",
      "1964/1964 [==============================] - 0s 152us/step - loss: 0.2885 - val_loss: 0.2892\n",
      "Epoch 33/200\n",
      "1964/1964 [==============================] - 0s 152us/step - loss: 0.2841 - val_loss: 0.2889\n",
      "Epoch 34/200\n",
      "1964/1964 [==============================] - 0s 152us/step - loss: 0.2877 - val_loss: 0.2888\n",
      "Epoch 35/200\n",
      "1964/1964 [==============================] - 0s 150us/step - loss: 0.2852 - val_loss: 0.2890\n",
      "Epoch 36/200\n",
      "1964/1964 [==============================] - 0s 152us/step - loss: 0.2844 - val_loss: 0.2893\n",
      "Epoch 37/200\n",
      "1964/1964 [==============================] - 0s 165us/step - loss: 0.2882 - val_loss: 0.2890\n",
      "Epoch 38/200\n",
      "1964/1964 [==============================] - 0s 164us/step - loss: 0.2854 - val_loss: 0.2888\n",
      "Epoch 39/200\n",
      "1964/1964 [==============================] - 0s 162us/step - loss: 0.2894 - val_loss: 0.2886\n",
      "Epoch 40/200\n",
      "1964/1964 [==============================] - 0s 170us/step - loss: 0.2827 - val_loss: 0.2887\n",
      "Epoch 41/200\n",
      "1964/1964 [==============================] - 0s 171us/step - loss: 0.2848 - val_loss: 0.2885\n",
      "Epoch 42/200\n",
      "1964/1964 [==============================] - 0s 168us/step - loss: 0.2911 - val_loss: 0.2886\n",
      "Epoch 43/200\n",
      "1964/1964 [==============================] - 0s 165us/step - loss: 0.2900 - val_loss: 0.2886\n",
      "Epoch 44/200\n",
      "1964/1964 [==============================] - 0s 169us/step - loss: 0.2902 - val_loss: 0.2886\n",
      "Epoch 45/200\n",
      "1964/1964 [==============================] - 0s 166us/step - loss: 0.2851 - val_loss: 0.2890\n",
      "Epoch 46/200\n",
      "1964/1964 [==============================] - 0s 163us/step - loss: 0.2847 - val_loss: 0.2888\n",
      "Epoch 47/200\n",
      "1964/1964 [==============================] - 0s 170us/step - loss: 0.2841 - val_loss: 0.2889\n",
      "Epoch 48/200\n",
      "1964/1964 [==============================] - 0s 168us/step - loss: 0.2835 - val_loss: 0.2883\n",
      "Epoch 49/200\n",
      "1964/1964 [==============================] - 0s 164us/step - loss: 0.2849 - val_loss: 0.2884\n",
      "Epoch 50/200\n",
      "1964/1964 [==============================] - 0s 164us/step - loss: 0.2853 - val_loss: 0.2883\n",
      "Epoch 51/200\n",
      "1964/1964 [==============================] - 0s 162us/step - loss: 0.2890 - val_loss: 0.2885\n",
      "Epoch 52/200\n",
      "1964/1964 [==============================] - 0s 163us/step - loss: 0.2848 - val_loss: 0.2886\n",
      "Epoch 53/200\n",
      "1964/1964 [==============================] - 0s 161us/step - loss: 0.2839 - val_loss: 0.2889\n",
      "Epoch 54/200\n",
      "1964/1964 [==============================] - 0s 161us/step - loss: 0.2854 - val_loss: 0.2885\n",
      "Epoch 55/200\n",
      "1964/1964 [==============================] - 0s 169us/step - loss: 0.2837 - val_loss: 0.2885\n",
      "Epoch 56/200\n",
      "1964/1964 [==============================] - 0s 166us/step - loss: 0.2848 - val_loss: 0.2884\n",
      "Epoch 57/200\n",
      "1964/1964 [==============================] - 0s 167us/step - loss: 0.2868 - val_loss: 0.2883\n",
      "Epoch 58/200\n",
      "1964/1964 [==============================] - 0s 165us/step - loss: 0.2863 - val_loss: 0.2884\n",
      "Epoch 59/200\n",
      "1964/1964 [==============================] - 0s 166us/step - loss: 0.2862 - val_loss: 0.2882\n",
      "Epoch 60/200\n",
      "1964/1964 [==============================] - 0s 164us/step - loss: 0.2873 - val_loss: 0.2883\n",
      "Epoch 61/200\n",
      "1964/1964 [==============================] - 0s 172us/step - loss: 0.2833 - val_loss: 0.2883\n",
      "Epoch 62/200\n",
      "1964/1964 [==============================] - 0s 163us/step - loss: 0.2824 - val_loss: 0.2885\n",
      "Epoch 63/200\n",
      "1964/1964 [==============================] - 0s 165us/step - loss: 0.2856 - val_loss: 0.2888\n",
      "Epoch 64/200\n",
      "1964/1964 [==============================] - 0s 162us/step - loss: 0.2862 - val_loss: 0.2885\n",
      "Epoch 65/200\n",
      "1964/1964 [==============================] - 0s 166us/step - loss: 0.2853 - val_loss: 0.2884\n",
      "Epoch 66/200\n",
      "1964/1964 [==============================] - 0s 172us/step - loss: 0.2841 - val_loss: 0.2886\n",
      "------------------------------\n",
      "CV mean score: 0.2959, std: 0.0220.\n",
      "[0.28402959524659543, 0.267593791161127, 0.31362409310134665, 0.3284609442701954, 0.28577444364449806]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "prediction = np.zeros((len(test_y), 3)) # testing predictions\n",
    "# Training and cross-validation\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=3)\n",
    "scores = []\n",
    "oof = np.zeros_like(train_y)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(train_x)):\n",
    "    X_tr, X_val = train_x[train_index], train_x[valid_index]\n",
    "    Y_tr, Y_val = train_y[train_index], train_y[valid_index]\n",
    "    \n",
    "    classif_model = build_mlp_model([train_x.shape[-1]])\n",
    "    classif_model.compile(optimizer=Adam(lr=1e-3), loss=\"categorical_crossentropy\")\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=5e-5),\n",
    "                kc.ModelCheckpoint('stage_1_finals/nn_ensemble_checkpoint_f{}.pt'.format(fold_n), monitor='val_loss', verbose=0, save_best_only=True, mode='min')]\n",
    "    # train the model\n",
    "    classif_model.fit(x=X_tr, y=Y_tr, epochs=200, batch_size=32, \n",
    "                      callbacks=callbacks, validation_data=(X_val, Y_val), verbose=1)   \n",
    "    \n",
    "    pred_valid = classif_model.predict(x=X_val, verbose=0)\n",
    "    oof[valid_index] = pred_valid\n",
    "    pred = classif_model.predict(x=test_x, verbose=0)\n",
    "\n",
    "    # oof[valid_index] = pred_valid.reshape(-1,)\n",
    "    scores.append(log_loss(Y_val, pred_valid))\n",
    "    prediction += pred\n",
    "    \n",
    "prediction /= 5   \n",
    "print(\"-\" * 30)\n",
    "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "print(scores)\n",
    "#print(\"Test score:\", log_loss(test_y, prediction)) # this line should be removed in the stage 2\n",
    "print(\"-\" * 30)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.2992591341007771\n"
     ]
    }
   ],
   "source": [
    "#print(\"Test score:\", log_loss(test_y, prediction/5)) # this line should be removed in the stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a submission\n",
    "submission = pd.read_csv(\"data/sample_submission_stage_1.csv\", index_col = \"ID\")\n",
    "submission[\"A\"] = prediction[:,0]\n",
    "submission[\"B\"] = prediction[:,1]\n",
    "submission[\"NEITHER\"] = prediction[:,2]\n",
    "submission.to_csv(\"stage_1_finals/24_nn_ensembles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
