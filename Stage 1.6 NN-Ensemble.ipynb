{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission_stage_1.csv', 'test_stage_1.tsv', 'trees']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"data\"))\n",
    "import zipfile\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from keras import backend, models, layers, initializers, regularizers, constraints, optimizers\n",
    "from keras import callbacks as kc\n",
    "from keras import optimizers as ko\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the OOFs and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oof_filenames = os.listdir('oof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bnli-mh-bert-base-uncased-seq512-8.csv',\n",
       " 'bnli-mh-bert-large-cased-seq300-18.csv',\n",
       " 'bnli-mh-bert-large-uncased-seq300-19.csv',\n",
       " 'HBnli-mh-bert-base-uncased-seq512-8.csv',\n",
       " 'HBnli-mh-bert-large-cased-seq300-18.csv',\n",
       " 'HBnli-mh-bert-large-uncased-seq300-19.csv',\n",
       " 'Hnaive-bert-base-uncased-seq512-8.csv',\n",
       " 'Hnaive-bert-large-cased-seq300-18.csv',\n",
       " 'Hnaive-bert-large-uncased-seq300-19.csv',\n",
       " 'Hnli-mh-bert-base-uncased-seq512-8.csv',\n",
       " 'Hnli-mh-bert-large-cased-seq300-18.csv',\n",
       " 'Hnli-mh-bert-large-uncased-seq300-19.csv',\n",
       " 'naive-bert-base-uncased-seq512-8.csv',\n",
       " 'naive-bert-large-cased-seq300-18.csv',\n",
       " 'naive-bert-large-uncased-seq300-19.csv',\n",
       " 'nli-mh-bert-base-uncased-seq512-8.csv',\n",
       " 'nli-mh-bert-large-cased-seq300-18.csv',\n",
       " 'nli-mh-bert-large-uncased-seq300-19.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v for v in oof_filenames if 'bert-base-cased' not in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tag(oof_name):\n",
    "    if oof_name.startswith('naive'):\n",
    "        return '-'.join(oof_name.split('-')[1:4])\n",
    "    elif oof_name.startswith('nli'):\n",
    "        return '-'.join(oof_name.split('-')[2:5])\n",
    "    else:\n",
    "        pritn(\"[WTF?]\", oof_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oofs = [pd.read_csv(os.path.join('oof', v)).values for v in oof_filenames]\n",
    "\n",
    "dev_filename = \"contextual_embeddings_gap_development.json\"\n",
    "val_filename = \"contextual_embeddings_gap_validation.json\"\n",
    "test_filename = \"contextual_embeddings_gap_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_test = []\n",
    "remove_validation = []\n",
    "remove_development = [209, 1506, 1988]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_labels():\n",
    "    dev = pd.read_csv('gap-development.tsv', sep = '\\t')\n",
    "    test =  pd.read_csv('gap-test.tsv', sep = '\\t')\n",
    "    validation =  pd.read_csv('gap-validation.tsv', sep = '\\t')\n",
    "    \n",
    "    def _get_labels(df):\n",
    "        labels = df[['A-coref', 'B-coref']].values\n",
    "        ys = []\n",
    "        for label in labels:\n",
    "            y = np.zeros(3)\n",
    "            if label[0]:\n",
    "                y[0] = 1\n",
    "            elif label[1]:\n",
    "                y[1] = 1\n",
    "            else:\n",
    "                y[2] = 1\n",
    "            ys.append(y)\n",
    "        return np.array(ys)\n",
    "        \n",
    "    dev_labels, test_labels, valid_labels = _get_labels(dev), _get_labels(test), _get_labels(validation)  \n",
    "    dev_labels = np.delete(dev_labels, remove_development, 0)\n",
    "    test_labels = np.delete(test_labels, remove_test, 0)\n",
    "    valid_labels = np.delete(valid_labels, remove_validation, 0)\n",
    "    print(\"Shape of test, valid, dev samples\", test_labels.shape, valid_labels.shape, dev_labels.shape)\n",
    "    return test_labels, valid_labels, dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test, valid, dev samples (2000, 3) (454, 3) (1997, 3)\n"
     ]
    }
   ],
   "source": [
    "test_labels, valid_labels, dev_labels = get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n",
      "(4451, 3)\n"
     ]
    }
   ],
   "source": [
    "for oof in oofs:\n",
    "    print(oof.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (4451, 72) train_y shape: (4451, 3)\n"
     ]
    }
   ],
   "source": [
    "train_x = np.concatenate(oofs, axis=1)\n",
    "train_y = np.concatenate([test_labels, valid_labels, dev_labels], axis=0)\n",
    "\n",
    "print(\"train_x shape:\", train_x.shape, \"train_y shape:\", train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp_model(feature_nums):\n",
    "    features_inputs = Input(shape=feature_nums, name='mata-features', dtype=\"float32\")\n",
    "    features = features_inputs\n",
    "    \n",
    "    depth = 5\n",
    "    for i in range(depth):\n",
    "        new_features = Dense(24, activation='relu')(features)\n",
    "        new_features = Dropout(0.5)(new_features)\n",
    "        features = Concatenate()([features, new_features])\n",
    "\n",
    "    out_ = Dense(3, activation='softmax')(features)\n",
    "    \n",
    "    model = Model(inputs=[features_inputs], outputs=out_)\n",
    "    model.compile(optimizer=Adam(lr=1e-3, decay=1e-6,), loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 24)           0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 24)           2328        concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 24)           0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 120)          0           concatenate_16[0][0]             \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 24)           2904        concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 24)           0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 144)          0           concatenate_17[0][0]             \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 24)           3480        concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 24)           0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 168)          0           concatenate_18[0][0]             \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 24)           4056        concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 24)           0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 192)          0           concatenate_19[0][0]             \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 3)            579         concatenate_20[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 3815 samples, validate on 636 samples\n",
      "Epoch 1/200\n",
      "3815/3815 [==============================] - 1s 316us/step - loss: 0.4289 - val_loss: 0.3015\n",
      "Epoch 2/200\n",
      "3815/3815 [==============================] - 1s 165us/step - loss: 0.3144 - val_loss: 0.2902\n",
      "Epoch 3/200\n",
      "3815/3815 [==============================] - 1s 160us/step - loss: 0.3035 - val_loss: 0.2873\n",
      "Epoch 4/200\n",
      "3815/3815 [==============================] - 1s 164us/step - loss: 0.2939 - val_loss: 0.2828\n",
      "Epoch 5/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2916 - val_loss: 0.2810\n",
      "Epoch 6/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2912 - val_loss: 0.2825\n",
      "Epoch 7/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2864 - val_loss: 0.2824\n",
      "Epoch 8/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2857 - val_loss: 0.2818\n",
      "Epoch 9/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2820 - val_loss: 0.2763\n",
      "Epoch 10/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2830 - val_loss: 0.2776\n",
      "Epoch 11/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2787 - val_loss: 0.2760\n",
      "Epoch 12/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2804 - val_loss: 0.2791\n",
      "Epoch 13/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2778 - val_loss: 0.2809\n",
      "Epoch 14/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2778 - val_loss: 0.2767\n",
      "Epoch 15/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2763 - val_loss: 0.2784\n",
      "Epoch 16/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2750 - val_loss: 0.2776\n",
      "Epoch 17/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2725 - val_loss: 0.2747\n",
      "Epoch 18/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2711 - val_loss: 0.2745\n",
      "Epoch 19/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2699 - val_loss: 0.2747\n",
      "Epoch 20/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2676 - val_loss: 0.2737\n",
      "Epoch 21/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2669 - val_loss: 0.2735\n",
      "Epoch 22/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2698 - val_loss: 0.2721\n",
      "Epoch 23/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2687 - val_loss: 0.2734\n",
      "Epoch 24/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2681 - val_loss: 0.2727\n",
      "Epoch 25/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2670 - val_loss: 0.2719\n",
      "Epoch 26/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2650 - val_loss: 0.2725\n",
      "Epoch 27/200\n",
      "3815/3815 [==============================] - 1s 160us/step - loss: 0.2639 - val_loss: 0.2720\n",
      "Epoch 28/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2663 - val_loss: 0.2724\n",
      "Epoch 29/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2638 - val_loss: 0.2715\n",
      "Epoch 30/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2627 - val_loss: 0.2742\n",
      "Epoch 31/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2634 - val_loss: 0.2734\n",
      "Epoch 32/200\n",
      "3815/3815 [==============================] - 1s 164us/step - loss: 0.2625 - val_loss: 0.2724\n",
      "Epoch 33/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2624 - val_loss: 0.2730\n",
      "Epoch 34/200\n",
      "3815/3815 [==============================] - 1s 164us/step - loss: 0.2637 - val_loss: 0.2735\n",
      "Epoch 35/200\n",
      "3815/3815 [==============================] - 1s 166us/step - loss: 0.2599 - val_loss: 0.2721\n",
      "Epoch 36/200\n",
      "3815/3815 [==============================] - 1s 167us/step - loss: 0.2617 - val_loss: 0.2734\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2597 - val_loss: 0.2724\n",
      "Epoch 38/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2585 - val_loss: 0.2720\n",
      "Epoch 39/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2594 - val_loss: 0.2723\n",
      "Epoch 40/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2579 - val_loss: 0.2720\n",
      "Epoch 41/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2570 - val_loss: 0.2719\n",
      "Epoch 42/200\n",
      "3815/3815 [==============================] - 1s 160us/step - loss: 0.2573 - val_loss: 0.2717\n",
      "Epoch 43/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2556 - val_loss: 0.2715\n",
      "Epoch 44/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2571 - val_loss: 0.2718\n",
      "Epoch 45/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2565 - val_loss: 0.2718\n",
      "Epoch 46/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2584 - val_loss: 0.2721\n",
      "Epoch 47/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2568 - val_loss: 0.2718\n",
      "Epoch 48/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2556 - val_loss: 0.2718\n",
      "Epoch 49/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2549 - val_loss: 0.2719\n",
      "Epoch 50/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2533 - val_loss: 0.2718\n",
      "Epoch 51/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2560 - val_loss: 0.2718\n",
      "Epoch 52/200\n",
      "3815/3815 [==============================] - 1s 164us/step - loss: 0.2572 - val_loss: 0.2717\n",
      "Epoch 53/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2558 - val_loss: 0.2720\n",
      "Epoch 54/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2565 - val_loss: 0.2720\n",
      "Epoch 55/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2578 - val_loss: 0.2719\n",
      "Epoch 56/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2533 - val_loss: 0.2720\n",
      "Epoch 57/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2556 - val_loss: 0.2720\n",
      "Epoch 58/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2559 - val_loss: 0.2719\n",
      "Epoch 59/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2540 - val_loss: 0.2718\n",
      "Epoch 60/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2549 - val_loss: 0.2719\n",
      "Epoch 61/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2523 - val_loss: 0.2718\n",
      "Epoch 62/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2561 - val_loss: 0.2722\n",
      "Epoch 63/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2536 - val_loss: 0.2720\n",
      "Epoch 64/200\n",
      "3815/3815 [==============================] - 1s 160us/step - loss: 0.2545 - val_loss: 0.2722\n",
      "Epoch 65/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2535 - val_loss: 0.2720\n",
      "Epoch 66/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2525 - val_loss: 0.2723\n",
      "Epoch 67/200\n",
      "3815/3815 [==============================] - 1s 164us/step - loss: 0.2559 - val_loss: 0.2722\n",
      "Epoch 68/200\n",
      "3815/3815 [==============================] - 1s 166us/step - loss: 0.2554 - val_loss: 0.2723\n",
      "Epoch 69/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2548 - val_loss: 0.2725\n",
      "Epoch 70/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2546 - val_loss: 0.2723\n",
      "Epoch 71/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2558 - val_loss: 0.2722\n",
      "Epoch 72/200\n",
      "3815/3815 [==============================] - 1s 164us/step - loss: 0.2540 - val_loss: 0.2719\n",
      "Epoch 73/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2527 - val_loss: 0.2721\n",
      "Epoch 74/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2507 - val_loss: 0.2722\n",
      "Epoch 75/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2534 - val_loss: 0.2722\n",
      "Epoch 76/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2537 - val_loss: 0.2722\n",
      "Epoch 77/200\n",
      "3815/3815 [==============================] - 1s 160us/step - loss: 0.2528 - val_loss: 0.2720\n",
      "Epoch 78/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2519 - val_loss: 0.2719\n",
      "Epoch 79/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2520 - val_loss: 0.2719\n",
      "Epoch 80/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2542 - val_loss: 0.2721\n",
      "Epoch 81/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2495 - val_loss: 0.2721\n",
      "Epoch 82/200\n",
      "3815/3815 [==============================] - 1s 165us/step - loss: 0.2568 - val_loss: 0.2723\n",
      "Epoch 83/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2527 - val_loss: 0.2723\n",
      "Epoch 84/200\n",
      "3815/3815 [==============================] - 1s 147us/step - loss: 0.2531 - val_loss: 0.2724\n",
      "Epoch 85/200\n",
      "3815/3815 [==============================] - 1s 147us/step - loss: 0.2549 - val_loss: 0.2723\n",
      "Epoch 86/200\n",
      "3815/3815 [==============================] - 1s 147us/step - loss: 0.2545 - val_loss: 0.2722\n",
      "Epoch 87/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2500 - val_loss: 0.2723\n",
      "Epoch 88/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2531 - val_loss: 0.2726\n",
      "Epoch 89/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2537 - val_loss: 0.2724\n",
      "Epoch 90/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2503 - val_loss: 0.2723\n",
      "Epoch 91/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2506 - val_loss: 0.2723\n",
      "Epoch 92/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2508 - val_loss: 0.2725\n",
      "Epoch 93/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2536 - val_loss: 0.2724\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 24)           0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 24)           2328        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 24)           0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 120)          0           concatenate_21[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 24)           2904        concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 24)           0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 144)          0           concatenate_22[0][0]             \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 24)           3480        concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 24)           0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 168)          0           concatenate_23[0][0]             \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 24)           4056        concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 24)           0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 192)          0           concatenate_24[0][0]             \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 3)            579         concatenate_25[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3815 samples, validate on 636 samples\n",
      "Epoch 1/200\n",
      "3815/3815 [==============================] - 1s 312us/step - loss: 0.4469 - val_loss: 0.2915\n",
      "Epoch 2/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.3183 - val_loss: 0.2838\n",
      "Epoch 3/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.3033 - val_loss: 0.2824\n",
      "Epoch 4/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2977 - val_loss: 0.2891\n",
      "Epoch 5/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2933 - val_loss: 0.2895\n",
      "Epoch 6/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2909 - val_loss: 0.2866\n",
      "Epoch 7/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2848 - val_loss: 0.2899\n",
      "Epoch 8/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.2878 - val_loss: 0.2852\n",
      "Epoch 9/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2844 - val_loss: 0.2818\n",
      "Epoch 10/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2802 - val_loss: 0.2854\n",
      "Epoch 11/200\n",
      "3815/3815 [==============================] - 1s 164us/step - loss: 0.2778 - val_loss: 0.2854\n",
      "Epoch 12/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2793 - val_loss: 0.2818\n",
      "Epoch 13/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2795 - val_loss: 0.2819\n",
      "Epoch 14/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.2776 - val_loss: 0.2834\n",
      "Epoch 15/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2764 - val_loss: 0.2832\n",
      "Epoch 16/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2765 - val_loss: 0.2833\n",
      "Epoch 17/200\n",
      "3815/3815 [==============================] - 1s 167us/step - loss: 0.2708 - val_loss: 0.2833\n",
      "Epoch 18/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2714 - val_loss: 0.2833\n",
      "Epoch 19/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2750 - val_loss: 0.2826\n",
      "Epoch 20/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2733 - val_loss: 0.2834\n",
      "Epoch 21/200\n",
      "3815/3815 [==============================] - 1s 166us/step - loss: 0.2730 - val_loss: 0.2836\n",
      "Epoch 22/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2743 - val_loss: 0.2837\n",
      "Epoch 23/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2728 - val_loss: 0.2837\n",
      "Epoch 24/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2718 - val_loss: 0.2836\n",
      "Epoch 25/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2703 - val_loss: 0.2835\n",
      "Epoch 26/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2711 - val_loss: 0.2834\n",
      "Epoch 27/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2691 - val_loss: 0.2837\n",
      "Epoch 28/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2718 - val_loss: 0.2838\n",
      "Epoch 29/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2707 - val_loss: 0.2839\n",
      "Epoch 30/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2683 - val_loss: 0.2839\n",
      "Epoch 31/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2695 - val_loss: 0.2835\n",
      "Epoch 32/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2705 - val_loss: 0.2835\n",
      "Epoch 33/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2715 - val_loss: 0.2832\n",
      "Epoch 34/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2712 - val_loss: 0.2829\n",
      "Epoch 35/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2704 - val_loss: 0.2832\n",
      "Epoch 36/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2681 - val_loss: 0.2832\n",
      "Epoch 37/200\n",
      "3815/3815 [==============================] - 1s 160us/step - loss: 0.2682 - val_loss: 0.2832\n",
      "Epoch 38/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2689 - val_loss: 0.2836\n",
      "Epoch 39/200\n",
      "3815/3815 [==============================] - 1s 165us/step - loss: 0.2693 - val_loss: 0.2840\n",
      "Epoch 40/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2686 - val_loss: 0.2842\n",
      "Epoch 41/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2692 - val_loss: 0.2840\n",
      "Epoch 42/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2674 - val_loss: 0.2839\n",
      "Epoch 43/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2681 - val_loss: 0.2839\n",
      "Epoch 44/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2682 - val_loss: 0.2836\n",
      "Epoch 45/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2684 - val_loss: 0.2837\n",
      "Epoch 46/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2675 - val_loss: 0.2837\n",
      "Epoch 47/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2677 - val_loss: 0.2841\n",
      "Epoch 48/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2675 - val_loss: 0.2839\n",
      "Epoch 49/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2643 - val_loss: 0.2841\n",
      "Epoch 50/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2668 - val_loss: 0.2839\n",
      "Epoch 51/200\n",
      "3815/3815 [==============================] - 1s 160us/step - loss: 0.2661 - val_loss: 0.2835\n",
      "Epoch 52/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2662 - val_loss: 0.2839\n",
      "Epoch 53/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2697 - val_loss: 0.2838\n",
      "Epoch 54/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2687 - val_loss: 0.2840\n",
      "Epoch 55/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2683 - val_loss: 0.2837\n",
      "Epoch 56/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2673 - val_loss: 0.2837\n",
      "Epoch 57/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2648 - val_loss: 0.2839\n",
      "Epoch 58/200\n",
      "3815/3815 [==============================] - 1s 167us/step - loss: 0.2677 - val_loss: 0.2837\n",
      "Epoch 59/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2672 - val_loss: 0.2840\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 24)           0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 24)           2328        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 24)           0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 120)          0           concatenate_26[0][0]             \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 24)           2904        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 24)           0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 144)          0           concatenate_27[0][0]             \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 24)           3480        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 24)           0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 168)          0           concatenate_28[0][0]             \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 24)           4056        concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 24)           0           dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 192)          0           concatenate_29[0][0]             \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 3)            579         concatenate_30[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3815 samples, validate on 636 samples\n",
      "Epoch 1/200\n",
      "3815/3815 [==============================] - 1s 327us/step - loss: 0.4187 - val_loss: 0.3259\n",
      "Epoch 2/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.3174 - val_loss: 0.3237\n",
      "Epoch 3/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2952 - val_loss: 0.3222\n",
      "Epoch 4/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2940 - val_loss: 0.3103\n",
      "Epoch 5/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2950 - val_loss: 0.3153\n",
      "Epoch 6/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2904 - val_loss: 0.3099\n",
      "Epoch 7/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2839 - val_loss: 0.3098\n",
      "Epoch 8/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2823 - val_loss: 0.3133\n",
      "Epoch 9/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.2823 - val_loss: 0.3072\n",
      "Epoch 10/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2781 - val_loss: 0.3075\n",
      "Epoch 11/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2766 - val_loss: 0.3065\n",
      "Epoch 12/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2743 - val_loss: 0.3127\n",
      "Epoch 13/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2763 - val_loss: 0.3130\n",
      "Epoch 14/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2742 - val_loss: 0.3113\n",
      "Epoch 15/200\n",
      "3815/3815 [==============================] - 1s 165us/step - loss: 0.2732 - val_loss: 0.3063\n",
      "Epoch 16/200\n",
      "3815/3815 [==============================] - 1s 173us/step - loss: 0.2707 - val_loss: 0.3118\n",
      "Epoch 17/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2700 - val_loss: 0.3045\n",
      "Epoch 18/200\n",
      "3815/3815 [==============================] - 1s 167us/step - loss: 0.2721 - val_loss: 0.3050\n",
      "Epoch 19/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2671 - val_loss: 0.3055\n",
      "Epoch 20/200\n",
      "3815/3815 [==============================] - 1s 169us/step - loss: 0.2681 - val_loss: 0.3072\n",
      "Epoch 21/200\n",
      "3815/3815 [==============================] - 1s 166us/step - loss: 0.2654 - val_loss: 0.3160\n",
      "Epoch 22/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2659 - val_loss: 0.3038\n",
      "Epoch 23/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2668 - val_loss: 0.3108\n",
      "Epoch 24/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2708 - val_loss: 0.3130\n",
      "Epoch 25/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2653 - val_loss: 0.3015\n",
      "Epoch 26/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2670 - val_loss: 0.3082\n",
      "Epoch 27/200\n",
      "3815/3815 [==============================] - 1s 160us/step - loss: 0.2628 - val_loss: 0.3055\n",
      "Epoch 28/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2622 - val_loss: 0.3126\n",
      "Epoch 29/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2655 - val_loss: 0.3064\n",
      "Epoch 30/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2613 - val_loss: 0.3072\n",
      "Epoch 31/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2582 - val_loss: 0.3087\n",
      "Epoch 32/200\n",
      "3815/3815 [==============================] - 1s 166us/step - loss: 0.2585 - val_loss: 0.3047\n",
      "Epoch 33/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2576 - val_loss: 0.3045\n",
      "Epoch 34/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2607 - val_loss: 0.3116\n",
      "Epoch 35/200\n",
      "3815/3815 [==============================] - 1s 161us/step - loss: 0.2568 - val_loss: 0.3099\n",
      "Epoch 36/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2559 - val_loss: 0.3097\n",
      "Epoch 37/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2530 - val_loss: 0.3060\n",
      "Epoch 38/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2545 - val_loss: 0.3071\n",
      "Epoch 39/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2544 - val_loss: 0.3083\n",
      "Epoch 40/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2534 - val_loss: 0.3124\n",
      "Epoch 41/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2510 - val_loss: 0.3096\n",
      "Epoch 42/200\n",
      "3815/3815 [==============================] - 1s 147us/step - loss: 0.2525 - val_loss: 0.3090\n",
      "Epoch 43/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2513 - val_loss: 0.3084\n",
      "Epoch 44/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2501 - val_loss: 0.3078\n",
      "Epoch 45/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2521 - val_loss: 0.3068\n",
      "Epoch 46/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2490 - val_loss: 0.3073\n",
      "Epoch 47/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2509 - val_loss: 0.3078\n",
      "Epoch 48/200\n",
      "3815/3815 [==============================] - 1s 146us/step - loss: 0.2529 - val_loss: 0.3080\n",
      "Epoch 49/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2524 - val_loss: 0.3077\n",
      "Epoch 50/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2475 - val_loss: 0.3082\n",
      "Epoch 51/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.2510 - val_loss: 0.3086\n",
      "Epoch 52/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2491 - val_loss: 0.3088\n",
      "Epoch 53/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2502 - val_loss: 0.3088\n",
      "Epoch 54/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2489 - val_loss: 0.3091\n",
      "Epoch 55/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2489 - val_loss: 0.3091\n",
      "Epoch 56/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2469 - val_loss: 0.3088\n",
      "Epoch 57/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.2485 - val_loss: 0.3087\n",
      "Epoch 58/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2474 - val_loss: 0.3088\n",
      "Epoch 59/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2494 - val_loss: 0.3086\n",
      "Epoch 60/200\n",
      "3815/3815 [==============================] - 1s 166us/step - loss: 0.2487 - val_loss: 0.3087\n",
      "Epoch 61/200\n",
      "3815/3815 [==============================] - 1s 160us/step - loss: 0.2493 - val_loss: 0.3084\n",
      "Epoch 62/200\n",
      "3815/3815 [==============================] - 1s 149us/step - loss: 0.2477 - val_loss: 0.3090\n",
      "Epoch 63/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2484 - val_loss: 0.3091\n",
      "Epoch 64/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2460 - val_loss: 0.3096\n",
      "Epoch 65/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2451 - val_loss: 0.3098\n",
      "Epoch 66/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2491 - val_loss: 0.3091\n",
      "Epoch 67/200\n",
      "3815/3815 [==============================] - 1s 159us/step - loss: 0.2510 - val_loss: 0.3084\n",
      "Epoch 68/200\n",
      "3815/3815 [==============================] - 1s 165us/step - loss: 0.2480 - val_loss: 0.3086\n",
      "Epoch 69/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2485 - val_loss: 0.3093\n",
      "Epoch 70/200\n",
      "3815/3815 [==============================] - 1s 166us/step - loss: 0.2472 - val_loss: 0.3094\n",
      "Epoch 71/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2428 - val_loss: 0.3098\n",
      "Epoch 72/200\n",
      "3815/3815 [==============================] - 1s 164us/step - loss: 0.2468 - val_loss: 0.3096\n",
      "Epoch 73/200\n",
      "3815/3815 [==============================] - 1s 166us/step - loss: 0.2471 - val_loss: 0.3088\n",
      "Epoch 74/200\n",
      "3815/3815 [==============================] - 1s 169us/step - loss: 0.2462 - val_loss: 0.3087\n",
      "Epoch 75/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2453 - val_loss: 0.3097\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 24)           0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 24)           2328        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 24)           0           dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 120)          0           concatenate_31[0][0]             \n",
      "                                                                 dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 24)           2904        concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 24)           0           dense_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 144)          0           concatenate_32[0][0]             \n",
      "                                                                 dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 24)           3480        concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 24)           0           dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 168)          0           concatenate_33[0][0]             \n",
      "                                                                 dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 24)           4056        concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 24)           0           dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 192)          0           concatenate_34[0][0]             \n",
      "                                                                 dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 3)            579         concatenate_35[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3815 samples, validate on 636 samples\n",
      "Epoch 1/200\n",
      "3815/3815 [==============================] - 1s 333us/step - loss: 0.4134 - val_loss: 0.2854\n",
      "Epoch 2/200\n",
      "3815/3815 [==============================] - 1s 181us/step - loss: 0.3200 - val_loss: 0.2723\n",
      "Epoch 3/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.3053 - val_loss: 0.2624\n",
      "Epoch 4/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.3009 - val_loss: 0.2604\n",
      "Epoch 5/200\n",
      "3815/3815 [==============================] - 1s 149us/step - loss: 0.2988 - val_loss: 0.2579\n",
      "Epoch 6/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2934 - val_loss: 0.2584\n",
      "Epoch 7/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2881 - val_loss: 0.2624\n",
      "Epoch 8/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.2893 - val_loss: 0.2582\n",
      "Epoch 9/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2841 - val_loss: 0.2610\n",
      "Epoch 10/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.2859 - val_loss: 0.2575\n",
      "Epoch 11/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2863 - val_loss: 0.2595\n",
      "Epoch 12/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2831 - val_loss: 0.2633\n",
      "Epoch 13/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2846 - val_loss: 0.2582\n",
      "Epoch 14/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2813 - val_loss: 0.2588\n",
      "Epoch 15/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2795 - val_loss: 0.2611\n",
      "Epoch 16/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2770 - val_loss: 0.2563\n",
      "Epoch 17/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2762 - val_loss: 0.2579\n",
      "Epoch 18/200\n",
      "3815/3815 [==============================] - 1s 149us/step - loss: 0.2753 - val_loss: 0.2592\n",
      "Epoch 19/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2739 - val_loss: 0.2546\n",
      "Epoch 20/200\n",
      "3815/3815 [==============================] - 1s 149us/step - loss: 0.2714 - val_loss: 0.2568\n",
      "Epoch 21/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2714 - val_loss: 0.2588\n",
      "Epoch 22/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2730 - val_loss: 0.2570\n",
      "Epoch 23/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2734 - val_loss: 0.2581\n",
      "Epoch 24/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2713 - val_loss: 0.2617\n",
      "Epoch 25/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2717 - val_loss: 0.2569\n",
      "Epoch 26/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2685 - val_loss: 0.2558\n",
      "Epoch 27/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2686 - val_loss: 0.2552\n",
      "Epoch 28/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2683 - val_loss: 0.2562\n",
      "Epoch 29/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2656 - val_loss: 0.2579\n",
      "Epoch 30/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2685 - val_loss: 0.2574\n",
      "Epoch 31/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2688 - val_loss: 0.2566\n",
      "Epoch 32/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2681 - val_loss: 0.2567\n",
      "Epoch 33/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2652 - val_loss: 0.2561\n",
      "Epoch 34/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2637 - val_loss: 0.2567\n",
      "Epoch 35/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2667 - val_loss: 0.2569\n",
      "Epoch 36/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2654 - val_loss: 0.2566\n",
      "Epoch 37/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2643 - val_loss: 0.2565\n",
      "Epoch 38/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2627 - val_loss: 0.2562\n",
      "Epoch 39/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2663 - val_loss: 0.2563\n",
      "Epoch 40/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2622 - val_loss: 0.2561\n",
      "Epoch 41/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2615 - val_loss: 0.2566\n",
      "Epoch 42/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2633 - val_loss: 0.2566\n",
      "Epoch 43/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.2628 - val_loss: 0.2564\n",
      "Epoch 44/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2653 - val_loss: 0.2566\n",
      "Epoch 45/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2626 - val_loss: 0.2564\n",
      "Epoch 46/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2614 - val_loss: 0.2563\n",
      "Epoch 47/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2633 - val_loss: 0.2565\n",
      "Epoch 48/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2614 - val_loss: 0.2563\n",
      "Epoch 49/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2639 - val_loss: 0.2565\n",
      "Epoch 50/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2643 - val_loss: 0.2571\n",
      "Epoch 51/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2635 - val_loss: 0.2572\n",
      "Epoch 52/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2620 - val_loss: 0.2571\n",
      "Epoch 53/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.2596 - val_loss: 0.2568\n",
      "Epoch 54/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2619 - val_loss: 0.2569\n",
      "Epoch 55/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2622 - val_loss: 0.2570\n",
      "Epoch 56/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2614 - val_loss: 0.2570\n",
      "Epoch 57/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2649 - val_loss: 0.2566\n",
      "Epoch 58/200\n",
      "3815/3815 [==============================] - 1s 149us/step - loss: 0.2614 - val_loss: 0.2562\n",
      "Epoch 59/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2601 - val_loss: 0.2566\n",
      "Epoch 60/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2617 - val_loss: 0.2567\n",
      "Epoch 61/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2611 - val_loss: 0.2567\n",
      "Epoch 62/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2627 - val_loss: 0.2566\n",
      "Epoch 63/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2640 - val_loss: 0.2564\n",
      "Epoch 64/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2629 - val_loss: 0.2571\n",
      "Epoch 65/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2619 - val_loss: 0.2567\n",
      "Epoch 66/200\n",
      "3815/3815 [==============================] - 1s 149us/step - loss: 0.2625 - val_loss: 0.2568\n",
      "Epoch 67/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2602 - val_loss: 0.2570\n",
      "Epoch 68/200\n",
      "3815/3815 [==============================] - 1s 149us/step - loss: 0.2597 - val_loss: 0.2568\n",
      "Epoch 69/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2618 - val_loss: 0.2571\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 24)           0           dense_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 24)           2328        concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 24)           0           dense_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 120)          0           concatenate_36[0][0]             \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 24)           2904        concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 24)           0           dense_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 144)          0           concatenate_37[0][0]             \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 24)           3480        concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 24)           0           dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 168)          0           concatenate_38[0][0]             \n",
      "                                                                 dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 24)           4056        concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 24)           0           dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 192)          0           concatenate_39[0][0]             \n",
      "                                                                 dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 3)            579         concatenate_40[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3815 samples, validate on 636 samples\n",
      "Epoch 1/200\n",
      "3815/3815 [==============================] - 1s 357us/step - loss: 0.4238 - val_loss: 0.2488\n",
      "Epoch 2/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.3168 - val_loss: 0.2482\n",
      "Epoch 3/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.3057 - val_loss: 0.2397\n",
      "Epoch 4/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.3026 - val_loss: 0.2394\n",
      "Epoch 5/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.3001 - val_loss: 0.2377\n",
      "Epoch 6/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2972 - val_loss: 0.2360\n",
      "Epoch 7/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2944 - val_loss: 0.2346\n",
      "Epoch 8/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2920 - val_loss: 0.2369\n",
      "Epoch 9/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2941 - val_loss: 0.2332\n",
      "Epoch 10/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2883 - val_loss: 0.2303\n",
      "Epoch 11/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2893 - val_loss: 0.2310\n",
      "Epoch 12/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2903 - val_loss: 0.2304\n",
      "Epoch 13/200\n",
      "3815/3815 [==============================] - 1s 173us/step - loss: 0.2870 - val_loss: 0.2303\n",
      "Epoch 14/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2853 - val_loss: 0.2300\n",
      "Epoch 15/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2862 - val_loss: 0.2281\n",
      "Epoch 16/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2852 - val_loss: 0.2264\n",
      "Epoch 17/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2813 - val_loss: 0.2274\n",
      "Epoch 18/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2809 - val_loss: 0.2290\n",
      "Epoch 19/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2805 - val_loss: 0.2299\n",
      "Epoch 20/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2808 - val_loss: 0.2276\n",
      "Epoch 21/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2784 - val_loss: 0.2272\n",
      "Epoch 22/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.2761 - val_loss: 0.2254\n",
      "Epoch 23/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2756 - val_loss: 0.2260\n",
      "Epoch 24/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2778 - val_loss: 0.2265\n",
      "Epoch 25/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2729 - val_loss: 0.2288\n",
      "Epoch 26/200\n",
      "3815/3815 [==============================] - 1s 149us/step - loss: 0.2737 - val_loss: 0.2270\n",
      "Epoch 27/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2744 - val_loss: 0.2273\n",
      "Epoch 28/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2719 - val_loss: 0.2269\n",
      "Epoch 29/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2707 - val_loss: 0.2255\n",
      "Epoch 30/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2722 - val_loss: 0.2255\n",
      "Epoch 31/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2670 - val_loss: 0.2267\n",
      "Epoch 32/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2686 - val_loss: 0.2254\n",
      "Epoch 33/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.2693 - val_loss: 0.2258\n",
      "Epoch 34/200\n",
      "3815/3815 [==============================] - 1s 153us/step - loss: 0.2677 - val_loss: 0.2263\n",
      "Epoch 35/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2718 - val_loss: 0.2262\n",
      "Epoch 36/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2679 - val_loss: 0.2268\n",
      "Epoch 37/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2695 - val_loss: 0.2267\n",
      "Epoch 38/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2693 - val_loss: 0.2270\n",
      "Epoch 39/200\n",
      "3815/3815 [==============================] - 1s 165us/step - loss: 0.2670 - val_loss: 0.2269\n",
      "Epoch 40/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2644 - val_loss: 0.2268\n",
      "Epoch 41/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2657 - val_loss: 0.2269\n",
      "Epoch 42/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2655 - val_loss: 0.2269\n",
      "Epoch 43/200\n",
      "3815/3815 [==============================] - 1s 152us/step - loss: 0.2659 - val_loss: 0.2273\n",
      "Epoch 44/200\n",
      "3815/3815 [==============================] - 1s 149us/step - loss: 0.2695 - val_loss: 0.2269\n",
      "Epoch 45/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2672 - val_loss: 0.2269\n",
      "Epoch 46/200\n",
      "3815/3815 [==============================] - 1s 166us/step - loss: 0.2676 - val_loss: 0.2269\n",
      "Epoch 47/200\n",
      "3815/3815 [==============================] - 1s 164us/step - loss: 0.2657 - val_loss: 0.2267\n",
      "Epoch 48/200\n",
      "3815/3815 [==============================] - 1s 165us/step - loss: 0.2669 - val_loss: 0.2267\n",
      "Epoch 49/200\n",
      "3815/3815 [==============================] - 1s 167us/step - loss: 0.2685 - val_loss: 0.2265\n",
      "Epoch 50/200\n",
      "3815/3815 [==============================] - 1s 166us/step - loss: 0.2673 - val_loss: 0.2267\n",
      "Epoch 51/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2658 - val_loss: 0.2266\n",
      "Epoch 52/200\n",
      "3815/3815 [==============================] - 1s 167us/step - loss: 0.2683 - val_loss: 0.2265\n",
      "Epoch 53/200\n",
      "3815/3815 [==============================] - 1s 164us/step - loss: 0.2663 - val_loss: 0.2263\n",
      "Epoch 54/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2669 - val_loss: 0.2264\n",
      "Epoch 55/200\n",
      "3815/3815 [==============================] - 1s 162us/step - loss: 0.2645 - val_loss: 0.2263\n",
      "Epoch 56/200\n",
      "3815/3815 [==============================] - 1s 163us/step - loss: 0.2670 - val_loss: 0.2266\n",
      "Epoch 57/200\n",
      "3815/3815 [==============================] - 1s 170us/step - loss: 0.2645 - val_loss: 0.2267\n",
      "Epoch 58/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2643 - val_loss: 0.2266\n",
      "Epoch 59/200\n",
      "3815/3815 [==============================] - 1s 164us/step - loss: 0.2611 - val_loss: 0.2266\n",
      "Epoch 60/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2668 - val_loss: 0.2265\n",
      "Epoch 61/200\n",
      "3815/3815 [==============================] - 1s 151us/step - loss: 0.2666 - val_loss: 0.2268\n",
      "Epoch 62/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2673 - val_loss: 0.2268\n",
      "Epoch 63/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2661 - val_loss: 0.2270\n",
      "Epoch 64/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2647 - val_loss: 0.2269\n",
      "Epoch 65/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2645 - val_loss: 0.2272\n",
      "Epoch 66/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2643 - val_loss: 0.2270\n",
      "Epoch 67/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2669 - val_loss: 0.2271\n",
      "Epoch 68/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2640 - val_loss: 0.2272\n",
      "Epoch 69/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2637 - val_loss: 0.2269\n",
      "Epoch 70/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2647 - val_loss: 0.2268\n",
      "Epoch 71/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2643 - val_loss: 0.2266\n",
      "Epoch 72/200\n",
      "3815/3815 [==============================] - 1s 149us/step - loss: 0.2635 - val_loss: 0.2270\n",
      "Epoch 73/200\n",
      "3815/3815 [==============================] - 1s 149us/step - loss: 0.2651 - val_loss: 0.2267\n",
      "Epoch 74/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2640 - val_loss: 0.2266\n",
      "Epoch 75/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2651 - val_loss: 0.2267\n",
      "Epoch 76/200\n",
      "3815/3815 [==============================] - 1s 149us/step - loss: 0.2656 - val_loss: 0.2268\n",
      "Epoch 77/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2610 - val_loss: 0.2267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2634 - val_loss: 0.2267\n",
      "Epoch 79/200\n",
      "3815/3815 [==============================] - 1s 150us/step - loss: 0.2658 - val_loss: 0.2268\n",
      "Epoch 80/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2624 - val_loss: 0.2270\n",
      "Epoch 81/200\n",
      "3815/3815 [==============================] - 1s 147us/step - loss: 0.2632 - val_loss: 0.2270\n",
      "Epoch 82/200\n",
      "3815/3815 [==============================] - 1s 148us/step - loss: 0.2664 - val_loss: 0.2273\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 24)           0           dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 24)           2328        concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 24)           0           dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 120)          0           concatenate_41[0][0]             \n",
      "                                                                 dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 24)           2904        concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 24)           0           dense_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 144)          0           concatenate_42[0][0]             \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 24)           3480        concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 24)           0           dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 168)          0           concatenate_43[0][0]             \n",
      "                                                                 dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 24)           4056        concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 24)           0           dense_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 192)          0           concatenate_44[0][0]             \n",
      "                                                                 dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 3)            579         concatenate_45[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 3815 samples, validate on 636 samples\n",
      "Epoch 1/200\n",
      "3815/3815 [==============================] - 1s 361us/step - loss: 0.3904 - val_loss: 0.3315\n",
      "Epoch 2/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.3119 - val_loss: 0.3176\n",
      "Epoch 3/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2994 - val_loss: 0.3197\n",
      "Epoch 4/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2922 - val_loss: 0.3176\n",
      "Epoch 5/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2883 - val_loss: 0.3196\n",
      "Epoch 6/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2857 - val_loss: 0.3229\n",
      "Epoch 7/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2809 - val_loss: 0.3127\n",
      "Epoch 8/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2815 - val_loss: 0.3134\n",
      "Epoch 9/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2761 - val_loss: 0.3212\n",
      "Epoch 10/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2755 - val_loss: 0.3145\n",
      "Epoch 11/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2695 - val_loss: 0.3143\n",
      "Epoch 12/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2742 - val_loss: 0.3151\n",
      "Epoch 13/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2704 - val_loss: 0.3123\n",
      "Epoch 14/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2687 - val_loss: 0.3126\n",
      "Epoch 15/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2690 - val_loss: 0.3140\n",
      "Epoch 16/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2653 - val_loss: 0.3130\n",
      "Epoch 17/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2639 - val_loss: 0.3131\n",
      "Epoch 18/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2686 - val_loss: 0.3136\n",
      "Epoch 19/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2632 - val_loss: 0.3129\n",
      "Epoch 20/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2634 - val_loss: 0.3131\n",
      "Epoch 21/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2600 - val_loss: 0.3146\n",
      "Epoch 22/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2648 - val_loss: 0.3142\n",
      "Epoch 23/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2593 - val_loss: 0.3145\n",
      "Epoch 24/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2613 - val_loss: 0.3138\n",
      "Epoch 25/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2605 - val_loss: 0.3138\n",
      "Epoch 26/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2596 - val_loss: 0.3130\n",
      "Epoch 27/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2592 - val_loss: 0.3135\n",
      "Epoch 28/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2626 - val_loss: 0.3134\n",
      "Epoch 29/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2604 - val_loss: 0.3130\n",
      "Epoch 30/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2605 - val_loss: 0.3134\n",
      "Epoch 31/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2605 - val_loss: 0.3134\n",
      "Epoch 32/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2617 - val_loss: 0.3134\n",
      "Epoch 33/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2602 - val_loss: 0.3131\n",
      "Epoch 34/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2607 - val_loss: 0.3132\n",
      "Epoch 35/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2595 - val_loss: 0.3137\n",
      "Epoch 36/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2593 - val_loss: 0.3135\n",
      "Epoch 37/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2600 - val_loss: 0.3134\n",
      "Epoch 38/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2585 - val_loss: 0.3136\n",
      "Epoch 39/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2577 - val_loss: 0.3135\n",
      "Epoch 40/200\n",
      "3815/3815 [==============================] - 1s 158us/step - loss: 0.2577 - val_loss: 0.3138\n",
      "Epoch 41/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2612 - val_loss: 0.3136\n",
      "Epoch 42/200\n",
      "3815/3815 [==============================] - 1s 157us/step - loss: 0.2595 - val_loss: 0.3136\n",
      "Epoch 43/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2568 - val_loss: 0.3134\n",
      "Epoch 44/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2581 - val_loss: 0.3132\n",
      "Epoch 45/200\n",
      "3815/3815 [==============================] - 1s 156us/step - loss: 0.2596 - val_loss: 0.3135\n",
      "Epoch 46/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2582 - val_loss: 0.3140\n",
      "Epoch 47/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2585 - val_loss: 0.3139\n",
      "Epoch 48/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2582 - val_loss: 0.3143\n",
      "Epoch 49/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2564 - val_loss: 0.3143\n",
      "Epoch 50/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2599 - val_loss: 0.3142\n",
      "Epoch 51/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2585 - val_loss: 0.3142\n",
      "Epoch 52/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2581 - val_loss: 0.3143\n",
      "Epoch 53/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2559 - val_loss: 0.3142\n",
      "Epoch 54/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2547 - val_loss: 0.3143\n",
      "Epoch 55/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2553 - val_loss: 0.3149\n",
      "Epoch 56/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2571 - val_loss: 0.3152\n",
      "Epoch 57/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2575 - val_loss: 0.3149\n",
      "Epoch 58/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2590 - val_loss: 0.3147\n",
      "Epoch 59/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2558 - val_loss: 0.3143\n",
      "Epoch 60/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2557 - val_loss: 0.3143\n",
      "Epoch 61/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2591 - val_loss: 0.3142\n",
      "Epoch 62/200\n",
      "3815/3815 [==============================] - 1s 154us/step - loss: 0.2545 - val_loss: 0.3145\n",
      "Epoch 63/200\n",
      "3815/3815 [==============================] - 1s 155us/step - loss: 0.2567 - val_loss: 0.3147\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_55 (Dense)                (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 24)           0           dense_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 24)           2328        concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 24)           0           dense_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 120)          0           concatenate_46[0][0]             \n",
      "                                                                 dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 24)           2904        concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 24)           0           dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 144)          0           concatenate_47[0][0]             \n",
      "                                                                 dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 24)           3480        concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 24)           0           dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 168)          0           concatenate_48[0][0]             \n",
      "                                                                 dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 24)           4056        concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 24)           0           dense_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 192)          0           concatenate_49[0][0]             \n",
      "                                                                 dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 3)            579         concatenate_50[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 3816 samples, validate on 635 samples\n",
      "Epoch 1/200\n",
      "3816/3816 [==============================] - 1s 376us/step - loss: 0.3818 - val_loss: 0.3186\n",
      "Epoch 2/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.3083 - val_loss: 0.3231\n",
      "Epoch 3/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2991 - val_loss: 0.3127\n",
      "Epoch 4/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2946 - val_loss: 0.3115\n",
      "Epoch 5/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2855 - val_loss: 0.3074\n",
      "Epoch 6/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2866 - val_loss: 0.3092\n",
      "Epoch 7/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2849 - val_loss: 0.3096\n",
      "Epoch 8/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2803 - val_loss: 0.3128\n",
      "Epoch 9/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2790 - val_loss: 0.3100\n",
      "Epoch 10/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2797 - val_loss: 0.3113\n",
      "Epoch 11/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2760 - val_loss: 0.3083\n",
      "Epoch 12/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2743 - val_loss: 0.3078\n",
      "Epoch 13/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2719 - val_loss: 0.3069\n",
      "Epoch 14/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2726 - val_loss: 0.3064\n",
      "Epoch 15/200\n",
      "3816/3816 [==============================] - 1s 157us/step - loss: 0.2697 - val_loss: 0.3065\n",
      "Epoch 16/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2706 - val_loss: 0.3083\n",
      "Epoch 17/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2658 - val_loss: 0.3068\n",
      "Epoch 18/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2689 - val_loss: 0.3049\n",
      "Epoch 19/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2692 - val_loss: 0.3068\n",
      "Epoch 20/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2672 - val_loss: 0.3054\n",
      "Epoch 21/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2674 - val_loss: 0.3062\n",
      "Epoch 22/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2671 - val_loss: 0.3060\n",
      "Epoch 23/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2647 - val_loss: 0.3066\n",
      "Epoch 24/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2631 - val_loss: 0.3054\n",
      "Epoch 25/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2648 - val_loss: 0.3064\n",
      "Epoch 26/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2631 - val_loss: 0.3078\n",
      "Epoch 27/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2637 - val_loss: 0.3090\n",
      "Epoch 28/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2651 - val_loss: 0.3074\n",
      "Epoch 29/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2610 - val_loss: 0.3076\n",
      "Epoch 30/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2573 - val_loss: 0.3079\n",
      "Epoch 31/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2628 - val_loss: 0.3071\n",
      "Epoch 32/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2595 - val_loss: 0.3074\n",
      "Epoch 33/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2605 - val_loss: 0.3069\n",
      "Epoch 34/200\n",
      "3816/3816 [==============================] - 1s 157us/step - loss: 0.2591 - val_loss: 0.3069\n",
      "Epoch 35/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2591 - val_loss: 0.3069\n",
      "Epoch 36/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2584 - val_loss: 0.3071\n",
      "Epoch 37/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2577 - val_loss: 0.3073\n",
      "Epoch 38/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2573 - val_loss: 0.3071\n",
      "Epoch 39/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2584 - val_loss: 0.3070\n",
      "Epoch 40/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2590 - val_loss: 0.3072\n",
      "Epoch 41/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2591 - val_loss: 0.3073\n",
      "Epoch 42/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2569 - val_loss: 0.3077\n",
      "Epoch 43/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2577 - val_loss: 0.3075\n",
      "Epoch 44/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2571 - val_loss: 0.3076\n",
      "Epoch 45/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2563 - val_loss: 0.3072\n",
      "Epoch 46/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2573 - val_loss: 0.3076\n",
      "Epoch 47/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2588 - val_loss: 0.3079\n",
      "Epoch 48/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2575 - val_loss: 0.3078\n",
      "Epoch 49/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2607 - val_loss: 0.3077\n",
      "Epoch 50/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2581 - val_loss: 0.3080\n",
      "Epoch 51/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2570 - val_loss: 0.3079\n",
      "Epoch 52/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2586 - val_loss: 0.3077\n",
      "Epoch 53/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2565 - val_loss: 0.3079\n",
      "Epoch 54/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2576 - val_loss: 0.3083\n",
      "Epoch 55/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2566 - val_loss: 0.3084\n",
      "Epoch 56/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2575 - val_loss: 0.3085\n",
      "Epoch 57/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2554 - val_loss: 0.3082\n",
      "Epoch 58/200\n",
      "3816/3816 [==============================] - 1s 155us/step - loss: 0.2570 - val_loss: 0.3085\n",
      "Epoch 59/200\n",
      "3816/3816 [==============================] - 1s 156us/step - loss: 0.2564 - val_loss: 0.3085\n",
      "Epoch 60/200\n",
      "3816/3816 [==============================] - 1s 171us/step - loss: 0.2569 - val_loss: 0.3081\n",
      "Epoch 61/200\n",
      "3816/3816 [==============================] - 1s 161us/step - loss: 0.2586 - val_loss: 0.3082\n",
      "Epoch 62/200\n",
      "3816/3816 [==============================] - 1s 153us/step - loss: 0.2560 - val_loss: 0.3079\n",
      "Epoch 63/200\n",
      "3816/3816 [==============================] - 1s 154us/step - loss: 0.2554 - val_loss: 0.3078\n",
      "Epoch 64/200\n",
      "3816/3816 [==============================] - 1s 169us/step - loss: 0.2581 - val_loss: 0.3076\n",
      "Epoch 65/200\n",
      "3816/3816 [==============================] - 1s 168us/step - loss: 0.2543 - val_loss: 0.3076\n",
      "Epoch 66/200\n",
      "3816/3816 [==============================] - 1s 168us/step - loss: 0.2563 - val_loss: 0.3077\n",
      "Epoch 67/200\n",
      "3816/3816 [==============================] - 1s 169us/step - loss: 0.2585 - val_loss: 0.3074\n",
      "Epoch 68/200\n",
      "3816/3816 [==============================] - 1s 169us/step - loss: 0.2549 - val_loss: 0.3074\n",
      "------------------------------\n",
      "CV mean score: 0.2788, std: 0.0288.\n",
      "[0.2715094511251389, 0.28176234808381434, 0.3015015890767634, 0.2545801950999354, 0.22538418597128074, 0.31225999429493956, 0.30485192240164505]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Training and cross-validation\n",
    "folds = KFold(n_splits=7, shuffle=True, random_state=3)\n",
    "scores = []\n",
    "oof = np.zeros_like(train_y)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(train_x)):\n",
    "    X_tr, X_val = train_x[train_index], train_x[valid_index]\n",
    "    Y_tr, Y_val = train_y[train_index], train_y[valid_index]\n",
    "    \n",
    "    classif_model = build_mlp_model([train_x.shape[-1]])\n",
    "    classif_model.compile(optimizer=Adam(lr=1e-3), loss=\"categorical_crossentropy\")\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=5e-5),\n",
    "                kc.ModelCheckpoint('stage_1_finals/nn_ensemble_checkpoint_f{}.pt'.format(fold_n), monitor='val_loss', verbose=0, save_best_only=True, mode='min')]\n",
    "    # train the model\n",
    "    classif_model.fit(x=X_tr, y=Y_tr, epochs=200, batch_size=32, \n",
    "                      callbacks=callbacks, validation_data=(X_val, Y_val), verbose=1)   \n",
    "    \n",
    "    pred_valid = classif_model.predict(x=X_val, verbose=0)\n",
    "    oof[valid_index] = pred_valid\n",
    "\n",
    "    # oof[valid_index] = pred_valid.reshape(-1,)\n",
    "    scores.append(log_loss(Y_val, pred_valid))\n",
    "    \n",
    "print(\"-\" * 30)\n",
    "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "print(scores)\n",
    "print(\"-\" * 30)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
