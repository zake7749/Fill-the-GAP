{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission_stage_1.csv', 'test_stage_1.tsv', 'trees']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"data\"))\n",
    "import zipfile\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from keras import backend, models, layers, initializers, regularizers, constraints, optimizers\n",
    "from keras import callbacks as kc\n",
    "from keras import optimizers as ko\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import time\n",
    "\n",
    "\n",
    "dense_layer_sizes = [37]\n",
    "dropout_rate = 0.6\n",
    "learning_rate = 0.001\n",
    "n_fold = 5\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "patience = 100\n",
    "# n_test = 100\n",
    "lambd = 0.1 # L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp_model_for_base_bert(input_shape, split_size):\n",
    "    X_input = layers.Input(input_shape)\n",
    "\n",
    "    # First dense layer\n",
    "    A = layers.Lambda(lambda x: x[:, :split_size])(X_input)\n",
    "    B = layers.Lambda(lambda x: x[:, split_size:split_size*2])(X_input)\n",
    "    P = layers.Lambda(lambda x: x[:, split_size*2:])(X_input)\n",
    "\n",
    "    head_num = 6\n",
    "    res = []\n",
    "    for head in range(head_num):\n",
    "        query_encoder = layers.Dense(dense_layer_sizes[0], activation='selu', kernel_regularizer = regularizers.l2(1e-6))\n",
    "        ans_encoder = layers.Dense(dense_layer_sizes[0], activation='selu', kernel_regularizer = regularizers.l2(1e-6))\n",
    "        \n",
    "        d_ratio = 0.6\n",
    "        \n",
    "        a, b = query_encoder(layers.Dropout(d_ratio)(A)), query_encoder(layers.Dropout(d_ratio)(B))\n",
    "        p = query_encoder(layers.Dropout(d_ratio)(P))\n",
    "        \n",
    "        amp = layers.Multiply()([a, p])\n",
    "        bmp = layers.Multiply()([b, p])\n",
    "        \n",
    "        asp = layers.Lambda(lambda v: v[0] - v[1])([p, a])\n",
    "        bsp = layers.Lambda(lambda v: v[0] - v[1])([p, b])        \n",
    "        \n",
    "        ia = layers.Concatenate()([a, p, amp, asp])\n",
    "        ib = layers.Concatenate()([b, p, bmp, bsp])\n",
    "        nli_encoder = layers.Dense(dense_layer_sizes[0], activation='selu')\n",
    "        ia, ib = nli_encoder(ia), nli_encoder(ib)\n",
    "        \n",
    "        out = layers.Concatenate()([ia, ib])\n",
    "        res.append(out)\n",
    "    \n",
    "    res = layers.Add()(res)\n",
    "    res = layers.Dropout(0.8)(res)\n",
    "    X = layers.Dense(dense_layer_sizes[0], name = 'dense0')(res)\n",
    "    X = layers.BatchNormalization(name = 'bn0')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.Dropout(dropout_rate, seed = 7)(X)\n",
    "\n",
    "    # Output layer\n",
    "    X = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n",
    "    X = layers.Activation('softmax')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(input = X_input, output = X, name = \"classif_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n------------------------------\\nFor the model bert-large-uncased-seq256-19\\nCV mean score: 0.3441, std: 0.0215.\\n[0.3307294825242308, 0.3187784074490988, 0.34923320252765083, 0.3821268940718645, 0.3397000497226646]\\nTest score: 0.32621788223567494\\n------------------------------\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_mlp_model_for_larget_bert(input_shape, split_size):\n",
    "    X_input = layers.Input(input_shape)\n",
    "\n",
    "    # First dense layer\n",
    "    A = layers.Lambda(lambda x: x[:, :split_size])(X_input)\n",
    "    B = layers.Lambda(lambda x: x[:, split_size:split_size*2])(X_input)\n",
    "    P = layers.Lambda(lambda x: x[:, split_size*2:])(X_input)\n",
    "\n",
    "    head_num = 6\n",
    "    res = []\n",
    "    for head in range(head_num):\n",
    "        query_encoder = layers.Dense(dense_layer_sizes[0], activation='selu', kernel_regularizer = regularizers.l2(1e-6))\n",
    "        ans_encoder = layers.Dense(dense_layer_sizes[0], activation='selu', kernel_regularizer = regularizers.l2(1e-6))\n",
    "        \n",
    "        d_ratio = 0.7\n",
    "        \n",
    "        a, b = query_encoder(layers.Dropout(d_ratio)(A)), query_encoder(layers.Dropout(d_ratio)(B))\n",
    "        p = query_encoder(layers.Dropout(d_ratio)(P))\n",
    "        \n",
    "        amp = layers.Multiply()([a, p])\n",
    "        bmp = layers.Multiply()([b, p])\n",
    "        \n",
    "        asp = layers.Lambda(lambda v: v[0] - v[1])([p, a])\n",
    "        bsp = layers.Lambda(lambda v: v[0] - v[1])([p, b])        \n",
    "        \n",
    "        ia = layers.Concatenate()([a, p, amp, asp])\n",
    "        ib = layers.Concatenate()([b, p, bmp, bsp])\n",
    "        nli_encoder = layers.Dense(dense_layer_sizes[0], activation='selu')\n",
    "        ia, ib = nli_encoder(ia), nli_encoder(ib)\n",
    "        \n",
    "        out = layers.Concatenate()([ia, ib])\n",
    "        res.append(out)\n",
    "    \n",
    "    res = layers.Concatenate()(res)\n",
    "    res = layers.Dropout(0.85)(res)\n",
    "    X = layers.Dense(dense_layer_sizes[0], name = 'dense0')(res)\n",
    "    X = layers.BatchNormalization(name = 'bn0')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.Dropout(dropout_rate, seed = 7)(X)\n",
    "    \n",
    "    # Output layer\n",
    "    X = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n",
    "    X = layers.Activation('softmax')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(input = X_input, output = X, name = \"classif_model\")\n",
    "    return model\n",
    "\n",
    "'''\n",
    "------------------------------\n",
    "For the model bert-large-uncased-seq256-19\n",
    "CV mean score: 0.3441, std: 0.0215.\n",
    "[0.3307294825242308, 0.3187784074490988, 0.34923320252765083, 0.3821268940718645, 0.3397000497226646]\n",
    "Test score: 0.32621788223567494\n",
    "------------------------------\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def base_nli_model(input_shape, split_size):\n",
    "    X_input = layers.Input(input_shape)\n",
    "\n",
    "    # First dense layer\n",
    "    A = layers.Lambda(lambda x: x[:, :split_size])(X_input)\n",
    "    B = layers.Lambda(lambda x: x[:, split_size:split_size*2])(X_input)\n",
    "    P = layers.Lambda(lambda x: x[:, split_size*2:])(X_input)\n",
    "\n",
    "    X1 = layers.Dropout(0.6, seed = 7)(A)\n",
    "    X2 = layers.Dropout(0.6, seed = 7)(B)\n",
    "    Y = layers.Dropout(0.6, seed = 7)(P)    \n",
    "    \n",
    "    def interaction(a, b):\n",
    "        sub = layers.Lambda(lambda a: K.abs(a[0] - a[1]))([a, b])\n",
    "        mult = layers.Lambda(lambda a: a[0] * a[1])([a, b])\n",
    "        return layers.Concatenate()([a, b, sub, mult,])    \n",
    "    \n",
    "    word_encoder = layers.Dense(512, activation='selu')\n",
    "    X1 = word_encoder(X1)\n",
    "    X2 = word_encoder(X2)\n",
    "    Y = word_encoder(Y)\n",
    "\n",
    "    I_X1_Y = interaction(X1, Y)\n",
    "    I_X2_Y = interaction(X2, Y)\n",
    "\n",
    "    dense_encoder = layers.Dense(128, activation='selu')\n",
    "\n",
    "    I_X1_Y = layers.Dropout(0.75)(dense_encoder(I_X1_Y))\n",
    "    I_X2_Y = layers.Dropout(0.75)(dense_encoder(I_X2_Y))\n",
    "    features = layers.Concatenate()([I_X1_Y, I_X2_Y])\n",
    "\n",
    "    X = layers.Dense(dense_layer_sizes[0], name = 'dense0')(features)\n",
    "    X = layers.BatchNormalization(name = 'bn0')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.Dropout(dropout_rate, seed = 7)(X)\n",
    "\n",
    "    # Output layer\n",
    "    X = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n",
    "    X = layers.Activation('softmax')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(input = X_input, output = X, name = \"classif_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "0ea0ac2603b0a4bbdaa2776c8e37ad7894a99f5a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_json(embeddings, embedding_size):\n",
    "    '''\n",
    "    Parses the embeddigns given by BERT, and suitably formats them to be passed to the MLP model\n",
    "\n",
    "    Input: embeddings, a DataFrame containing contextual embeddings from BERT, as well as the labels for the classification problem\n",
    "    columns: \"emb_A\": contextual embedding for the word A\n",
    "             \"emb_B\": contextual embedding for the word B\n",
    "             \"emb_P\": contextual embedding for the pronoun\n",
    "             \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n",
    "\n",
    "    Output: X, a numpy array containing, for each line in the GAP file, the concatenation of the embeddings of the target words\n",
    "            Y, a numpy array containing, for each line in the GAP file, the one-hot encoded answer to the coreference problem\n",
    "    '''\n",
    "    embeddings.sort_index(inplace = True) # Sorting the DataFrame, because reading from the json file messed with the order\n",
    "    X = np.zeros((len(embeddings),3* embedding_size))\n",
    "    Y = np.zeros((len(embeddings), 3))\n",
    "\n",
    "    # Concatenate features\n",
    "    for i in range(len(embeddings)):\n",
    "        A = np.array(embeddings.loc[i,\"emb_A\"])\n",
    "        B = np.array(embeddings.loc[i,\"emb_B\"])\n",
    "        P = np.array(embeddings.loc[i,\"emb_P\"])\n",
    "        X[i] = np.concatenate((A,B,P))\n",
    "\n",
    "    # One-hot encoding for labels\n",
    "    for i in range(len(embeddings)):\n",
    "        label = embeddings.loc[i,\"label\"]\n",
    "        if label == \"A\":\n",
    "            Y[i,0] = 1\n",
    "        elif label == \"B\":\n",
    "            Y[i,1] = 1\n",
    "        else:\n",
    "            Y[i,2] = 1\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_filename = \"contextual_embeddings_gap_development.json\"\n",
    "val_filename = \"contextual_embeddings_gap_validation.json\"\n",
    "test_filename = \"contextual_embeddings_gap_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep all BERTs learn on the same corpus\n",
    "remove_test = []\n",
    "remove_validation = []\n",
    "remove_development = [209, 1506, 1988]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_tag, model_func,\n",
    "                dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "                oof_folder='oof/', pred_folder='outputs/'):\n",
    "    '''\n",
    "    Arguments:\n",
    "        tag (data_tag): the tag of model and vec\n",
    "        embedding_size: the size of bert embedding\n",
    "        layer: to indicate which layer of bert is going to use\n",
    "        checkpoint_path: the path to model_checkpoint folder\n",
    "        n_fold: the number of CV folds\n",
    "        model_tag: the prefix of the prediction and oof file and model chekpoint\n",
    "    '''\n",
    "\n",
    "    tag = tag + str(layer) # follow the original naming style\n",
    "    dev_filename = tag + dev_filename\n",
    "    val_filename = tag + val_filename\n",
    "    test_filename = tag + test_filename\n",
    "    \n",
    "    development = pd.read_json(os.path.join(dev_folder_path, dev_filename))\n",
    "    X_development, Y_development = parse_json(development, embedding_size)\n",
    "\n",
    "    validation = pd.read_json(os.path.join(val_folder_path, val_filename))\n",
    "    X_validation, Y_validation = parse_json(validation, embedding_size)\n",
    "\n",
    "    test = pd.read_json(os.path.join(test_folder_path, test_filename))\n",
    "    X_test, Y_test = parse_json(test, embedding_size)\n",
    "\n",
    "    # There may be a few NaN values, where the offset of a target word is greater than the max_seq_length of BERT.\n",
    "    # They are very few, so I'm just dropping the rows.\n",
    "    # remove_test = [row for row in range(len(X_test)) if np.sum(np.isnan(X_test[row].reshape(-1)))]\n",
    "    X_test = np.delete(X_test, remove_test, 0)\n",
    "    Y_test = np.delete(Y_test, remove_test, 0)\n",
    "\n",
    "    # remove_validation = [row for row in range(len(X_validation)) if np.sum(np.isnan(X_validation[row].reshape(-1)))]\n",
    "    X_validation = np.delete(X_validation, remove_validation, 0)\n",
    "    Y_validation = np.delete(Y_validation, remove_validation, 0)\n",
    "\n",
    "    # We want predictions for all development rows. So instead of removing rows, make them 0\n",
    "    # remove_development = [row for row in range(len(X_development)) if np.sum(np.isnan(X_development[row].reshape(-1)))]\n",
    "    X_development = np.delete(X_development, remove_development, 0)\n",
    "    Y_development = np.delete(Y_development, remove_development, 0)\n",
    "    \n",
    "    # Will train on data from the gap-test and gap-validation files, in total 2454 rows\n",
    "    X_train = np.concatenate((X_test, X_validation, X_development), axis = 0)\n",
    "    Y_train = np.concatenate((Y_test, Y_validation, Y_development), axis = 0)\n",
    "\n",
    "    # Will predict probabilities for data from the gap-development file; initializing the predictions\n",
    "    prediction = np.zeros((len(X_development), 3)) # testing predictions\n",
    "\n",
    "    # Training and cross-validation\n",
    "    folds = KFold(n_splits=n_fold, shuffle=True, random_state=3)\n",
    "    scores = []\n",
    "    oof = np.zeros_like(Y_train)\n",
    "    \n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train)):\n",
    "        # split training and validation data\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_tr, X_val = X_train[train_index], X_train[valid_index]\n",
    "        Y_tr, Y_val = Y_train[train_index], Y_train[valid_index]\n",
    "\n",
    "        # Define the model, re-initializing for each fold\n",
    "        classif_model = model_func([X_train.shape[-1]], split_size=embedding_size)\n",
    "        classif_model.compile(optimizer=optimizers.Adam(lr=learning_rate), loss=\"categorical_crossentropy\")\n",
    "        \n",
    "        callbacks = [kc.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "                     kc.ModelCheckpoint(os.path.join(checkpoint_path, model_tag + tag + str(fold_n) + '.pt'), monitor='val_loss', verbose=0, save_best_only=True, mode='min')]\n",
    "\n",
    "        # train the model\n",
    "        classif_model.fit(x=X_tr, y=Y_tr, epochs=epochs, batch_size=batch_size, \n",
    "                          callbacks=callbacks, validation_data=(X_val, Y_val), verbose=0)\n",
    "\n",
    "        # make predictions on validation and test data\n",
    "        pred_valid = classif_model.predict(x=X_val, verbose=0)\n",
    "        oof[valid_index] = pred_valid\n",
    "        pred = classif_model.predict(x=X_development, verbose=0)\n",
    "\n",
    "        # oof[valid_index] = pred_valid.reshape(-1,)\n",
    "        scores.append(log_loss(Y_val, pred_valid))\n",
    "        prediction += pred\n",
    "    \n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"For the model\", tag)\n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    print(scores)\n",
    "    \n",
    "    # Write the prediction to file for submission\n",
    "    oof_df = pd.DataFrame(oof)\n",
    "    oof_df.to_csv(oof_folder + model_tag + tag + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Base\n",
    "## Uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_base\"\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 7\n",
    "tag = \"bert-base-uncased-seq512-\"\n",
    "pred_tag = \"nli-mh-\"\n",
    "model_func = build_mlp_model_for_base_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Apr 15 15:06:04 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Apr 15 15:07:32 2019\n",
      "Fold 2 started at Mon Apr 15 15:09:23 2019\n",
      "Fold 3 started at Mon Apr 15 15:11:10 2019\n",
      "Fold 4 started at Mon Apr 15 15:13:08 2019\n",
      "Fold 5 started at Mon Apr 15 15:14:59 2019\n",
      "Fold 6 started at Mon Apr 15 15:16:51 2019\n",
      "------------------------------\n",
      "For the model bert-base-uncased-seq512-8\n",
      "CV mean score: 0.3897, std: 0.0303.\n",
      "[0.41874208962182374, 0.3576011612451981, 0.42278881478925096, 0.35695556738233025, 0.35123012553249394, 0.4055829368912067, 0.414897930121091]\n"
     ]
    }
   ],
   "source": [
    "train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='outputs/', model_tag=pred_tag, model_func=model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Apr 15 15:19:20 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Apr 15 15:20:01 2019\n",
      "Fold 2 started at Mon Apr 15 15:20:33 2019\n",
      "Fold 3 started at Mon Apr 15 15:21:09 2019\n",
      "Fold 4 started at Mon Apr 15 15:21:44 2019\n",
      "Fold 5 started at Mon Apr 15 15:22:24 2019\n",
      "Fold 6 started at Mon Apr 15 15:23:05 2019\n",
      "------------------------------\n",
      "For the model bert-base-uncased-seq512-8\n",
      "CV mean score: 0.3894, std: 0.0345.\n",
      "[0.43335572715629767, 0.36244312029276277, 0.4074310181190556, 0.35165307776648813, 0.3416434165458056, 0.400636516729902, 0.42857470172439066]\n"
     ]
    }
   ],
   "source": [
    "pred_tag = \"bnli-mh-\"\n",
    "model_func = base_nli_model\n",
    "train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='outputs/', model_tag=pred_tag, model_func=model_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_base_cased\"\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 7\n",
    "tag = \"bert-base-cased-seq512-\"\n",
    "pred_tag = \"nli-mh-\"\n",
    "model_func = build_mlp_model_for_base_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Apr 15 15:23:53 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Apr 15 15:25:59 2019\n",
      "Fold 2 started at Mon Apr 15 15:28:36 2019\n",
      "Fold 3 started at Mon Apr 15 15:30:33 2019\n",
      "Fold 4 started at Mon Apr 15 15:32:45 2019\n",
      "Fold 5 started at Mon Apr 15 15:35:03 2019\n",
      "Fold 6 started at Mon Apr 15 15:37:19 2019\n",
      "------------------------------\n",
      "For the model bert-base-cased-seq512-8\n",
      "CV mean score: 0.4394, std: 0.0275.\n",
      "[0.4649052174103669, 0.4075077879509842, 0.4420038184371437, 0.4103142511106075, 0.41890867841885787, 0.4445338689644511, 0.48744606034038224]\n"
     ]
    }
   ],
   "source": [
    "train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='outputs/', model_tag=pred_tag, model_func=model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Apr 15 15:39:28 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Apr 15 15:40:18 2019\n",
      "Fold 2 started at Mon Apr 15 15:41:02 2019\n",
      "Fold 3 started at Mon Apr 15 15:41:52 2019\n",
      "Fold 4 started at Mon Apr 15 15:42:47 2019\n",
      "Fold 5 started at Mon Apr 15 15:43:39 2019\n",
      "Fold 6 started at Mon Apr 15 15:44:31 2019\n",
      "------------------------------\n",
      "For the model bert-base-cased-seq512-8\n",
      "CV mean score: 0.4412, std: 0.0335.\n",
      "[0.4671664478725156, 0.40499301248086533, 0.44609084672334093, 0.40980316669044653, 0.4086470947931843, 0.44959538845483027, 0.5023763490928792]\n"
     ]
    }
   ],
   "source": [
    "pred_tag = \"bnli-mh-\"\n",
    "model_func = base_nli_model\n",
    "train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='outputs/', model_tag=pred_tag, model_func=model_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Big\n",
    "## Uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_big\"\n",
    "embedding_size = 1024\n",
    "layer = 19\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 7\n",
    "tag = \"bert-large-uncased-seq300-\"\n",
    "pred_tag = \"nli-mh-\"\n",
    "model_func = build_mlp_model_for_larget_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Apr 15 15:45:25 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Apr 15 15:47:50 2019\n",
      "Fold 2 started at Mon Apr 15 15:49:59 2019\n",
      "Fold 3 started at Mon Apr 15 15:51:58 2019\n",
      "Fold 4 started at Mon Apr 15 15:54:53 2019\n",
      "Fold 5 started at Mon Apr 15 15:57:54 2019\n",
      "Fold 6 started at Mon Apr 15 15:59:56 2019\n",
      "------------------------------\n",
      "For the model bert-large-uncased-seq300-19\n",
      "CV mean score: 0.3196, std: 0.0333.\n",
      "[0.3042907009121622, 0.31883628042389156, 0.34259295103589726, 0.28846384482261234, 0.2685273202696522, 0.34067832038491824, 0.3739988215781444]\n"
     ]
    }
   ],
   "source": [
    "train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='outputs/', model_tag=pred_tag, model_func=model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Apr 15 16:02:23 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Apr 15 16:03:16 2019\n",
      "Fold 2 started at Mon Apr 15 16:04:08 2019\n",
      "Fold 3 started at Mon Apr 15 16:05:11 2019\n",
      "Fold 4 started at Mon Apr 15 16:06:09 2019\n",
      "Fold 5 started at Mon Apr 15 16:07:20 2019\n",
      "Fold 6 started at Mon Apr 15 16:08:26 2019\n",
      "------------------------------\n",
      "For the model bert-large-uncased-seq300-19\n",
      "CV mean score: 0.3183, std: 0.0395.\n",
      "[0.30168902294112826, 0.3380411308095498, 0.34800934412409756, 0.285086517611375, 0.2506359343848238, 0.32666263570260395, 0.37827246434799594]\n"
     ]
    }
   ],
   "source": [
    "pred_tag = \"bnli-mh-\"\n",
    "model_func = base_nli_model\n",
    "train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='outputs/', model_tag=pred_tag, model_func=model_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_big_cased\"\n",
    "embedding_size = 1024\n",
    "layer = 18\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 7\n",
    "tag = \"bert-large-cased-seq300-\"\n",
    "pred_tag = \"nli-mh-\"\n",
    "model_func = build_mlp_model_for_larget_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Apr 15 16:09:28 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Apr 15 16:12:38 2019\n",
      "Fold 2 started at Mon Apr 15 16:16:02 2019\n",
      "Fold 3 started at Mon Apr 15 16:19:01 2019\n",
      "Fold 4 started at Mon Apr 15 16:22:36 2019\n",
      "Fold 5 started at Mon Apr 15 16:26:58 2019\n",
      "Fold 6 started at Mon Apr 15 16:29:58 2019\n",
      "------------------------------\n",
      "For the model bert-large-cased-seq300-18\n",
      "CV mean score: 0.3592, std: 0.0317.\n",
      "[0.3704772674046243, 0.3360922776462632, 0.3821525974518586, 0.3268818250513997, 0.31075084876940323, 0.38929441007353516, 0.398596004242633]\n"
     ]
    }
   ],
   "source": [
    "train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='outputs/', model_tag=pred_tag, model_func=model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Apr 15 16:33:04 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Apr 15 16:34:24 2019\n",
      "Fold 2 started at Mon Apr 15 16:35:46 2019\n",
      "Fold 3 started at Mon Apr 15 16:36:55 2019\n",
      "Fold 4 started at Mon Apr 15 16:38:18 2019\n",
      "Fold 5 started at Mon Apr 15 16:39:37 2019\n",
      "Fold 6 started at Mon Apr 15 16:41:06 2019\n",
      "------------------------------\n",
      "For the model bert-large-cased-seq300-18\n",
      "CV mean score: 0.3593, std: 0.0312.\n",
      "[0.38562625229912195, 0.35086932665420956, 0.3764865739071894, 0.33176049842435495, 0.30063397475827996, 0.3739431194782421, 0.3960768230630664]\n"
     ]
    }
   ],
   "source": [
    "pred_tag = \"bnli-mh-\"\n",
    "model_func = base_nli_model\n",
    "train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='outputs/', model_tag=pred_tag, model_func=model_func)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
