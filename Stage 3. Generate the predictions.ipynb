{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission_stage_1.csv', 'test_stage_1.tsv', 'trees']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"data\"))\n",
    "import zipfile\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from keras import backend, models, layers, initializers, regularizers, constraints, optimizers\n",
    "from keras import callbacks as kc\n",
    "from keras import optimizers as ko\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate BERT embeddings for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission_stage_1.csv', 'test_stage_1.tsv', 'trees']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"data\"))\n",
    "import zipfile\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import modeling\n",
    "import extract_features\n",
    "import tokenization\n",
    "import tensorflow as tf\n",
    "\n",
    "def compute_offset_no_spaces(text, offset):\n",
    "    count = 0\n",
    "    for pos in range(offset):\n",
    "        if text[pos] != \" \": count +=1\n",
    "    return count\n",
    "\n",
    "def count_chars_no_special(text):\n",
    "    count = 0\n",
    "    special_char_list = [\"#\"]\n",
    "    for pos in range(len(text)):\n",
    "        if text[pos] not in special_char_list: count +=1\n",
    "    return count\n",
    "\n",
    "def count_length_no_special(text):\n",
    "    count = 0\n",
    "    special_char_list = [\"#\", \" \"]\n",
    "    for pos in range(len(text)):\n",
    "        if text[pos] not in special_char_list: count +=1\n",
    "    return count\n",
    "\n",
    "def run_bert(data, output, os_command, embedding_size=1024, layer=-1):\n",
    "    '''\n",
    "    Runs a forward propagation of BERT on input text, extracting contextual word embeddings\n",
    "    Input: data, a pandas DataFrame containing the information in one of the GAP files\n",
    "\n",
    "    Output: emb, a pandas DataFrame containing contextual embeddings for the words A, B and Pronoun. Each embedding is a numpy array of shape (768)\n",
    "    columns: \"emb_A\": the embedding for word A\n",
    "             \"emb_B\": the embedding for word B\n",
    "             \"emb_P\": the embedding for the pronoun\n",
    "             \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n",
    "    '''\n",
    "    # From the current file, take the text only, and write it in a file which will be passed to BERT\n",
    "    text = data[\"Text\"]\n",
    "    text.to_csv(\"input.txt\", index = False, header = False)\n",
    "\n",
    "    # The script extract_features.py runs forward propagation through BERT, and writes the output in the file output.jsonl\n",
    "    # I'm lazy, so I'm only saving the output of the last layer. Feel free to change --layers = -1 to save the output of other layers.\n",
    "\n",
    "    res = os.system(os_command)\n",
    "    bert_output = pd.read_json(\"output.jsonl\", lines = True)\n",
    "\n",
    "    os.system(\"rm output.jsonl\")\n",
    "    os.system(\"rm input.txt\")\n",
    "\n",
    "    index = data.index\n",
    "    columns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n",
    "    emb = pd.DataFrame(index = index, columns = columns)\n",
    "    emb.index.name = \"ID\"\n",
    "\n",
    "    for i in range(len(data)): # For each line in the data file\n",
    "        # get the words A, B, Pronoun. Convert them to lower case, since we're using the uncased version of BERT\n",
    "        P = data.loc[i,\"Pronoun\"].lower()\n",
    "        A = data.loc[i,\"A\"].lower()\n",
    "        B = data.loc[i,\"B\"].lower()\n",
    "\n",
    "        # For each word, find the offset not counting spaces. This is necessary for comparison with the output of BERT\n",
    "        P_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"Pronoun-offset\"])\n",
    "        A_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"A-offset\"])\n",
    "        B_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"B-offset\"])\n",
    "        # Figure out the length of A, B, not counting spaces or special characters\n",
    "        A_length = count_length_no_special(A)\n",
    "        B_length = count_length_no_special(B)\n",
    "\n",
    "        # Initialize embeddings with zeros\n",
    "        emb_A = np.zeros(embedding_size)\n",
    "        emb_B = np.zeros(embedding_size)\n",
    "        emb_P = np.zeros(embedding_size)\n",
    "\n",
    "        # Initialize counts\n",
    "        count_chars = 0\n",
    "        cnt_A, cnt_B, cnt_P = 0, 0, 0\n",
    "\n",
    "        features = pd.DataFrame(bert_output.loc[i,\"features\"]) # Get the BERT embeddings for the current line in the data file\n",
    "        for j in range(2,len(features)):  # Iterate over the BERT tokens for the current line; we skip over the first 2 tokens, which don't correspond to words\n",
    "            token = features.loc[j,\"token\"]\n",
    "\n",
    "            # See if the character count until the current token matches the offset of any of the 3 target words\n",
    "            if count_chars  == P_offset: \n",
    "                # print(token)\n",
    "                emb_P += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "                cnt_P += 1\n",
    "            if count_chars in range(A_offset, A_offset + A_length): \n",
    "                # print(token)\n",
    "                emb_A += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "                cnt_A +=1\n",
    "            if count_chars in range(B_offset, B_offset + B_length): \n",
    "                # print(token)\n",
    "                emb_B += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "                cnt_B +=1\t\t\t\t\t\t\t\t\n",
    "            # Update the character count\n",
    "            count_chars += count_length_no_special(token)\n",
    "        # Taking the average between tokens in the span of A or B, so divide the current value by the count\t\n",
    "        emb_A /= cnt_A\n",
    "        emb_B /= cnt_B\n",
    "\n",
    "        # Work out the label of the current piece of text\n",
    "        label = \"Neither\"\n",
    "        if (data.loc[i,\"A-coref\"] == True):\n",
    "            label = \"A\"\n",
    "        if (data.loc[i,\"B-coref\"] == True):\n",
    "            label = \"B\"\n",
    "\n",
    "        # Put everything together in emb\n",
    "        emb.iloc[i] = [emb_A, emb_B, emb_P, label]\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert uncased base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-22ef08767cff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gap-development.tsv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m test_emb = run_bert(test_data, embedding_size=embedding_size, os_command=os_command,\n\u001b[1;32m----> 9\u001b[1;33m                     output=\"{}contextual_embeddings_gap_test.json\".format(tag), layer=layer)\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtest_emb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vector/bert_base/{}contextual_embeddings_gap_second_stage_test.json\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'columns'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-b18f3ac64fa2>\u001b[0m in \u001b[0;36mrun_bert\u001b[1;34m(data, output, os_command, embedding_size, layer)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# I'm lazy, so I'm only saving the output of the last layer. Feel free to change --layers = -1 to save the output of other layers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_command\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[0mbert_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"output.jsonl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tag = 'bert-base-uncased-seq512-'\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "tag = tag + str(layer)\n",
    "os_command = \"python extract_features.py \\\n",
    "      --input_file=input.txt \\\n",
    "      --output_file=output.jsonl \\\n",
    "      --vocab_file=uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "      --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "      --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "      --layers={} \\\n",
    "      --max_seq_length=512 \\\n",
    "      --batch_size=1\".format(layer)\n",
    "\n",
    "test_data = pd.read_csv(\"gap-development.tsv\", sep = '\\t')\n",
    "test_emb = run_bert(test_data, embedding_size=embedding_size, os_command=os_command,\n",
    "                    output=\"{}contextual_embeddings_gap_test.json\".format(tag), layer=layer)\n",
    "test_emb.to_json(\"vector/bert_base/{}contextual_embeddings_gap_second_stage_test.json\".format(tag), orient = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert cased base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag = 'bert-base-cased-seq512-'\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "tag = tag + str(layer)\n",
    "os_command = \"python extract_features.py \\\n",
    "      --input_file=input.txt \\\n",
    "      --output_file=output.jsonl \\\n",
    "      --vocab_file=cased_L-12_H-768_A-12/vocab.txt \\\n",
    "      --bert_config_file=cased_L-12_H-768_A-12/bert_config.json \\\n",
    "      --init_checkpoint=cased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "      --layers={} \\\n",
    "      --max_seq_length=512 \\\n",
    "      --batch_size=1\".format(layer)\n",
    "\n",
    "test_data = pd.read_csv(\"gap-development.tsv\", sep = '\\t')\n",
    "test_emb = run_bert(test_data, embedding_size=embedding_size, os_command=os_command,\n",
    "                    output=\"{}contextual_embeddings_gap_test.json\".format(tag), layer=layer)\n",
    "test_emb.to_json(\"vector/bert_base_cased/{}contextual_embeddings_gap_second_stage_test.json\".format(tag), orient = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert uncased large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag = 'bert-large-uncased-seq300-'\n",
    "embedding_size = 1024\n",
    "layer = 19\n",
    "tag = tag + str(layer)\n",
    "os_command = \"python extract_features.py \\\n",
    "      --input_file=input.txt \\\n",
    "      --output_file=output.jsonl \\\n",
    "      --vocab_file=uncased_L-24_H-1024_A-16/vocab.txt \\\n",
    "      --bert_config_file=uncased_L-24_H-1024_A-16/bert_config.json \\\n",
    "      --init_checkpoint=uncased_L-24_H-1024_A-16/bert_model.ckpt \\\n",
    "      --layers={} \\\n",
    "      --max_seq_length=300 \\\n",
    "      --batch_size=1\".format(layer)\n",
    "\n",
    "test_data = pd.read_csv(\"gap-development.tsv\", sep = '\\t')\n",
    "test_emb = run_bert(test_data, embedding_size=embedding_size, os_command=os_command,\n",
    "                    output=\"{}contextual_embeddings_gap_test.json\".format(tag), layer=layer)\n",
    "test_emb.to_json(\"vector/bert_big/{}contextual_embeddings_gap_second_stage_test.json\".format(tag), orient = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert cased large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag = 'bert-large-cased-seq300-'\n",
    "embedding_size = 1024\n",
    "layer = 18\n",
    "tag = tag + str(layer)\n",
    "os_command = \"python extract_features.py \\\n",
    "      --input_file=input.txt \\\n",
    "      --output_file=output.jsonl \\\n",
    "      --vocab_file=cased_L-24_H-1024_A-16/vocab.txt \\\n",
    "      --bert_config_file=cased_L-24_H-1024_A-16/bert_config.json \\\n",
    "      --init_checkpoint=cased_L-24_H-1024_A-16/bert_model.ckpt \\\n",
    "      --layers={} \\\n",
    "      --max_seq_length=300 \\\n",
    "      --batch_size=1\".format(layer)\n",
    "\n",
    "test_data = pd.read_csv(\"gap-development.tsv\", sep = '\\t')\n",
    "test_emb = run_bert(test_data, embedding_size=embedding_size, os_command=os_command,\n",
    "                    output=\"{}contextual_embeddings_gap_test.json\".format(tag), layer=layer)\n",
    "test_emb.to_json(\"vector/bert_big_cased/{}contextual_embeddings_gap_second_stage_test.json\".format(tag), orient = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_json(embeddings, embedding_size):\n",
    "    '''\n",
    "    Parses the embeddigns given by BERT, and suitably formats them to be passed to the MLP model\n",
    "\n",
    "    Input: embeddings, a DataFrame containing contextual embeddings from BERT, as well as the labels for the classification problem\n",
    "    columns: \"emb_A\": contextual embedding for the word A\n",
    "             \"emb_B\": contextual embedding for the word B\n",
    "             \"emb_P\": contextual embedding for the pronoun\n",
    "             \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n",
    "\n",
    "    Output: X, a numpy array containing, for each line in the GAP file, the concatenation of the embeddings of the target words\n",
    "            Y, a numpy array containing, for each line in the GAP file, the one-hot encoded answer to the coreference problem\n",
    "    '''\n",
    "    embeddings.sort_index(inplace = True) # Sorting the DataFrame, because reading from the json file messed with the order\n",
    "    X = np.zeros((len(embeddings),3* embedding_size))\n",
    "    Y = np.zeros((len(embeddings), 3))\n",
    "\n",
    "    # Concatenate features\n",
    "    for i in range(len(embeddings)):\n",
    "        A = np.array(embeddings.loc[i,\"emb_A\"])\n",
    "        B = np.array(embeddings.loc[i,\"emb_B\"])\n",
    "        P = np.array(embeddings.loc[i,\"emb_P\"])\n",
    "        X[i] = np.concatenate((A,B,P))\n",
    "\n",
    "    # One-hot encoding for labels\n",
    "    for i in range(len(embeddings)):\n",
    "        label = embeddings.loc[i,\"label\"]\n",
    "        if label == \"A\":\n",
    "            Y[i,0] = 1\n",
    "        elif label == \"B\":\n",
    "            Y[i,1] = 1\n",
    "        else:\n",
    "            Y[i,2] = 1\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictons for stage 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_filename = \"contextual_embeddings_gap_development.json\"\n",
    "val_filename = \"contextual_embeddings_gap_validation.json\"\n",
    "test_filename = \"contextual_embeddings_gap_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_layer_sizes = [37]\n",
    "dropout_rate = 0.6\n",
    "learning_rate = 0.001\n",
    "n_fold = 5\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "patience = 100\n",
    "# n_test = 100\n",
    "lambd = 0.1 # L2 regularization\n",
    "\n",
    "def build_mlp_model(input_shape):\n",
    "\tX_input = layers.Input(input_shape)\n",
    "\tX = layers.Dropout(dropout_rate, seed = 7)(X_input)\n",
    "\t# First dense layer\n",
    "\tX = layers.Dense(dense_layer_sizes[0], name = 'dense0')(X)\n",
    "\tX = layers.BatchNormalization(name = 'bn0')(X)\n",
    "\tX = layers.Activation('relu')(X)\n",
    "\tX = layers.Dropout(dropout_rate, seed = 7)(X)\n",
    "\n",
    "\t# Output layer\n",
    "\tX = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n",
    "\tX = layers.Activation('softmax')(X)\n",
    "\n",
    "\t# Create model\n",
    "\tmodel = models.Model(input = X_input, output = X, name = \"classif_model\")\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_tag,\n",
    "                dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "                oof_folder='oof/', pred_folder='test_outputs/'):\n",
    "\n",
    "    tag = tag + str(layer) # follow the original naming style\n",
    "    dev_filename = tag + dev_filename\n",
    "    val_filename = tag + val_filename\n",
    "    test_filename = tag + test_filename\n",
    "    \n",
    "    development = pd.read_json(os.path.join(dev_folder_path, dev_filename))\n",
    "    X_development, Y_development = parse_json(development, embedding_size)\n",
    "\n",
    "    # We want predictions for all development rows. So instead of removing rows, make them 0\n",
    "    remove_development = [row for row in range(len(X_development)) if np.sum(np.isnan(X_development[row].reshape(-1)))]\n",
    "    X_development[remove_development] = np.zeros((3 * embedding_size))\n",
    "\n",
    "    prediction = np.zeros((len(X_development), 3)) # testing predictions\n",
    "\n",
    "    for fold_n in range(n_fold):\n",
    "        # split training and validation data\n",
    "        print('Make predictions for', fold_n, 'started at', time.ctime())\n",
    "        classif_model = build_mlp_model([X_development.shape[-1]])\n",
    "        classif_model.load_weights(os.path.join(checkpoint_path, model_tag + tag + str(fold_n) + '.pt'))\n",
    "\n",
    "        pred = classif_model.predict(x=X_development, verbose=0)\n",
    "        prediction += pred\n",
    "    \n",
    "    prediction /= n_fold\n",
    "    # prediction[remove_development] = [0.4, 0.4, 0.2] This is useless for prediction stage\n",
    "\n",
    "    submission = pd.read_csv(\"data/sample_submission_stage_1.csv\", index_col = \"ID\")\n",
    "    submission[\"A\"] = prediction[:,0]\n",
    "    submission[\"B\"] = prediction[:,1]\n",
    "    submission[\"NEITHER\"] = prediction[:,2]\n",
    "    submission.to_csv(pred_folder + model_tag + tag + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:15:31 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:15:32 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:15:32 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:15:32 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:15:33 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_base\"\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-base-uncased-seq512-\"\n",
    "pred_tag = \"naive-\"\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:15:34 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:15:34 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:15:34 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:15:34 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:15:35 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_base_cased\"\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-base-cased-seq512-\"\n",
    "pred_tag = \"naive-\"\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:15:36 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:15:36 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:15:37 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:15:37 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:15:37 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_big\"\n",
    "embedding_size = 1024\n",
    "layer = 19\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-large-uncased-seq300-\"\n",
    "pred_tag = \"naive-\"\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:15:39 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:15:39 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:15:39 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:15:40 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:15:40 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_big_cased\"\n",
    "embedding_size = 1024\n",
    "layer = 18\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-large-cased-seq300-\"\n",
    "pred_tag = \"naive-\"\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictons for stage 1.3\n",
    "\n",
    "### Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp_model_for_base_bert(input_shape, split_size):\n",
    "    X_input = layers.Input(input_shape)\n",
    "\n",
    "    # First dense layer\n",
    "    A = layers.Lambda(lambda x: x[:, :split_size])(X_input)\n",
    "    B = layers.Lambda(lambda x: x[:, split_size:split_size*2])(X_input)\n",
    "    P = layers.Lambda(lambda x: x[:, split_size*2:])(X_input)\n",
    "\n",
    "    head_num = 6\n",
    "    res = []\n",
    "    for head in range(head_num):\n",
    "        query_encoder = layers.Dense(dense_layer_sizes[0], activation='selu', kernel_regularizer = regularizers.l2(1e-6))\n",
    "        ans_encoder = layers.Dense(dense_layer_sizes[0], activation='selu', kernel_regularizer = regularizers.l2(1e-6))\n",
    "        \n",
    "        d_ratio = 0.6\n",
    "        \n",
    "        a, b = query_encoder(layers.Dropout(d_ratio)(A)), query_encoder(layers.Dropout(d_ratio)(B))\n",
    "        p = query_encoder(layers.Dropout(d_ratio)(P))\n",
    "        \n",
    "        amp = layers.Multiply()([a, p])\n",
    "        bmp = layers.Multiply()([b, p])\n",
    "        \n",
    "        asp = layers.Lambda(lambda v: v[0] - v[1])([p, a])\n",
    "        bsp = layers.Lambda(lambda v: v[0] - v[1])([p, b])        \n",
    "        \n",
    "        ia = layers.Concatenate()([a, p, amp, asp])\n",
    "        ib = layers.Concatenate()([b, p, bmp, bsp])\n",
    "        nli_encoder = layers.Dense(dense_layer_sizes[0], activation='selu')\n",
    "        ia, ib = nli_encoder(ia), nli_encoder(ib)\n",
    "        \n",
    "        out = layers.Concatenate()([ia, ib])\n",
    "        res.append(out)\n",
    "    \n",
    "    res = layers.Add()(res)\n",
    "    res = layers.Dropout(0.8)(res)\n",
    "    X = layers.Dense(dense_layer_sizes[0], name = 'dense0')(res)\n",
    "    X = layers.BatchNormalization(name = 'bn0')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.Dropout(dropout_rate, seed = 7)(X)\n",
    "\n",
    "    # Output layer\n",
    "    X = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n",
    "    X = layers.Activation('softmax')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(input = X_input, output = X, name = \"classif_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n------------------------------\\nFor the model bert-large-uncased-seq256-19\\nCV mean score: 0.3441, std: 0.0215.\\n[0.3307294825242308, 0.3187784074490988, 0.34923320252765083, 0.3821268940718645, 0.3397000497226646]\\nTest score: 0.32621788223567494\\n------------------------------\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_mlp_model_for_larget_bert(input_shape, split_size):\n",
    "    X_input = layers.Input(input_shape)\n",
    "\n",
    "    # First dense layer\n",
    "    A = layers.Lambda(lambda x: x[:, :split_size])(X_input)\n",
    "    B = layers.Lambda(lambda x: x[:, split_size:split_size*2])(X_input)\n",
    "    P = layers.Lambda(lambda x: x[:, split_size*2:])(X_input)\n",
    "\n",
    "    head_num = 6\n",
    "    res = []\n",
    "    for head in range(head_num):\n",
    "        query_encoder = layers.Dense(dense_layer_sizes[0], activation='selu', kernel_regularizer = regularizers.l2(1e-6))\n",
    "        ans_encoder = layers.Dense(dense_layer_sizes[0], activation='selu', kernel_regularizer = regularizers.l2(1e-6))\n",
    "        \n",
    "        d_ratio = 0.7\n",
    "        \n",
    "        a, b = query_encoder(layers.Dropout(d_ratio)(A)), query_encoder(layers.Dropout(d_ratio)(B))\n",
    "        p = query_encoder(layers.Dropout(d_ratio)(P))\n",
    "        \n",
    "        amp = layers.Multiply()([a, p])\n",
    "        bmp = layers.Multiply()([b, p])\n",
    "        \n",
    "        asp = layers.Lambda(lambda v: v[0] - v[1])([p, a])\n",
    "        bsp = layers.Lambda(lambda v: v[0] - v[1])([p, b])        \n",
    "        \n",
    "        ia = layers.Concatenate()([a, p, amp, asp])\n",
    "        ib = layers.Concatenate()([b, p, bmp, bsp])\n",
    "        nli_encoder = layers.Dense(dense_layer_sizes[0], activation='selu')\n",
    "        ia, ib = nli_encoder(ia), nli_encoder(ib)\n",
    "        \n",
    "        out = layers.Concatenate()([ia, ib])\n",
    "        res.append(out)\n",
    "    \n",
    "    res = layers.Concatenate()(res)\n",
    "    res = layers.Dropout(0.85)(res)\n",
    "    X = layers.Dense(dense_layer_sizes[0], name = 'dense0')(res)\n",
    "    X = layers.BatchNormalization(name = 'bn0')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.Dropout(dropout_rate, seed = 7)(X)\n",
    "    \n",
    "    # Output layer\n",
    "    X = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n",
    "    X = layers.Activation('softmax')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(input = X_input, output = X, name = \"classif_model\")\n",
    "    return model\n",
    "\n",
    "'''\n",
    "------------------------------\n",
    "For the model bert-large-uncased-seq256-19\n",
    "CV mean score: 0.3441, std: 0.0215.\n",
    "[0.3307294825242308, 0.3187784074490988, 0.34923320252765083, 0.3821268940718645, 0.3397000497226646]\n",
    "Test score: 0.32621788223567494\n",
    "------------------------------\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def base_nli_model(input_shape, split_size):\n",
    "    X_input = layers.Input(input_shape)\n",
    "\n",
    "    # First dense layer\n",
    "    A = layers.Lambda(lambda x: x[:, :split_size])(X_input)\n",
    "    B = layers.Lambda(lambda x: x[:, split_size:split_size*2])(X_input)\n",
    "    P = layers.Lambda(lambda x: x[:, split_size*2:])(X_input)\n",
    "\n",
    "    X1 = layers.Dropout(0.6, seed = 7)(A)\n",
    "    X2 = layers.Dropout(0.6, seed = 7)(B)\n",
    "    Y = layers.Dropout(0.6, seed = 7)(P)    \n",
    "    \n",
    "    def interaction(a, b):\n",
    "        sub = layers.Lambda(lambda a: K.abs(a[0] - a[1]))([a, b])\n",
    "        mult = layers.Lambda(lambda a: a[0] * a[1])([a, b])\n",
    "        return layers.Concatenate()([a, b, sub, mult,])    \n",
    "    \n",
    "    word_encoder = layers.Dense(512, activation='selu')\n",
    "    X1 = word_encoder(X1)\n",
    "    X2 = word_encoder(X2)\n",
    "    Y = word_encoder(Y)\n",
    "\n",
    "    I_X1_Y = interaction(X1, Y)\n",
    "    I_X2_Y = interaction(X2, Y)\n",
    "\n",
    "    dense_encoder = layers.Dense(128, activation='selu')\n",
    "\n",
    "    I_X1_Y = layers.Dropout(0.75)(dense_encoder(I_X1_Y))\n",
    "    I_X2_Y = layers.Dropout(0.75)(dense_encoder(I_X2_Y))\n",
    "    features = layers.Concatenate()([I_X1_Y, I_X2_Y])\n",
    "\n",
    "    X = layers.Dense(dense_layer_sizes[0], name = 'dense0')(features)\n",
    "    X = layers.BatchNormalization(name = 'bn0')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.Dropout(dropout_rate, seed = 7)(X)\n",
    "\n",
    "    # Output layer\n",
    "    X = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n",
    "    X = layers.Activation('softmax')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(input = X_input, output = X, name = \"classif_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_tag, model_func,\n",
    "                dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "                oof_folder='oof/', pred_folder='test_outputs/'):\n",
    "\n",
    "    tag = tag + str(layer) # follow the original naming style\n",
    "    dev_filename = tag + dev_filename\n",
    "    val_filename = tag + val_filename\n",
    "    test_filename = tag + test_filename\n",
    "    \n",
    "    development = pd.read_json(os.path.join(dev_folder_path, dev_filename))\n",
    "    X_development, Y_development = parse_json(development, embedding_size)\n",
    "\n",
    "    # We want predictions for all development rows. So instead of removing rows, make them 0\n",
    "    remove_development = [row for row in range(len(X_development)) if np.sum(np.isnan(X_development[row].reshape(-1)))]\n",
    "    X_development[remove_development] = np.zeros((3 * embedding_size))\n",
    "\n",
    "    prediction = np.zeros((len(X_development), 3)) # testing predictions\n",
    "\n",
    "    for fold_n in range(n_fold):\n",
    "        # split training and validation data\n",
    "        print('Make predictions for', fold_n, 'started at', time.ctime())\n",
    "        classif_model = model_func([X_development.shape[-1]], split_size=embedding_size)\n",
    "        classif_model.load_weights(os.path.join(checkpoint_path, model_tag + tag + str(fold_n) + '.pt'))\n",
    "\n",
    "        pred = classif_model.predict(x=X_development, verbose=0)\n",
    "        prediction += pred\n",
    "    \n",
    "    prediction /= n_fold\n",
    "    # prediction[remove_development] = [0.4, 0.4, 0.2] This is useless for prediction stage\n",
    "\n",
    "    submission = pd.read_csv(\"data/sample_submission_stage_1.csv\", index_col = \"ID\")\n",
    "    submission[\"A\"] = prediction[:,0]\n",
    "    submission[\"B\"] = prediction[:,1]\n",
    "    submission[\"NEITHER\"] = prediction[:,2]\n",
    "    submission.to_csv(pred_folder + model_tag + tag + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:15:42 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:15:43 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:15:45 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:15:46 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:15:48 2019\n",
      "Make predictions for 0 started at Mon Apr 15 14:15:50 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:15:51 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:15:52 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:15:53 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:15:54 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_base\"\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-base-uncased-seq512-\"\n",
    "pred_tag = \"nli-mh-\"\n",
    "model_func = build_mlp_model_for_base_bert\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag, model_func=model_func)\n",
    "\n",
    "pred_tag = \"bnli-mh-\"\n",
    "model_func = base_nli_model\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag, model_func=model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:15:56 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:15:57 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:15:59 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:16:01 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:16:03 2019\n",
      "Make predictions for 0 started at Mon Apr 15 14:16:06 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:16:07 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:16:09 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:16:10 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:16:12 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_base_cased\"\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-base-cased-seq512-\"\n",
    "pred_tag = \"nli-mh-\"\n",
    "model_func = build_mlp_model_for_base_bert\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag, model_func=model_func)\n",
    "\n",
    "pred_tag = \"bnli-mh-\"\n",
    "model_func = base_nli_model\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag, model_func=model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:16:14 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:16:16 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:16:19 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:16:21 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:16:24 2019\n",
      "Make predictions for 0 started at Mon Apr 15 14:16:27 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:16:29 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:16:31 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:16:33 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:16:35 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_big\"\n",
    "embedding_size = 1024\n",
    "layer = 19\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-large-uncased-seq300-\"\n",
    "pred_tag = \"nli-mh-\"\n",
    "model_func = build_mlp_model_for_larget_bert\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag, model_func=model_func)\n",
    "\n",
    "pred_tag = \"bnli-mh-\"\n",
    "model_func = base_nli_model\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag, model_func=model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:16:38 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:16:42 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:16:44 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:16:47 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:16:50 2019\n",
      "Make predictions for 0 started at Mon Apr 15 14:16:54 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:16:57 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:16:59 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:17:02 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:17:05 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_big_cased\"\n",
    "embedding_size = 1024\n",
    "layer = 18\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-large-cased-seq300-\"\n",
    "pred_tag = \"nli-mh-\"\n",
    "model_func = build_mlp_model_for_larget_bert\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag, model_func=model_func)\n",
    "\n",
    "pred_tag = \"bnli-mh-\"\n",
    "model_func = base_nli_model\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag, model_func=model_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predictons for stage 1.4\n",
    "\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Load the GAP data\n",
    "\n",
    "data = pd.concat([pd.read_csv('gap-development.tsv', sep='\\t'),\n",
    "                  pd.read_csv('gap-validation.tsv', sep='\\t'),\n",
    "                  pd.read_csv('gap-test.tsv', sep='\\t'),\n",
    "                 ], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ee47031ec745ebaedf42486a718351"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As wrongly disqualified: 57 , Bs wrongly disqualified: 94 \n",
      "\n",
      "development-155 (154): A>\n",
      "\tThe official first single, ``Fallin''', was written solely by Keys, and\n",
      "\ttopped the US Billboard Hot 100 chart. ``A Woman's Worth'', written by\n",
      "\tKeys and Erika Rose, is a ``jazz-tinged'' song with\n",
      "\tlyrics which speak of how men should treat and respect women. Keys released\n",
      "\ther second album, The Diary of Alicia Keys, in December 2003.\n",
      "\n",
      "The[DET] >det> single | official[ADJ] >amod> single | first[ADJ] >advmod> single | \n",
      "single[ADJ] >nsubjpass> written | ,[PUNCT] >punct> single | ``[PUNCT] >punct> single | \n",
      "Fallin[PROPN] >appos> single | '[PUNCT] >punct> Fallin | ''[PUNCT] >punct> single | \n",
      ",[PUNCT] >punct> written | was[VERB] >auxpass> written | written[VERB] >ROOT> written | \n",
      "solely[ADV] >advmod> written | by[ADP] >agent> written | Keys[NOUN] >pobj> by | \n",
      ",[PUNCT] >punct> written | and[CCONJ] >cc> written | topped[VERB] >conj> written | \n",
      "the[DET] >det> chart | US[PROPN] >compound> chart | Billboard[PROPN] >nmod> chart | \n",
      "Hot[PROPN] >nmod> chart | 100[NUM] >nummod> Hot | chart[NOUN] >dobj> topped | \n",
      ".[PUNCT] >punct> written | ``[PUNCT] >punct> Worth | A[DET] >det> Woman | \n",
      "Woman[NOUN] >poss> Worth | 's[PART] >case> Woman | Worth[PROPN] >nsubj> is | \n",
      "''[PUNCT] >punct> Worth | ,[PUNCT] >punct> Worth | written[VERB] >acl> Worth | \n",
      "by[ADP] >agent> written | Keys[PROPN] >pobj> by | and[CCONJ] >cc> Keys | \n",
      "Erika[PROPN] >compound> Rose | Rose[PROPN] >conj> Keys | ,[PUNCT] >punct> is | \n",
      "is[VERB] >ROOT> is | a[DET] >det> song | ``[PUNCT] >punct> song | \n",
      "jazz[NOUN] >npadvmod> tinged | -[PUNCT] >punct> tinged | tinged[VERB] >amod> song | \n",
      "''[PUNCT] >punct> song | song[NOUN] >attr> is | with[ADP] >prep> song | \n",
      "lyrics[NOUN] >pobj> with | which[ADJ] >nsubj> speak | speak[VERB] >relcl> song | \n",
      "of[ADP] >prep> speak | how[ADV] >advmod> treat | men[NOUN] >nsubj> treat | \n",
      "should[VERB] >aux> treat | treat[VERB] >pcomp> of | and[CCONJ] >cc> treat | \n",
      "respect[VERB] >conj> treat | women[NOUN] >dobj> respect | .[PUNCT] >punct> is | \n",
      "Keys[NOUN] >nsubj> released | released[VERB] >ROOT> released | her[ADJ] >poss> album | \n",
      "second[ADJ] >amod> album | album[NOUN] >dobj> released | ,[PUNCT] >punct> album | \n",
      "The[DET] >det> Diary | Diary[PROPN] >appos> album | of[ADP] >prep> Diary | \n",
      "Alicia[PROPN] >compound> Keys | Keys[PROPN] >pobj> of | ,[PUNCT] >punct> album | \n",
      "in[ADP] >prep> released | December[PROPN] >pobj> in | 2003[NUM] >nummod> December | \n",
      ".[PUNCT] >punct> released | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def domain(t):\n",
    "    while not t._.subj and not t._.poss and\\\n",
    "            not (t.dep_ == 'xcomp' and t.head._.obj) and\\\n",
    "            t != t.head:\n",
    "        t = t.head\n",
    "    return t\n",
    "\n",
    "def ccom(t):\n",
    "    return [t2 for t2 in t.head._.d]\n",
    "\n",
    "# spacy extensions:\n",
    "#   doc._.to(offset) => t at text char offset\n",
    "#   t._.c => t's children (list)                     #   t._.d => t's descendents (list)\n",
    "#   t._.subj => t's subject else False               #   t._.obj => t's object else False\n",
    "#   t._.domain => t's syntactic domain               #   t._.ccom => t's c-command domain\n",
    "\n",
    "spacy.tokens.doc.Doc.set_extension(\n",
    "    'to', method=lambda doc, offset: [t for t in doc if t.idx == offset][0], force=True)\n",
    "spacy.tokens.token.Token.set_extension(\n",
    "    'c', getter=lambda t: [c for c in t.children], force=True)\n",
    "spacy.tokens.token.Token.set_extension(\n",
    "    'd', getter=lambda t: [c for c in t.sent if t in list(c.ancestors)], force=True)\n",
    "spacy.tokens.token.Token.set_extension(\n",
    "    'subj', getter=lambda t: ([c for c in t._.c if c.dep_.startswith('nsubj')] + [False])[0], force=True)\n",
    "spacy.tokens.token.Token.set_extension(\n",
    "    'obj', getter=lambda t: ([c for c in t._.c if c.dep_.startswith('dobj')] + [False])[0], force=True)\n",
    "spacy.tokens.token.Token.set_extension(\n",
    "    'poss', getter=lambda t: ([c for c in t._.c if c.dep_.startswith('poss')] + [False])[0], force=True)\n",
    "spacy.tokens.token.Token.set_extension(\n",
    "    'span', method=lambda t, t2: t.doc[t.i:t2.i] if t.i < t2.i else t.doc[t2.i:t.i], force=True)\n",
    "spacy.tokens.token.Token.set_extension('domain', getter=domain, force=True)\n",
    "spacy.tokens.token.Token.set_extension('ccom', getter=ccom, force=True)\n",
    "\n",
    "def applyDisq(condition, candidates, candidate_dict, debug = False):\n",
    "    badnames = sum([nameset(c, candidate_dict) for c in candidates if c in condition[0]], [])\n",
    "    badcands = [c for c in candidates if c.text in badnames]\n",
    "    if debug and len(badcands) > 0: print('Disqualified:', badcands, '<', condition[1])\n",
    "    return [c for c in candidates if c not in badcands]\n",
    "\n",
    "# Apply a list of disqualifying conditions\n",
    "def applyDisqs(conditions, candidates, candidate_dict, debug = False):\n",
    "    for condition in conditions:\n",
    "        if len(candidates) < 1: return candidates\n",
    "        candidates = applyDisq(condition, candidates, candidate_dict, debug)\n",
    "    return candidates\n",
    "\n",
    "# Pass the list of disqualifying conditions for possessive pronouns (his, her)\n",
    "def disqGen(t, candidates, candidate_dict, debug = False):\n",
    "    conds = [(t._.ccom,\n",
    "             \"disqualify candidates c-commanded by genpn; e.g. e.g. *Julia read his_i book about John_i's life.\"),\n",
    "             ([t2 for t2 in candidates if t in t2._.ccom and t2.head.dep_ == 'appos'],\n",
    "             \"disqualify candidates modified by an appositive with genpn; e.g. *I wanted to see John_i, his_i father.\")\n",
    "            ]\n",
    "    return applyDisqs(conds, candidates, candidate_dict, debug)\n",
    "\n",
    "# Pass the list of list of disqualifying conditions for other pronouns\n",
    "def disqOthers(t, candidates, candidate_dict, debug = False):\n",
    "    conds = [([t2 for t2 in t._.ccom if t2.i > t.i],\n",
    "             \"disqualify candidates c-commanded by pn, unless they were preposed;\\\n",
    "             e.g. *He_i cried before John_i laughed. vs. Before John_i laughed, he_i cried.\"),\n",
    "             ([t2 for t2 in candidates if t in t2._.ccom and t2._.domain == t._.domain\n",
    "              and not (t.head.text == 'with' and t.head.head.lemma_ == 'take')],\n",
    "             \"disqualify candidates that c-command pn, unless in different domain;\\\n",
    "             e.g. Mary said that *John_i hit him_i. vs. John_i said that Mary hit him_i;\\\n",
    "             random hard-coded exception: `take with'\"),\n",
    "             ([t2 for t2 in candidates if t2._.domain.dep_ == 'xcomp' and t2._.domain.head._.obj and t2 == t2._.domain.head._.obj],\n",
    "             \"for xcomps with subjects parsed as upstairs dobj, disallow coref with that dobj;\\\n",
    "             e.g. *Mary wanted John_i to forgive him_i.\")\n",
    "            ]\n",
    "    return applyDisqs(conds, candidates, candidate_dict, debug)\n",
    "\n",
    "# Decide whether possessive or not and call appropriate function\n",
    "def disq(t, candidates, candidate_dict, debug = False):\n",
    "    func = disqGen if t.dep_ == 'poss' else disqOthers\n",
    "    candidates = func(t, candidates, candidate_dict, debug)\n",
    "    return candidates\n",
    "\n",
    "def find_head(w, wo, doc):\n",
    "    t = False; backtrack = 0\n",
    "    while not t:\n",
    "        try:\n",
    "            t = doc._.to(wo)\n",
    "        except IndexError:\n",
    "            wo -= 1; backtrack += 1\n",
    "    while t.dep_ == 'compound' and t.head.idx >= wo and t.head.idx < len(w) + wo + backtrack: t = t.head\n",
    "    return t\n",
    "\n",
    "# Returns subsequences of a name\n",
    "def subnames(name):\n",
    "    if type(name) != str: name = candidate_dict[name]\n",
    "    parts = name.split(' ')\n",
    "    subnames_ = []\n",
    "    for i in range(len(parts)): \n",
    "        for j in range(i + 1, len(parts) + 1): \n",
    "            sub = ' '.join(parts[i:j])\n",
    "            if len(sub) > 2: subnames_.append(sub)\n",
    "    return subnames_\n",
    "\n",
    "# Returns subsequences of a name unless potentially ambiguous (if another candidate picks out same subsequence)\n",
    "def nameset(name, candidate_dict):\n",
    "    if type(name) != str: name = candidate_dict[name]\n",
    "    subnames_ = [sn for sn in subnames(name)]\n",
    "    return [c for c in subnames_ if c not in sum([subnames(c) for c in candidate_dict.values() \n",
    "                                                  if c not in subnames_ and name not in subnames(c)], [])]\n",
    "\n",
    "# Given the original candidate dict and the final candidate list, returns new dict grouping putative candidate instances under a single key\n",
    "def candInstances(candidates, candidate_dict):\n",
    "    candidates_by_name = {}\n",
    "    for c in sorted(candidates, key = lambda c: len(candidate_dict[c]), reverse = True):\n",
    "        name = candidate_dict[c]\n",
    "        for name2 in candidates_by_name.keys():\n",
    "            if name in nameset(name2, candidate_dict): name = name2; break\n",
    "        candidates_by_name[name] = candidates_by_name.get(name, []) + [c]\n",
    "    return candidates_by_name\n",
    "\n",
    "import gender_guesser.detector as gender # oops\n",
    "gd = gender.Detector()\n",
    "\n",
    "# Needed to prune candidate dict-- removes non-provided candidates that don't match in most common gender with pn\n",
    "def filterGender(candidates_by_name, a, b, pn):\n",
    "    badnames = []\n",
    "    gender = 'female' if pn in ['She', 'she', 'her', 'Her'] else 'male'\n",
    "    for name in candidates_by_name.keys():\n",
    "        if a in subnames(name) or b in subnames(name): continue\n",
    "        genderii = gd.get_gender(name.split(' ')[0])\n",
    "        if gender == 'male' and genderii == 'female': badnames += [name]; continue\n",
    "        if gender == 'female' and genderii == 'male': badnames += [name]; continue\n",
    "    for name in badnames: candidates_by_name.pop(name)\n",
    "    return candidates_by_name\n",
    "\n",
    "from urllib.parse import unquote\n",
    "import re\n",
    "\n",
    "# Authors' metric 1: Does the Wikipedia url contain the candidate's name?\n",
    "def urlMatch(a, b, url, candidate_dict):\n",
    "    url = re.sub('[^\\x00-\\x7F]', '*', unquote(url.split('/')[-1])).replace('_', ' ').lower()\n",
    "    return {'a_url': (sorted([len(n.split(' ')) for n in nameset(a.lower(), candidate_dict) if n in nameset(url, candidate_dict)], reverse = True) + [0])[0],\n",
    "            'b_url': (sorted([len(n.split(' ')) for n in nameset(b.lower(), candidate_dict) if n in nameset(url, candidate_dict)], reverse = True) + [0])[0]}\n",
    "\n",
    "# Authors' metric 2: When pn is subject or object, does the candidate match?\n",
    "def parallel(t1, t2):\n",
    "    if t1.dep_.startswith('nsubj'): return t2.dep_.startswith('nsubj')\n",
    "    if t1.dep_.startswith('dobj'): return t2.dep_.startswith('dobj')\n",
    "    if t1.dep_.startswith('dative'): return t2.dep_.startswith('dative')\n",
    "    return False\n",
    "\n",
    "# Depth from a node to a parent node\n",
    "def depthTo(t1, t2):\n",
    "    depth = 0\n",
    "    while t1 != t2 and t1 != t1.head:\n",
    "        t1 = t1.head\n",
    "        depth += 1\n",
    "    return depth\n",
    "\n",
    "# Syntactic distance within a single tree\n",
    "def nodeDist(t1, t2):\n",
    "    if t1 == t2: return 0\n",
    "    if t2 in t1._.d: return depthTo(t2, t1)\n",
    "    if t1 in t2._.d: return depthTo(t1, t2)\n",
    "    t = t1\n",
    "    while t1 not in t._.d or t2 not in t._.d and t != t.head: t = t.head\n",
    "    return depthTo(t1, t) + depthTo(t2, t)\n",
    "\n",
    "# Authors' metric 3: Syntactic distance (within or across trees)\n",
    "def synDist(t, pn, doc, debug = False):\n",
    "    doc_sents = list(doc.sents)\n",
    "    sspan = doc_sents.index(pn.sent) - doc_sents.index(t.sent)\n",
    "    if sspan == 0: # same sentence\n",
    "        dist = nodeDist(t, pn)\n",
    "    else: # different sentence\n",
    "        dist = nodeDist(pn, doc_sents[doc_sents.index(pn.sent)].root) + nodeDist(t, doc_sents[doc_sents.index(t.sent)].root) # dist from two roots\n",
    "    if debug: \n",
    "        print('pn dist:', nodeDist(pn, doc_sents[doc_sents.index(pn.sent)].root), '; t dist:',\n",
    "              nodeDist(t, doc_sents[doc_sents.index(t.sent)].root), '; span:', sspan)\n",
    "    sspan = abs(sspan) * 1 if sspan >= 0 else abs(sspan) * 1.3 # less local if not preceding\n",
    "    return dist + sspan# * 0.7\n",
    "\n",
    "# Character distance\n",
    "def charDist(t1, t2):\n",
    "    if t2.idx > t1.idx:\n",
    "        return t2.idx - t1.idx + len(t1.text)\n",
    "    else:\n",
    "        return (t1.idx - t2.idx + len(t2.text)) * 1.3\n",
    "\n",
    "# Theta prominence: assign a 0.1 to 1 score based on dep role of candidate -- strong feature\n",
    "def thetaProminence(t, mult = 1, debug = False):\n",
    "    while t.dep_ == 'compound': t = t.head\n",
    "    if debug: print('t dep_:', t.dep_)\n",
    "    if t.dep_ == 'pobj': mult = 1.3 if t.head.i < t.head.head.i else 1\n",
    "    if t._.domain.dep_ == 'advcl': mult = 1.3 if t.head.i < t._.domain.head.i else 1\n",
    "    if t.dep_.startswith('nsubj'): score = 1\n",
    "    elif t.dep_.startswith('dobj'): score = 0.8\n",
    "    elif t.dep_.startswith('dative'): score = 0.6\n",
    "    elif t.dep_.startswith('pobj'): score = 0.4\n",
    "    elif t.dep_.startswith('poss'): score = 0.3\n",
    "    else: score = 0.1\n",
    "    if debug: print('mult:', mult, '; score:', score)\n",
    "    return min(1, score * mult)\n",
    "\n",
    "# Computes these metrics for each candidate, and returns, for each group of instances (A instances, B instances,\\\n",
    "# other instances), either the sum, or the highest difference from the mean\n",
    "def score(label, candidates_by_name, a_cand, b_cand, func, minsc = None, method = 'sum'):\n",
    "    if method == 'sum':\n",
    "        scores = {name: sum([func(t) for t in tokens]) for name, tokens in candidates_by_name.items()}\n",
    "    elif method == 'meandiff':\n",
    "        mean = np.mean(sum([[func(t) for t in tokens] for tokens in candidates_by_name.values()], []))\n",
    "        scores = {name: mean - min([func(t) for t in tokens]) for name, tokens in candidates_by_name.items()}\n",
    "    sca = scores[a_cand] if a_cand else minsc\n",
    "    scb = scores[b_cand] if b_cand else minsc\n",
    "    screst = [v for n, v in scores.items() if n != a_cand and n != b_cand]\n",
    "    if method == 'sum':\n",
    "        screst = sum(screst) if len(screst) > 0 else minsc\n",
    "    elif method == 'meandiff':\n",
    "        screst = max(screst) if len(screst) > 0 else minsc\n",
    "    return {'a_' + label: sca, 'b_' + label: scb, 'n_' + label: screst}\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Load a rowfull of data\n",
    "def load_row(data, i):\n",
    "    return tuple(data.iloc[i])\n",
    "\n",
    "# Row by row, populate features\n",
    "def annotateSet(data, minsc = None, debug = False):\n",
    "    \n",
    "    annotated_data = pd.DataFrame() # init placeholder df\n",
    "    row_batch = []\n",
    "\n",
    "    for i in tqdm(range(annotated_data.shape[0], data.shape[0])):\n",
    "        id, text, pn, pno, a, ao, ag, b, bo, bg, url = load_row(data, i)        \n",
    "        doc = nlp(text) # parse text into doc\n",
    "        pnt, at, bt = (doc._.to(pno), find_head(a, ao, doc), find_head(b, bo, doc)) # get the tokens that correspond to offsets\n",
    "        candidate_dict = {e.root: re.sub('\\'s$', '', e.text) for e in [e for e in doc.ents if e.root.ent_type_ == 'PERSON']} # first get every PERSON ent as candidate\n",
    "        candidate_dict.update({c.root: re.sub('\\'s$', '', c.text) for c in doc.noun_chunks if c.root.pos_ == 'PROPN' and c.text in sum([subnames(n) for n in candidate_dict.values()], []) and\n",
    "                               c.root not in candidate_dict.keys()}) # get some missed ones by looking at noun chunks with PROPN roots whose text match part of a candidate but are not already in list\n",
    "        candidate_dict.update({t: w for t, w in [(at, a), (bt, b)]}) # add provided cands, overwriting in the process\n",
    "\n",
    "        candidates = disq(pnt, list(candidate_dict.keys()), candidate_dict, debug = False)\n",
    "        candidates_by_name = candInstances(candidates, candidate_dict)\n",
    "        candidates_by_name = filterGender(candidates_by_name, a, b, pn)\n",
    "        a_cand = ([name for name, tokens in candidates_by_name.items() if at in tokens] + [False])[0]\n",
    "        b_cand = ([name for name, tokens in candidates_by_name.items() if bt in tokens] + [False])[0]\n",
    "    \n",
    "        # init row dict\n",
    "        features = {'id': id, 'label': 0 if ag else 1 if bg else 2}\n",
    "        # eliminated or not\n",
    "        features.update({'a_out': 0 if a_cand else 1, 'b_out': 0 if b_cand else 1})\n",
    "        # url match or not\n",
    "        features.update(urlMatch(a, b, url, candidate_dict))\n",
    "        # c-command or not\n",
    "        features.update({'a_cc': 1 if a_cand and pnt in at._.ccom else 0, 'b_cc': 1 if b_cand and pnt in bt._.ccom else 0})\n",
    "        # parallelism score\n",
    "        features.update(score('par', candidates_by_name, a_cand, b_cand, lambda t: parallel(t, pnt), minsc = minsc))\n",
    "        # theta prominence score\n",
    "        features.update(score('th', candidates_by_name, a_cand, b_cand, thetaProminence, minsc = minsc))\n",
    "        # syntactic distance score\n",
    "        features.update(score('loc', candidates_by_name, a_cand, b_cand, lambda t: synDist(t, pnt, doc), method='meandiff', minsc = minsc))\n",
    "        # number of candidates left\n",
    "        features.update({'n_cands': len(candidates_by_name)})\n",
    "        # char dist\n",
    "        features.update(score('cloc', candidates_by_name, a_cand, b_cand, lambda t: charDist(t, pnt), method='meandiff', minsc = minsc))\n",
    "\n",
    "        if debug: print(id, '>', 'a:', 1 if ag else 0, 'b:', 1 if bg else 0, features)\n",
    "        row_batch += [features]\n",
    "\n",
    "    # add rows to placeholder df\n",
    "    if annotated_data.shape[0] != data.shape[0]: annotated_data = annotated_data.append(row_batch, ignore_index = True)\n",
    "    \n",
    "    return annotated_data\n",
    "\n",
    "# Readable rows\n",
    "\n",
    "from textwrap import TextWrapper\n",
    "wrapper = TextWrapper(width=75)\n",
    "\n",
    "def style(w, wstyle):\n",
    "    if wstyle == None: return '{}{}{}'.format('\\033[1m', w, '\\033[0m') # bold\n",
    "    elif wstyle == True: return '{}{}{}'.format('\\033[92m', w, '\\033[0m') # green\n",
    "    elif wstyle == False: return '{}{}{}'.format('\\033[91m', w, '\\033[0m') # red\n",
    "\n",
    "def readable_rows(data, range):\n",
    "    rows = []\n",
    "    for i in range:\n",
    "        wois = sorted([(data.iloc[i]['Pronoun'], data.iloc[i]['Pronoun-offset'], None),\\\n",
    "                   (data.iloc[i]['A'], data.iloc[i]['A-offset'], data.iloc[i]['A-coref']),\\\n",
    "                   (data.iloc[i]['B'], data.iloc[i]['B-offset'], data.iloc[i]['B-coref'])],\\\n",
    "                   key = lambda x: x[1]) # sort by offset\n",
    "        text = ''; ftext = data.iloc[i]['Text']; coffset = 0\n",
    "        for w, woffset, wstyle in wois:\n",
    "            text += ftext[coffset:woffset] + style(w, wstyle)\n",
    "            coffset += len(ftext[coffset:woffset]) + len(w)\n",
    "        text += ftext[coffset:]\n",
    "        rows += [(str(i), data.iloc[i]['ID'], 'A' if data.iloc[i]['A-coref'] else 'B' if data.iloc[i]['B-coref'] else 'N', text)]\n",
    "    return rows\n",
    "\n",
    "def print_readable_rows(data, range):\n",
    "    for index, id, target, text in readable_rows(data, range):\n",
    "        text = '\\n\\t'.join(wrapper.wrap(text = text))\n",
    "        print('{} ({}): {}>\\n\\t{}\\n'.format(id, index, target, text))\n",
    "        \n",
    "def print_tokens(doc):\n",
    "    for i in range(0, len(doc), 3):\n",
    "        for t in doc[i:i+3]:\n",
    "            print(\"{}[{}] >{}> {}\".format(t.text, t.pos_, t.dep_, t.head.text), end = ' | ')\n",
    "        print('')\n",
    "        \n",
    "annotated_data = annotateSet(data)\n",
    "\n",
    "misdisqs_a = annotated_data.loc[(annotated_data['a_out'] == 1) & (annotated_data['label'] == 0)]\n",
    "misdisqs_b = annotated_data.loc[(annotated_data['b_out'] == 1) & (annotated_data['label'] == 1)]\n",
    "\n",
    "print('As wrongly disqualified:', misdisqs_a.shape[0], ', Bs wrongly disqualified:', misdisqs_b.shape[0], '\\n')\n",
    "\n",
    "for i in misdisqs_a.index[:1]: # the first\n",
    "    print_readable_rows(data, [i])\n",
    "    id, text, pn, pno, a, ao, ag, b, bo, bg, url = load_row(data,i)\n",
    "    doc = nlp(text); print_tokens(doc)\n",
    "    print('')\n",
    "    \n",
    "train = annotated_data.loc[(annotated_data['id'].str.contains('test'))] # swap later\n",
    "valid = annotated_data.loc[(annotated_data['id'].str.contains('validation'))]\n",
    "train_valid = pd.concat([train, valid])\n",
    "test = annotated_data.loc[(annotated_data['id'].str.contains('development'))] # swap later\n",
    "\n",
    "answer_columns = ['label']\n",
    "excl = ['id']\n",
    "excl += ['a_out', 'b_out']\n",
    "feature_columns = [col for col in annotated_data.columns if col not in answer_columns and col not in excl]\n",
    "\n",
    "meta_features_train_valid = train_valid[feature_columns].fillna(0)\n",
    "meta_features_test = test[feature_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_features_test shpae: (2000, 17)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "for col in meta_features_train_valid.columns:\n",
    "    std = MinMaxScaler()\n",
    "    meta_features_train_valid[col] = std.fit_transform(meta_features_train_valid[col].values.reshape(-1, 1))\n",
    "    meta_features_test[col] = std.transform(meta_features_test[col].values.reshape(-1, 1))\n",
    "    \n",
    "meta_features_test = meta_features_test.values\n",
    "print(\"meta_features_test shpae:\", meta_features_test.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp_model(input_shape, split_size, meta_features_shape):\n",
    "    X_input = layers.Input(input_shape)\n",
    "    meta_input = layers.Input(meta_features_shape)\n",
    "\n",
    "    # First dense layer\n",
    "    A = layers.Lambda(lambda x: x[:, :split_size])(X_input)\n",
    "    B = layers.Lambda(lambda x: x[:, split_size:split_size*2])(X_input)\n",
    "    P = layers.Lambda(lambda x: x[:, split_size*2:])(X_input)\n",
    "\n",
    "    head_num = 6\n",
    "    res = []\n",
    "    for head in range(head_num):\n",
    "        query_encoder = layers.Dense(dense_layer_sizes[0], activation='selu', kernel_regularizer = regularizers.l2(1e-6))\n",
    "        ans_encoder = layers.Dense(dense_layer_sizes[0], activation='selu', kernel_regularizer = regularizers.l2(1e-6))\n",
    "        \n",
    "        d_ratio = 0.6\n",
    "        \n",
    "        a, b = query_encoder(layers.Dropout(d_ratio)(A)), query_encoder(layers.Dropout(d_ratio)(B))\n",
    "        p = query_encoder(layers.Dropout(d_ratio)(P))\n",
    "        \n",
    "        amp = layers.Multiply()([a, p])\n",
    "        bmp = layers.Multiply()([b, p])\n",
    "        \n",
    "        asp = layers.Lambda(lambda v: v[0] - v[1])([p, a])\n",
    "        bsp = layers.Lambda(lambda v: v[0] - v[1])([p, b])        \n",
    "        \n",
    "        ia = layers.Concatenate()([a, p, amp, asp])\n",
    "        ib = layers.Concatenate()([b, p, bmp, bsp])\n",
    "        nli_encoder = layers.Dense(dense_layer_sizes[0], activation='selu')\n",
    "        ia, ib = nli_encoder(ia), nli_encoder(ib)\n",
    "        \n",
    "        out = layers.Concatenate()([ia, ib])\n",
    "        res.append(out)\n",
    "    \n",
    "    res = layers.Add()(res)\n",
    "    res = layers.Dropout(0.8)(res)\n",
    "    \n",
    "    meta_out = layers.Dense(20, activation='selu')(meta_input)\n",
    "    meta_out = layers.Dropout(0.25)(meta_out)    \n",
    "    \n",
    "    res = layers.Concatenate()([res, meta_out])\n",
    "    \n",
    "    X = layers.Dense(dense_layer_sizes[0], name = 'dense0')(res)\n",
    "    X = layers.BatchNormalization(name = 'bn0')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.Dropout(dropout_rate, seed = 7)(X)\n",
    "\n",
    "    # Output layer\n",
    "    X = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n",
    "    X = layers.Activation('softmax')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(input = [X_input, meta_input], output = X, name = \"classif_model\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_base_nli_model(input_shape, split_size, meta_features_shape):\n",
    "    X_input = layers.Input(input_shape)\n",
    "    meta_input = layers.Input(meta_features_shape)\n",
    "\n",
    "    # First dense layer\n",
    "    A = layers.Lambda(lambda x: x[:, :split_size])(X_input)\n",
    "    B = layers.Lambda(lambda x: x[:, split_size:split_size*2])(X_input)\n",
    "    P = layers.Lambda(lambda x: x[:, split_size*2:])(X_input)\n",
    "\n",
    "    query_encoder = layers.Dense(512, activation='selu', kernel_regularizer = regularizers.l2(1e-6))\n",
    "    ans_encoder = layers.Dense(512, activation='selu', kernel_regularizer = regularizers.l2(1e-6))\n",
    "\n",
    "    d_ratio = 0.6\n",
    "\n",
    "    a, b = query_encoder(layers.Dropout(d_ratio)(A)), query_encoder(layers.Dropout(d_ratio)(B))\n",
    "    p = query_encoder(layers.Dropout(d_ratio)(P))\n",
    "\n",
    "    amp = layers.Multiply()([a, p])\n",
    "    bmp = layers.Multiply()([b, p])\n",
    "\n",
    "    asp = layers.Lambda(lambda v: v[0] - v[1])([p, a])\n",
    "    bsp = layers.Lambda(lambda v: v[0] - v[1])([p, b])        \n",
    "\n",
    "    ia = layers.Concatenate()([a, p, amp, asp])\n",
    "    ib = layers.Concatenate()([b, p, bmp, bsp])\n",
    "    \n",
    "    nli_encoder = layers.Dense(128, activation='selu')\n",
    "    ia, ib = nli_encoder(ia), nli_encoder(ib)\n",
    "\n",
    "    features = layers.Concatenate()([ia, ib])    \n",
    "    features = layers.Dropout(0.75)(features)\n",
    "    \n",
    "    meta_out = layers.Dense(20, activation='selu')(meta_input)\n",
    "    meta_out = layers.Dropout(0.25)(meta_out)    \n",
    "    \n",
    "    res = layers.Concatenate()([features, meta_out])\n",
    "    \n",
    "    X = layers.Dense(dense_layer_sizes[0], name = 'dense0')(res)\n",
    "    X = layers.BatchNormalization(name = 'bn0')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.Dropout(dropout_rate, seed = 7)(X)\n",
    "\n",
    "    # Output layer\n",
    "    X = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n",
    "    X = layers.Activation('softmax')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(input = [X_input, meta_input], output = X, name = \"classif_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_tag, model_func, \n",
    "                dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "                oof_folder='oof/', pred_folder='outputs/'):\n",
    "\n",
    "    tag = tag + str(layer) # follow the original naming style\n",
    "    dev_filename = tag + dev_filename\n",
    "    val_filename = tag + val_filename\n",
    "    test_filename = tag + test_filename\n",
    "    \n",
    "    development = pd.read_json(os.path.join(dev_folder_path, dev_filename))\n",
    "    X_development, Y_development = parse_json(development, embedding_size)\n",
    "\n",
    "    remove_development = [row for row in range(len(X_development)) if np.sum(np.isnan(X_development[row].reshape(-1)))]\n",
    "    X_development[remove_development] = np.zeros((3 * embedding_size))\n",
    "\n",
    "    # Will predict probabilities for data from the gap-development file; initializing the predictions\n",
    "    prediction = np.zeros((len(X_development), 3)) # testing predictions\n",
    "    \n",
    "    for fold_n in range(n_fold):\n",
    "        # split training and validation data\n",
    "        print('Make predictions for', fold_n, 'started at', time.ctime())\n",
    "        # Define the model, re-initializing for each fold\n",
    "        classif_model = model_func([X_development.shape[-1]], split_size=embedding_size, meta_features_shape=[meta_features_test.shape[-1]])\n",
    "        classif_model.load_weights(os.path.join(checkpoint_path, model_tag + tag + str(fold_n) + '.pt'))\n",
    "\n",
    "        pred = classif_model.predict(x=[X_development, meta_features_test], verbose=0)\n",
    "        prediction += pred\n",
    "    \n",
    "    prediction /= n_fold\n",
    "    \n",
    "    submission = pd.read_csv(\"data/sample_submission_stage_1.csv\", index_col = \"ID\")\n",
    "    submission[\"A\"] = prediction[:,0]\n",
    "    submission[\"B\"] = prediction[:,1]\n",
    "    submission[\"NEITHER\"] = prediction[:,2]\n",
    "    submission.to_csv(pred_folder + model_tag + tag + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:19:16 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:19:20 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:19:23 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:19:27 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:19:31 2019\n",
      "Make predictions for 0 started at Mon Apr 15 14:19:35 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:104: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:19:38 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:19:41 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:19:44 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:19:48 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_base\"\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-base-uncased-seq512-\"\n",
    "pred_tag = \"Hnli-mh-\"\n",
    "model_func = build_mlp_model\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_func=model_func,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)\n",
    "\n",
    "pred_tag = \"HBnli-mh-\"\n",
    "model_func = build_base_nli_model\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_func=model_func,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:19:51 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:19:56 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:20:00 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:20:04 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:20:08 2019\n",
      "Make predictions for 0 started at Mon Apr 15 14:20:13 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:104: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:20:17 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:20:21 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:20:24 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:20:28 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_base_cased\"\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-base-cased-seq512-\"\n",
    "pred_tag = \"Hnli-mh-\"\n",
    "model_func = build_mlp_model\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_func=model_func,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)\n",
    "\n",
    "pred_tag = \"HBnli-mh-\"\n",
    "model_func = build_base_nli_model\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_func=model_func,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:20:33 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:20:37 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:20:42 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:20:47 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:20:52 2019\n",
      "Make predictions for 0 started at Mon Apr 15 14:20:57 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:104: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:21:02 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:21:06 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:21:10 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:21:15 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_big\"\n",
    "embedding_size = 1024\n",
    "layer = 19\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-large-uncased-seq300-\"\n",
    "pred_tag = \"Hnli-mh-\"\n",
    "model_func = build_mlp_model\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_func=model_func,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)\n",
    "\n",
    "pred_tag = \"HBnli-mh-\"\n",
    "model_func = build_base_nli_model\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_func=model_func,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:21:20 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:21:25 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:21:30 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:21:35 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:21:40 2019\n",
      "Make predictions for 0 started at Mon Apr 15 14:21:47 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:104: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:21:52 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:21:56 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:22:01 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:22:06 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_big_cased\"\n",
    "embedding_size = 1024\n",
    "layer = 18\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-large-cased-seq300-\"\n",
    "pred_tag = \"Hnli-mh-\"\n",
    "model_func = build_mlp_model\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_func=model_func,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)\n",
    "\n",
    "pred_tag = \"HBnli-mh-\"\n",
    "model_func = build_base_nli_model\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_func=model_func,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for stage 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp_model(input_shape, split_size, meta_features_shape,):\n",
    "    X_input = layers.Input(input_shape)\n",
    "    \n",
    "    meta_input = layers.Input(meta_features_shape)\n",
    "    meta_out = layers.Dropout(0.1, seed = 7)(meta_input)\n",
    "    meta_out = layers.Dense(20, activation='selu')(meta_out)\n",
    "    \n",
    "    X = layers.Dropout(dropout_rate, seed = 7)(X_input)\n",
    "    X = layers.Dense(dense_layer_sizes[0], name = 'dense0')(X)\n",
    "    X = layers.Concatenate()([meta_out, X])\n",
    "    \n",
    "    X = layers.BatchNormalization(name = 'bn0')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.Dropout(dropout_rate, seed = 7)(X)\n",
    "\n",
    "    # Output layer\n",
    "    X = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n",
    "    X = layers.Activation('softmax')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(input = [X_input, meta_input], output = X, name = \"classif_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_tag,\n",
    "                dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "                oof_folder='oof/', pred_folder='outputs/'):\n",
    "\n",
    "    tag = tag + str(layer) # follow the original naming style\n",
    "    dev_filename = tag + dev_filename\n",
    "    val_filename = tag + val_filename\n",
    "    test_filename = tag + test_filename\n",
    "    \n",
    "    development = pd.read_json(os.path.join(dev_folder_path, dev_filename))\n",
    "    X_development, Y_development = parse_json(development, embedding_size)\n",
    "\n",
    "    remove_development = [row for row in range(len(X_development)) if np.sum(np.isnan(X_development[row].reshape(-1)))]\n",
    "    X_development[remove_development] = np.zeros((3 * embedding_size))\n",
    "\n",
    "    # Will predict probabilities for data from the gap-development file; initializing the predictions\n",
    "    prediction = np.zeros((len(X_development), 3)) # testing predictions\n",
    "    \n",
    "    for fold_n in range(n_fold):\n",
    "        # split training and validation data\n",
    "        print('Make predictions for', fold_n, 'started at', time.ctime())\n",
    "        # Define the model, re-initializing for each fold\n",
    "        classif_model = build_mlp_model([X_development.shape[-1]], split_size=embedding_size, meta_features_shape=[meta_features_test.shape[-1]])\n",
    "        classif_model.load_weights(os.path.join(checkpoint_path, model_tag + tag + str(fold_n) + '.pt'))\n",
    "\n",
    "        pred = classif_model.predict(x=[X_development, meta_features_test], verbose=0)\n",
    "        prediction += pred\n",
    "    \n",
    "    prediction /= n_fold\n",
    "    \n",
    "    submission = pd.read_csv(\"data/sample_submission_stage_1.csv\", index_col = \"ID\")\n",
    "    submission[\"A\"] = prediction[:,0]\n",
    "    submission[\"B\"] = prediction[:,1]\n",
    "    submission[\"NEITHER\"] = prediction[:,2]\n",
    "    submission.to_csv(pred_folder + model_tag + tag + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:22:12 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:22:17 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:22:22 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:22:27 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:22:32 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_base\"\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-base-uncased-seq512-\"\n",
    "pred_tag = \"Hnaive-\"\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:22:38 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:22:43 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:22:48 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:22:53 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:22:58 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_base_cased\"\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-base-cased-seq512-\"\n",
    "pred_tag = \"Hnaive-\"\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:23:04 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:23:09 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:23:14 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:23:19 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:23:24 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_big\"\n",
    "embedding_size = 1024\n",
    "layer = 19\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-large-uncased-seq300-\"\n",
    "pred_tag = \"Hnaive-\"\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 0 started at Mon Apr 15 14:23:30 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for 1 started at Mon Apr 15 14:23:35 2019\n",
      "Make predictions for 2 started at Mon Apr 15 14:23:40 2019\n",
      "Make predictions for 3 started at Mon Apr 15 14:23:46 2019\n",
      "Make predictions for 4 started at Mon Apr 15 14:23:51 2019\n"
     ]
    }
   ],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_big_cased\"\n",
    "embedding_size = 1024\n",
    "layer = 18\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-large-cased-seq300-\"\n",
    "pred_tag = \"Hnaive-\"\n",
    "\n",
    "make_predictions(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='test_outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predictons for stage 1.42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the predictions of base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_files = os.listdir('test_outputs') ## MAKE SURE THE ORDER IS CORRECT!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds shape (2000, 72)\n"
     ]
    }
   ],
   "source": [
    "preds = [pd.read_csv(os.path.join('test_outputs', pred_file)).values[:, 1:] for pred_file in pred_files]\n",
    "test_x = np.concatenate(preds, axis=1)\n",
    "print(\"preds shape\", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp_model(feature_nums):\n",
    "    features_inputs = Input(shape=feature_nums, name='mata-features', dtype=\"float32\")\n",
    "    features = features_inputs\n",
    "    \n",
    "    depth = 5\n",
    "    for i in range(depth):\n",
    "        new_features = Dense(24, activation='relu')(features)\n",
    "        new_features = Dropout(0.5)(new_features)\n",
    "        features = Concatenate()([features, new_features])\n",
    "\n",
    "    out_ = Dense(3, activation='softmax')(features)\n",
    "    \n",
    "    model = Model(inputs=[features_inputs], outputs=out_)\n",
    "    model.compile(optimizer=Adam(lr=1e-3, decay=1e-6,), loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_881 (Dense)               (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1161 (Dropout)          (None, 24)           0           dense_881[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_911 (Concatenate)   (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_1161[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_882 (Dense)               (None, 24)           2328        concatenate_911[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1162 (Dropout)          (None, 24)           0           dense_882[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_912 (Concatenate)   (None, 120)          0           concatenate_911[0][0]            \n",
      "                                                                 dropout_1162[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_883 (Dense)               (None, 24)           2904        concatenate_912[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1163 (Dropout)          (None, 24)           0           dense_883[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_913 (Concatenate)   (None, 144)          0           concatenate_912[0][0]            \n",
      "                                                                 dropout_1163[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_884 (Dense)               (None, 24)           3480        concatenate_913[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1164 (Dropout)          (None, 24)           0           dense_884[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_914 (Concatenate)   (None, 168)          0           concatenate_913[0][0]            \n",
      "                                                                 dropout_1164[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_885 (Dense)               (None, 24)           4056        concatenate_914[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1165 (Dropout)          (None, 24)           0           dense_885[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_915 (Concatenate)   (None, 192)          0           concatenate_914[0][0]            \n",
      "                                                                 dropout_1165[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_886 (Dense)               (None, 3)            579         concatenate_915[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_887 (Dense)               (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1166 (Dropout)          (None, 24)           0           dense_887[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_916 (Concatenate)   (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_1166[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_888 (Dense)               (None, 24)           2328        concatenate_916[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1167 (Dropout)          (None, 24)           0           dense_888[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_917 (Concatenate)   (None, 120)          0           concatenate_916[0][0]            \n",
      "                                                                 dropout_1167[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_889 (Dense)               (None, 24)           2904        concatenate_917[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1168 (Dropout)          (None, 24)           0           dense_889[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_918 (Concatenate)   (None, 144)          0           concatenate_917[0][0]            \n",
      "                                                                 dropout_1168[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_890 (Dense)               (None, 24)           3480        concatenate_918[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1169 (Dropout)          (None, 24)           0           dense_890[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_919 (Concatenate)   (None, 168)          0           concatenate_918[0][0]            \n",
      "                                                                 dropout_1169[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_891 (Dense)               (None, 24)           4056        concatenate_919[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1170 (Dropout)          (None, 24)           0           dense_891[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_920 (Concatenate)   (None, 192)          0           concatenate_919[0][0]            \n",
      "                                                                 dropout_1170[0][0]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "dense_892 (Dense)               (None, 3)            579         concatenate_920[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_893 (Dense)               (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1171 (Dropout)          (None, 24)           0           dense_893[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_921 (Concatenate)   (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_1171[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_894 (Dense)               (None, 24)           2328        concatenate_921[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1172 (Dropout)          (None, 24)           0           dense_894[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_922 (Concatenate)   (None, 120)          0           concatenate_921[0][0]            \n",
      "                                                                 dropout_1172[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_895 (Dense)               (None, 24)           2904        concatenate_922[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1173 (Dropout)          (None, 24)           0           dense_895[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_923 (Concatenate)   (None, 144)          0           concatenate_922[0][0]            \n",
      "                                                                 dropout_1173[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_896 (Dense)               (None, 24)           3480        concatenate_923[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1174 (Dropout)          (None, 24)           0           dense_896[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_924 (Concatenate)   (None, 168)          0           concatenate_923[0][0]            \n",
      "                                                                 dropout_1174[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_897 (Dense)               (None, 24)           4056        concatenate_924[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1175 (Dropout)          (None, 24)           0           dense_897[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_925 (Concatenate)   (None, 192)          0           concatenate_924[0][0]            \n",
      "                                                                 dropout_1175[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_898 (Dense)               (None, 3)            579         concatenate_925[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_899 (Dense)               (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1176 (Dropout)          (None, 24)           0           dense_899[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_926 (Concatenate)   (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_1176[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_900 (Dense)               (None, 24)           2328        concatenate_926[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1177 (Dropout)          (None, 24)           0           dense_900[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_927 (Concatenate)   (None, 120)          0           concatenate_926[0][0]            \n",
      "                                                                 dropout_1177[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_901 (Dense)               (None, 24)           2904        concatenate_927[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1178 (Dropout)          (None, 24)           0           dense_901[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_928 (Concatenate)   (None, 144)          0           concatenate_927[0][0]            \n",
      "                                                                 dropout_1178[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_902 (Dense)               (None, 24)           3480        concatenate_928[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1179 (Dropout)          (None, 24)           0           dense_902[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_929 (Concatenate)   (None, 168)          0           concatenate_928[0][0]            \n",
      "                                                                 dropout_1179[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_903 (Dense)               (None, 24)           4056        concatenate_929[0][0]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "dropout_1180 (Dropout)          (None, 24)           0           dense_903[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_930 (Concatenate)   (None, 192)          0           concatenate_929[0][0]            \n",
      "                                                                 dropout_1180[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_904 (Dense)               (None, 3)            579         concatenate_930[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mata-features (InputLayer)      (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_905 (Dense)               (None, 24)           1752        mata-features[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1181 (Dropout)          (None, 24)           0           dense_905[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_931 (Concatenate)   (None, 96)           0           mata-features[0][0]              \n",
      "                                                                 dropout_1181[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_906 (Dense)               (None, 24)           2328        concatenate_931[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1182 (Dropout)          (None, 24)           0           dense_906[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_932 (Concatenate)   (None, 120)          0           concatenate_931[0][0]            \n",
      "                                                                 dropout_1182[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_907 (Dense)               (None, 24)           2904        concatenate_932[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1183 (Dropout)          (None, 24)           0           dense_907[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_933 (Concatenate)   (None, 144)          0           concatenate_932[0][0]            \n",
      "                                                                 dropout_1183[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_908 (Dense)               (None, 24)           3480        concatenate_933[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1184 (Dropout)          (None, 24)           0           dense_908[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_934 (Concatenate)   (None, 168)          0           concatenate_933[0][0]            \n",
      "                                                                 dropout_1184[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_909 (Dense)               (None, 24)           4056        concatenate_934[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1185 (Dropout)          (None, 24)           0           dense_909[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_935 (Concatenate)   (None, 192)          0           concatenate_934[0][0]            \n",
      "                                                                 dropout_1185[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_910 (Dense)               (None, 3)            579         concatenate_935[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 15,099\n",
      "Trainable params: 15,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prediction = np.zeros((len(test_x), 3)) # testing predictions\n",
    "# Training and cross-validation\n",
    "\n",
    "for fold_n in range(n_fold):\n",
    "    classif_model = build_mlp_model([test_x.shape[-1]])\n",
    "    classif_model.load_weights('stage_1_finals/nn_ensemble_checkpoint_f{}.pt'.format(fold_n))\n",
    "    pred = classif_model.predict(x=test_x, verbose=0)\n",
    "    prediction += pred\n",
    "    \n",
    "prediction /= n_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a submission\n",
    "submission = pd.read_csv(\"data/sample_submission_stage_1.csv\", index_col = \"ID\")\n",
    "submission[\"A\"] = prediction[:,0]\n",
    "submission[\"B\"] = prediction[:,1]\n",
    "submission[\"NEITHER\"] = prediction[:,2]\n",
    "submission.to_csv(\"stage_1_finals/s3_24_nn_ensembles.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"stage_1_finals/24_nn_ensembles.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 0.030780994892120384\n",
      "1506 0.3294986367225647\n",
      "1988 0.0691684007644654\n"
     ]
    }
   ],
   "source": [
    "for i, (a, b) in enumerate(zip(submission[\"A\"].values, sub[\"A\"].values)):\n",
    "    if abs(a - b) > 0.0001:\n",
    "        print(i, abs(a-b)) # There are 3 missing values in dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
