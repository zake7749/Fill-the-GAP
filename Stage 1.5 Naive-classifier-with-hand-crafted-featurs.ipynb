{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission_stage_1.csv', 'test_stage_1.tsv', 'trees']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"data\"))\n",
    "import zipfile\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from keras import backend, models, layers, initializers, regularizers, constraints, optimizers\n",
    "from keras import callbacks as kc\n",
    "from keras import optimizers as ko\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import time\n",
    "\n",
    "\n",
    "dense_layer_sizes = [37]\n",
    "dropout_rate = 0.6\n",
    "learning_rate = 0.001\n",
    "n_fold = 5\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "patience = 100\n",
    "# n_test = 100\n",
    "lambd = 0.1 # L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The features are from this [awesome kernel](https://www.kaggle.com/pheell/look-ma-no-embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Load the GAP data\n",
    "\n",
    "data = pd.concat([pd.read_csv('gap-development.tsv', sep='\\t'),\n",
    "                  pd.read_csv('gap-validation.tsv', sep='\\t'),\n",
    "                  pd.read_csv('gap-test.tsv', sep='\\t')\n",
    "                 ], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def domain(t):\n",
    "    while not t._.subj and not t._.poss and\\\n",
    "            not (t.dep_ == 'xcomp' and t.head._.obj) and\\\n",
    "            t != t.head:\n",
    "        t = t.head\n",
    "    return t\n",
    "\n",
    "def ccom(t):\n",
    "    return [t2 for t2 in t.head._.d]\n",
    "\n",
    "# spacy extensions:\n",
    "#   doc._.to(offset) => t at text char offset\n",
    "#   t._.c => t's children (list)                     #   t._.d => t's descendents (list)\n",
    "#   t._.subj => t's subject else False               #   t._.obj => t's object else False\n",
    "#   t._.domain => t's syntactic domain               #   t._.ccom => t's c-command domain\n",
    "\n",
    "spacy.tokens.doc.Doc.set_extension(\n",
    "    'to', method=lambda doc, offset: [t for t in doc if t.idx == offset][0], force=True)\n",
    "spacy.tokens.token.Token.set_extension(\n",
    "    'c', getter=lambda t: [c for c in t.children], force=True)\n",
    "spacy.tokens.token.Token.set_extension(\n",
    "    'd', getter=lambda t: [c for c in t.sent if t in list(c.ancestors)], force=True)\n",
    "spacy.tokens.token.Token.set_extension(\n",
    "    'subj', getter=lambda t: ([c for c in t._.c if c.dep_.startswith('nsubj')] + [False])[0], force=True)\n",
    "spacy.tokens.token.Token.set_extension(\n",
    "    'obj', getter=lambda t: ([c for c in t._.c if c.dep_.startswith('dobj')] + [False])[0], force=True)\n",
    "spacy.tokens.token.Token.set_extension(\n",
    "    'poss', getter=lambda t: ([c for c in t._.c if c.dep_.startswith('poss')] + [False])[0], force=True)\n",
    "spacy.tokens.token.Token.set_extension(\n",
    "    'span', method=lambda t, t2: t.doc[t.i:t2.i] if t.i < t2.i else t.doc[t2.i:t.i], force=True)\n",
    "spacy.tokens.token.Token.set_extension('domain', getter=domain, force=True)\n",
    "spacy.tokens.token.Token.set_extension('ccom', getter=ccom, force=True)\n",
    "\n",
    "def applyDisq(condition, candidates, candidate_dict, debug = False):\n",
    "    badnames = sum([nameset(c, candidate_dict) for c in candidates if c in condition[0]], [])\n",
    "    badcands = [c for c in candidates if c.text in badnames]\n",
    "    if debug and len(badcands) > 0: print('Disqualified:', badcands, '<', condition[1])\n",
    "    return [c for c in candidates if c not in badcands]\n",
    "\n",
    "# Apply a list of disqualifying conditions\n",
    "def applyDisqs(conditions, candidates, candidate_dict, debug = False):\n",
    "    for condition in conditions:\n",
    "        if len(candidates) < 1: return candidates\n",
    "        candidates = applyDisq(condition, candidates, candidate_dict, debug)\n",
    "    return candidates\n",
    "\n",
    "# Pass the list of disqualifying conditions for possessive pronouns (his, her)\n",
    "def disqGen(t, candidates, candidate_dict, debug = False):\n",
    "    conds = [(t._.ccom,\n",
    "             \"disqualify candidates c-commanded by genpn; e.g. e.g. *Julia read his_i book about John_i's life.\"),\n",
    "             ([t2 for t2 in candidates if t in t2._.ccom and t2.head.dep_ == 'appos'],\n",
    "             \"disqualify candidates modified by an appositive with genpn; e.g. *I wanted to see John_i, his_i father.\")\n",
    "            ]\n",
    "    return applyDisqs(conds, candidates, candidate_dict, debug)\n",
    "\n",
    "# Pass the list of list of disqualifying conditions for other pronouns\n",
    "def disqOthers(t, candidates, candidate_dict, debug = False):\n",
    "    conds = [([t2 for t2 in t._.ccom if t2.i > t.i],\n",
    "             \"disqualify candidates c-commanded by pn, unless they were preposed;\\\n",
    "             e.g. *He_i cried before John_i laughed. vs. Before John_i laughed, he_i cried.\"),\n",
    "             ([t2 for t2 in candidates if t in t2._.ccom and t2._.domain == t._.domain\n",
    "              and not (t.head.text == 'with' and t.head.head.lemma_ == 'take')],\n",
    "             \"disqualify candidates that c-command pn, unless in different domain;\\\n",
    "             e.g. Mary said that *John_i hit him_i. vs. John_i said that Mary hit him_i;\\\n",
    "             random hard-coded exception: `take with'\"),\n",
    "             ([t2 for t2 in candidates if t2._.domain.dep_ == 'xcomp' and t2._.domain.head._.obj and t2 == t2._.domain.head._.obj],\n",
    "             \"for xcomps with subjects parsed as upstairs dobj, disallow coref with that dobj;\\\n",
    "             e.g. *Mary wanted John_i to forgive him_i.\")\n",
    "            ]\n",
    "    return applyDisqs(conds, candidates, candidate_dict, debug)\n",
    "\n",
    "# Decide whether possessive or not and call appropriate function\n",
    "def disq(t, candidates, candidate_dict, debug = False):\n",
    "    func = disqGen if t.dep_ == 'poss' else disqOthers\n",
    "    candidates = func(t, candidates, candidate_dict, debug)\n",
    "    return candidates\n",
    "\n",
    "def find_head(w, wo, doc):\n",
    "    t = False; backtrack = 0\n",
    "    while not t:\n",
    "        try:\n",
    "            t = doc._.to(wo)\n",
    "        except IndexError:\n",
    "            wo -= 1; backtrack += 1\n",
    "    while t.dep_ == 'compound' and t.head.idx >= wo and t.head.idx < len(w) + wo + backtrack: t = t.head\n",
    "    return t\n",
    "\n",
    "# Returns subsequences of a name\n",
    "def subnames(name):\n",
    "    if type(name) != str: name = candidate_dict[name]\n",
    "    parts = name.split(' ')\n",
    "    subnames_ = []\n",
    "    for i in range(len(parts)): \n",
    "        for j in range(i + 1, len(parts) + 1): \n",
    "            sub = ' '.join(parts[i:j])\n",
    "            if len(sub) > 2: subnames_.append(sub)\n",
    "    return subnames_\n",
    "\n",
    "# Returns subsequences of a name unless potentially ambiguous (if another candidate picks out same subsequence)\n",
    "def nameset(name, candidate_dict):\n",
    "    if type(name) != str: name = candidate_dict[name]\n",
    "    subnames_ = [sn for sn in subnames(name)]\n",
    "    return [c for c in subnames_ if c not in sum([subnames(c) for c in candidate_dict.values() \n",
    "                                                  if c not in subnames_ and name not in subnames(c)], [])]\n",
    "\n",
    "# Given the original candidate dict and the final candidate list, returns new dict grouping putative candidate instances under a single key\n",
    "def candInstances(candidates, candidate_dict):\n",
    "    candidates_by_name = {}\n",
    "    for c in sorted(candidates, key = lambda c: len(candidate_dict[c]), reverse = True):\n",
    "        name = candidate_dict[c]\n",
    "        for name2 in candidates_by_name.keys():\n",
    "            if name in nameset(name2, candidate_dict): name = name2; break\n",
    "        candidates_by_name[name] = candidates_by_name.get(name, []) + [c]\n",
    "    return candidates_by_name\n",
    "\n",
    "import gender_guesser.detector as gender # oops\n",
    "gd = gender.Detector()\n",
    "\n",
    "# Needed to prune candidate dict-- removes non-provided candidates that don't match in most common gender with pn\n",
    "def filterGender(candidates_by_name, a, b, pn):\n",
    "    badnames = []\n",
    "    gender = 'female' if pn in ['She', 'she', 'her', 'Her'] else 'male'\n",
    "    for name in candidates_by_name.keys():\n",
    "        if a in subnames(name) or b in subnames(name): continue\n",
    "        genderii = gd.get_gender(name.split(' ')[0])\n",
    "        if gender == 'male' and genderii == 'female': badnames += [name]; continue\n",
    "        if gender == 'female' and genderii == 'male': badnames += [name]; continue\n",
    "    for name in badnames: candidates_by_name.pop(name)\n",
    "    return candidates_by_name\n",
    "\n",
    "from urllib.parse import unquote\n",
    "import re\n",
    "\n",
    "# Authors' metric 1: Does the Wikipedia url contain the candidate's name?\n",
    "def urlMatch(a, b, url, candidate_dict):\n",
    "    url = re.sub('[^\\x00-\\x7F]', '*', unquote(url.split('/')[-1])).replace('_', ' ').lower()\n",
    "    return {'a_url': (sorted([len(n.split(' ')) for n in nameset(a.lower(), candidate_dict) if n in nameset(url, candidate_dict)], reverse = True) + [0])[0],\n",
    "            'b_url': (sorted([len(n.split(' ')) for n in nameset(b.lower(), candidate_dict) if n in nameset(url, candidate_dict)], reverse = True) + [0])[0]}\n",
    "\n",
    "# Authors' metric 2: When pn is subject or object, does the candidate match?\n",
    "def parallel(t1, t2):\n",
    "    if t1.dep_.startswith('nsubj'): return t2.dep_.startswith('nsubj')\n",
    "    if t1.dep_.startswith('dobj'): return t2.dep_.startswith('dobj')\n",
    "    if t1.dep_.startswith('dative'): return t2.dep_.startswith('dative')\n",
    "    return False\n",
    "\n",
    "# Depth from a node to a parent node\n",
    "def depthTo(t1, t2):\n",
    "    depth = 0\n",
    "    while t1 != t2 and t1 != t1.head:\n",
    "        t1 = t1.head\n",
    "        depth += 1\n",
    "    return depth\n",
    "\n",
    "# Syntactic distance within a single tree\n",
    "def nodeDist(t1, t2):\n",
    "    if t1 == t2: return 0\n",
    "    if t2 in t1._.d: return depthTo(t2, t1)\n",
    "    if t1 in t2._.d: return depthTo(t1, t2)\n",
    "    t = t1\n",
    "    while t1 not in t._.d or t2 not in t._.d and t != t.head: t = t.head\n",
    "    return depthTo(t1, t) + depthTo(t2, t)\n",
    "\n",
    "# Authors' metric 3: Syntactic distance (within or across trees)\n",
    "def synDist(t, pn, doc, debug = False):\n",
    "    doc_sents = list(doc.sents)\n",
    "    sspan = doc_sents.index(pn.sent) - doc_sents.index(t.sent)\n",
    "    if sspan == 0: # same sentence\n",
    "        dist = nodeDist(t, pn)\n",
    "    else: # different sentence\n",
    "        dist = nodeDist(pn, doc_sents[doc_sents.index(pn.sent)].root) + nodeDist(t, doc_sents[doc_sents.index(t.sent)].root) # dist from two roots\n",
    "    if debug: \n",
    "        print('pn dist:', nodeDist(pn, doc_sents[doc_sents.index(pn.sent)].root), '; t dist:',\n",
    "              nodeDist(t, doc_sents[doc_sents.index(t.sent)].root), '; span:', sspan)\n",
    "    sspan = abs(sspan) * 1 if sspan >= 0 else abs(sspan) * 1.3 # less local if not preceding\n",
    "    return dist + sspan# * 0.7\n",
    "\n",
    "# Character distance\n",
    "def charDist(t1, t2):\n",
    "    if t2.idx > t1.idx:\n",
    "        return t2.idx - t1.idx + len(t1.text)\n",
    "    else:\n",
    "        return (t1.idx - t2.idx + len(t2.text)) * 1.3\n",
    "\n",
    "# Theta prominence: assign a 0.1 to 1 score based on dep role of candidate -- strong feature\n",
    "def thetaProminence(t, mult = 1, debug = False):\n",
    "    while t.dep_ == 'compound': t = t.head\n",
    "    if debug: print('t dep_:', t.dep_)\n",
    "    if t.dep_ == 'pobj': mult = 1.3 if t.head.i < t.head.head.i else 1\n",
    "    if t._.domain.dep_ == 'advcl': mult = 1.3 if t.head.i < t._.domain.head.i else 1\n",
    "    if t.dep_.startswith('nsubj'): score = 1\n",
    "    elif t.dep_.startswith('dobj'): score = 0.8\n",
    "    elif t.dep_.startswith('dative'): score = 0.6\n",
    "    elif t.dep_.startswith('pobj'): score = 0.4\n",
    "    elif t.dep_.startswith('poss'): score = 0.3\n",
    "    else: score = 0.1\n",
    "    if debug: print('mult:', mult, '; score:', score)\n",
    "    return min(1, score * mult)\n",
    "\n",
    "# Computes these metrics for each candidate, and returns, for each group of instances (A instances, B instances,\\\n",
    "# other instances), either the sum, or the highest difference from the mean\n",
    "def score(label, candidates_by_name, a_cand, b_cand, func, minsc = None, method = 'sum'):\n",
    "    if method == 'sum':\n",
    "        scores = {name: sum([func(t) for t in tokens]) for name, tokens in candidates_by_name.items()}\n",
    "    elif method == 'meandiff':\n",
    "        mean = np.mean(sum([[func(t) for t in tokens] for tokens in candidates_by_name.values()], []))\n",
    "        scores = {name: mean - min([func(t) for t in tokens]) for name, tokens in candidates_by_name.items()}\n",
    "    sca = scores[a_cand] if a_cand else minsc\n",
    "    scb = scores[b_cand] if b_cand else minsc\n",
    "    screst = [v for n, v in scores.items() if n != a_cand and n != b_cand]\n",
    "    if method == 'sum':\n",
    "        screst = sum(screst) if len(screst) > 0 else minsc\n",
    "    elif method == 'meandiff':\n",
    "        screst = max(screst) if len(screst) > 0 else minsc\n",
    "    return {'a_' + label: sca, 'b_' + label: scb, 'n_' + label: screst}\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Load a rowfull of data\n",
    "def load_row(data, i):\n",
    "    return tuple(data.iloc[i])\n",
    "\n",
    "# Row by row, populate features\n",
    "def annotateSet(data, minsc = None, debug = False):\n",
    "    \n",
    "    annotated_data = pd.DataFrame() # init placeholder df\n",
    "    row_batch = []\n",
    "\n",
    "    for i in tqdm(range(annotated_data.shape[0], data.shape[0])):\n",
    "        id, text, pn, pno, a, ao, ag, b, bo, bg, url = load_row(data, i)        \n",
    "        doc = nlp(text) # parse text into doc\n",
    "        pnt, at, bt = (doc._.to(pno), find_head(a, ao, doc), find_head(b, bo, doc)) # get the tokens that correspond to offsets\n",
    "        candidate_dict = {e.root: re.sub('\\'s$', '', e.text) for e in [e for e in doc.ents if e.root.ent_type_ == 'PERSON']} # first get every PERSON ent as candidate\n",
    "        candidate_dict.update({c.root: re.sub('\\'s$', '', c.text) for c in doc.noun_chunks if c.root.pos_ == 'PROPN' and c.text in sum([subnames(n) for n in candidate_dict.values()], []) and\n",
    "                               c.root not in candidate_dict.keys()}) # get some missed ones by looking at noun chunks with PROPN roots whose text match part of a candidate but are not already in list\n",
    "        candidate_dict.update({t: w for t, w in [(at, a), (bt, b)]}) # add provided cands, overwriting in the process\n",
    "\n",
    "        candidates = disq(pnt, list(candidate_dict.keys()), candidate_dict, debug = False)\n",
    "        candidates_by_name = candInstances(candidates, candidate_dict)\n",
    "        candidates_by_name = filterGender(candidates_by_name, a, b, pn)\n",
    "        a_cand = ([name for name, tokens in candidates_by_name.items() if at in tokens] + [False])[0]\n",
    "        b_cand = ([name for name, tokens in candidates_by_name.items() if bt in tokens] + [False])[0]\n",
    "    \n",
    "        # init row dict\n",
    "        features = {'id': id, 'label': 0 if ag else 1 if bg else 2}\n",
    "        # eliminated or not\n",
    "        features.update({'a_out': 0 if a_cand else 1, 'b_out': 0 if b_cand else 1})\n",
    "        # url match or not\n",
    "        features.update(urlMatch(a, b, url, candidate_dict))\n",
    "        # c-command or not\n",
    "        features.update({'a_cc': 1 if a_cand and pnt in at._.ccom else 0, 'b_cc': 1 if b_cand and pnt in bt._.ccom else 0})\n",
    "        # parallelism score\n",
    "        features.update(score('par', candidates_by_name, a_cand, b_cand, lambda t: parallel(t, pnt), minsc = minsc))\n",
    "        # theta prominence score\n",
    "        features.update(score('th', candidates_by_name, a_cand, b_cand, thetaProminence, minsc = minsc))\n",
    "        # syntactic distance score\n",
    "        features.update(score('loc', candidates_by_name, a_cand, b_cand, lambda t: synDist(t, pnt, doc), method='meandiff', minsc = minsc))\n",
    "        # number of candidates left\n",
    "        features.update({'n_cands': len(candidates_by_name)})\n",
    "        # char dist\n",
    "        features.update(score('cloc', candidates_by_name, a_cand, b_cand, lambda t: charDist(t, pnt), method='meandiff', minsc = minsc))\n",
    "\n",
    "        if debug: print(id, '>', 'a:', 1 if ag else 0, 'b:', 1 if bg else 0, features)\n",
    "        row_batch += [features]\n",
    "\n",
    "    # add rows to placeholder df\n",
    "    if annotated_data.shape[0] != data.shape[0]: annotated_data = annotated_data.append(row_batch, ignore_index = True)\n",
    "    \n",
    "    return annotated_data\n",
    "\n",
    "# Readable rows\n",
    "\n",
    "from textwrap import TextWrapper\n",
    "wrapper = TextWrapper(width=75)\n",
    "\n",
    "def style(w, wstyle):\n",
    "    if wstyle == None: return '{}{}{}'.format('\\033[1m', w, '\\033[0m') # bold\n",
    "    elif wstyle == True: return '{}{}{}'.format('\\033[92m', w, '\\033[0m') # green\n",
    "    elif wstyle == False: return '{}{}{}'.format('\\033[91m', w, '\\033[0m') # red\n",
    "\n",
    "def readable_rows(data, range):\n",
    "    rows = []\n",
    "    for i in range:\n",
    "        wois = sorted([(data.iloc[i]['Pronoun'], data.iloc[i]['Pronoun-offset'], None),\\\n",
    "                   (data.iloc[i]['A'], data.iloc[i]['A-offset'], data.iloc[i]['A-coref']),\\\n",
    "                   (data.iloc[i]['B'], data.iloc[i]['B-offset'], data.iloc[i]['B-coref'])],\\\n",
    "                   key = lambda x: x[1]) # sort by offset\n",
    "        text = ''; ftext = data.iloc[i]['Text']; coffset = 0\n",
    "        for w, woffset, wstyle in wois:\n",
    "            text += ftext[coffset:woffset] + style(w, wstyle)\n",
    "            coffset += len(ftext[coffset:woffset]) + len(w)\n",
    "        text += ftext[coffset:]\n",
    "        rows += [(str(i), data.iloc[i]['ID'], 'A' if data.iloc[i]['A-coref'] else 'B' if data.iloc[i]['B-coref'] else 'N', text)]\n",
    "    return rows\n",
    "\n",
    "def print_readable_rows(data, range):\n",
    "    for index, id, target, text in readable_rows(data, range):\n",
    "        text = '\\n\\t'.join(wrapper.wrap(text = text))\n",
    "        print('{} ({}): {}>\\n\\t{}\\n'.format(id, index, target, text))\n",
    "        \n",
    "def print_tokens(doc):\n",
    "    for i in range(0, len(doc), 3):\n",
    "        for t in doc[i:i+3]:\n",
    "            print(\"{}[{}] >{}> {}\".format(t.text, t.pos_, t.dep_, t.head.text), end = ' | ')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e56cf117be4be49690b82cae426a95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "annotated_data = annotateSet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As wrongly disqualified: 57 , Bs wrongly disqualified: 94 \n",
      "\n",
      "development-155 (154): A>\n",
      "\tThe official first single, ``Fallin''', was written solely by Keys, and\n",
      "\ttopped the US Billboard Hot 100 chart. ``A Woman's Worth'', written by\n",
      "\tKeys and Erika Rose, is a ``jazz-tinged'' song with\n",
      "\tlyrics which speak of how men should treat and respect women. Keys released\n",
      "\ther second album, The Diary of Alicia Keys, in December 2003.\n",
      "\n",
      "The[DET] >det> single | official[ADJ] >amod> single | first[ADJ] >advmod> single | \n",
      "single[ADJ] >nsubjpass> written | ,[PUNCT] >punct> single | ``[PUNCT] >punct> single | \n",
      "Fallin[PROPN] >appos> single | '[PUNCT] >punct> Fallin | ''[PUNCT] >punct> single | \n",
      ",[PUNCT] >punct> written | was[VERB] >auxpass> written | written[VERB] >ROOT> written | \n",
      "solely[ADV] >advmod> written | by[ADP] >agent> written | Keys[NOUN] >pobj> by | \n",
      ",[PUNCT] >punct> written | and[CCONJ] >cc> written | topped[VERB] >conj> written | \n",
      "the[DET] >det> chart | US[PROPN] >compound> chart | Billboard[PROPN] >nmod> chart | \n",
      "Hot[PROPN] >nmod> chart | 100[NUM] >nummod> Hot | chart[NOUN] >dobj> topped | \n",
      ".[PUNCT] >punct> written | ``[PUNCT] >punct> Worth | A[DET] >det> Woman | \n",
      "Woman[NOUN] >poss> Worth | 's[PART] >case> Woman | Worth[PROPN] >nsubj> is | \n",
      "''[PUNCT] >punct> Worth | ,[PUNCT] >punct> Worth | written[VERB] >acl> Worth | \n",
      "by[ADP] >agent> written | Keys[PROPN] >pobj> by | and[CCONJ] >cc> Keys | \n",
      "Erika[PROPN] >compound> Rose | Rose[PROPN] >conj> Keys | ,[PUNCT] >punct> is | \n",
      "is[VERB] >ROOT> is | a[DET] >det> song | ``[PUNCT] >punct> song | \n",
      "jazz[NOUN] >npadvmod> tinged | -[PUNCT] >punct> tinged | tinged[VERB] >amod> song | \n",
      "''[PUNCT] >punct> song | song[NOUN] >attr> is | with[ADP] >prep> song | \n",
      "lyrics[NOUN] >pobj> with | which[ADJ] >nsubj> speak | speak[VERB] >relcl> song | \n",
      "of[ADP] >prep> speak | how[ADV] >advmod> treat | men[NOUN] >nsubj> treat | \n",
      "should[VERB] >aux> treat | treat[VERB] >pcomp> of | and[CCONJ] >cc> treat | \n",
      "respect[VERB] >conj> treat | women[NOUN] >dobj> respect | .[PUNCT] >punct> is | \n",
      "Keys[NOUN] >nsubj> released | released[VERB] >ROOT> released | her[ADJ] >poss> album | \n",
      "second[ADJ] >amod> album | album[NOUN] >dobj> released | ,[PUNCT] >punct> album | \n",
      "The[DET] >det> Diary | Diary[PROPN] >appos> album | of[ADP] >prep> Diary | \n",
      "Alicia[PROPN] >compound> Keys | Keys[PROPN] >pobj> of | ,[PUNCT] >punct> album | \n",
      "in[ADP] >prep> released | December[PROPN] >pobj> in | 2003[NUM] >nummod> December | \n",
      ".[PUNCT] >punct> released | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "misdisqs_a = annotated_data.loc[(annotated_data['a_out'] == 1) & (annotated_data['label'] == 0)]\n",
    "misdisqs_b = annotated_data.loc[(annotated_data['b_out'] == 1) & (annotated_data['label'] == 1)]\n",
    "\n",
    "print('As wrongly disqualified:', misdisqs_a.shape[0], ', Bs wrongly disqualified:', misdisqs_b.shape[0], '\\n')\n",
    "\n",
    "for i in misdisqs_a.index[:1]: # the first\n",
    "    print_readable_rows(data, [i])\n",
    "    id, text, pn, pno, a, ao, ag, b, bo, bg, url = load_row(data,i)\n",
    "    doc = nlp(text); print_tokens(doc)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25e9334d0b8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEVCAYAAAAM3jVmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = annotated_data.loc[(annotated_data['id'].str.contains('test'))] # swap later\n",
    "valid = annotated_data.loc[(annotated_data['id'].str.contains('validation'))]\n",
    "train_valid = pd.concat([train, valid])\n",
    "test = annotated_data.loc[(annotated_data['id'].str.contains('development'))] # swap later\n",
    "\n",
    "answer_columns = ['label']\n",
    "excl = ['id']\n",
    "excl += ['a_out', 'b_out']\n",
    "feature_columns = [col for col in annotated_data.columns if col not in answer_columns and col not in excl]\n",
    "\n",
    "meta_features_train_valid = train_valid[feature_columns].fillna(0)\n",
    "meta_features_test = test[feature_columns].fillna(0)\n",
    "\n",
    "sns.heatmap(meta_features_train_valid.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "for col in meta_features_train_valid.columns:\n",
    "    std = MinMaxScaler()\n",
    "    meta_features_train_valid[col] = std.fit_transform(meta_features_train_valid[col].values.reshape(-1, 1))\n",
    "    meta_features_test[col] = std.transform(meta_features_test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep all BERTs learn on the same corpus\n",
    "remove_test = []\n",
    "remove_validation = []\n",
    "remove_development = [209, 1506, 1988]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_features_train_valid = meta_features_train_valid.values\n",
    "meta_features_test = meta_features_test.values # These should be fix on the stage 2 because they will use to train\n",
    "\n",
    "meta_features_train_valid = np.delete(meta_features_train_valid, remove_test, 0)\n",
    "meta_features_train_valid = np.delete(meta_features_train_valid, [v + len(train) for v in remove_validation], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_features_train_valid shpae: (2454, 17)\n",
      "meta_features_test shpae: (2000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"meta_features_train_valid shpae:\", meta_features_train_valid.shape)\n",
    "print(\"meta_features_test shpae:\", meta_features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLI Acrhitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp_model(input_shape, split_size, meta_features_shape,):\n",
    "    X_input = layers.Input(input_shape)\n",
    "    \n",
    "    meta_input = layers.Input(meta_features_shape)\n",
    "    meta_out = layers.Dropout(0.1, seed = 7)(meta_input)\n",
    "    meta_out = layers.Dense(20, activation='selu')(meta_out)\n",
    "    \n",
    "    X = layers.Dropout(dropout_rate, seed = 7)(X_input)\n",
    "    X = layers.Dense(dense_layer_sizes[0], name = 'dense0')(X)\n",
    "    X = layers.Concatenate()([meta_out, X])\n",
    "    \n",
    "    X = layers.BatchNormalization(name = 'bn0')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "    X = layers.Dropout(dropout_rate, seed = 7)(X)\n",
    "\n",
    "    # Output layer\n",
    "    X = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n",
    "    X = layers.Activation('softmax')(X)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(input = [X_input, meta_input], output = X, name = \"classif_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_filename = \"contextual_embeddings_gap_development.json\"\n",
    "val_filename = \"contextual_embeddings_gap_validation.json\"\n",
    "test_filename = \"contextual_embeddings_gap_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_json(embeddings, embedding_size):\n",
    "    '''\n",
    "    Parses the embeddigns given by BERT, and suitably formats them to be passed to the MLP model\n",
    "\n",
    "    Input: embeddings, a DataFrame containing contextual embeddings from BERT, as well as the labels for the classification problem\n",
    "    columns: \"emb_A\": contextual embedding for the word A\n",
    "             \"emb_B\": contextual embedding for the word B\n",
    "             \"emb_P\": contextual embedding for the pronoun\n",
    "             \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n",
    "\n",
    "    Output: X, a numpy array containing, for each line in the GAP file, the concatenation of the embeddings of the target words\n",
    "            Y, a numpy array containing, for each line in the GAP file, the one-hot encoded answer to the coreference problem\n",
    "    '''\n",
    "    embeddings.sort_index(inplace = True) # Sorting the DataFrame, because reading from the json file messed with the order\n",
    "    X = np.zeros((len(embeddings),3* embedding_size))\n",
    "    Y = np.zeros((len(embeddings), 3))\n",
    "\n",
    "    # Concatenate features\n",
    "    for i in range(len(embeddings)):\n",
    "        A = np.array(embeddings.loc[i,\"emb_A\"])\n",
    "        B = np.array(embeddings.loc[i,\"emb_B\"])\n",
    "        P = np.array(embeddings.loc[i,\"emb_P\"])\n",
    "        X[i] = np.concatenate((A,B,P))\n",
    "\n",
    "    # One-hot encoding for labels\n",
    "    for i in range(len(embeddings)):\n",
    "        label = embeddings.loc[i,\"label\"]\n",
    "        if label == \"A\":\n",
    "            Y[i,0] = 1\n",
    "        elif label == \"B\":\n",
    "            Y[i,1] = 1\n",
    "        else:\n",
    "            Y[i,2] = 1\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold, model_tag,\n",
    "                dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "                oof_folder='oof/', pred_folder='outputs/'):\n",
    "    '''\n",
    "    Arguments:\n",
    "        tag (data_tag): the tag of model and vec\n",
    "        embedding_size: the size of bert embedding\n",
    "        layer: to indicate which layer of bert is going to use\n",
    "        checkpoint_path: the path to model_checkpoint folder\n",
    "        n_fold: the number of CV folds\n",
    "        model_tag: the prefix of the prediction and oof file and model chekpoint\n",
    "    '''\n",
    "\n",
    "    tag = tag + str(layer) # follow the original naming style\n",
    "    dev_filename = tag + dev_filename\n",
    "    val_filename = tag + val_filename\n",
    "    test_filename = tag + test_filename\n",
    "    \n",
    "    development = pd.read_json(os.path.join(dev_folder_path, dev_filename))\n",
    "    X_development, Y_development = parse_json(development, embedding_size)\n",
    "\n",
    "    validation = pd.read_json(os.path.join(val_folder_path, val_filename))\n",
    "    X_validation, Y_validation = parse_json(validation, embedding_size)\n",
    "\n",
    "    test = pd.read_json(os.path.join(test_folder_path, test_filename))\n",
    "    X_test, Y_test = parse_json(test, embedding_size)\n",
    "\n",
    "    # There may be a few NaN values, where the offset of a target word is greater than the max_seq_length of BERT.\n",
    "    # They are very few, so I'm just dropping the rows.\n",
    "    # remove_test = [row for row in range(len(X_test)) if np.sum(np.isnan(X_test[row].reshape(-1)))]\n",
    "    X_test = np.delete(X_test, remove_test, 0)\n",
    "    Y_test = np.delete(Y_test, remove_test, 0)\n",
    "\n",
    "    # remove_validation = [row for row in range(len(X_validation)) if np.sum(np.isnan(X_validation[row].reshape(-1)))]\n",
    "    X_validation = np.delete(X_validation, remove_validation, 0)\n",
    "    Y_validation = np.delete(Y_validation, remove_validation, 0)\n",
    "\n",
    "    # We want predictions for all development rows. So instead of removing rows, make them 0\n",
    "    # remove_development = [row for row in range(len(X_development)) if np.sum(np.isnan(X_development[row].reshape(-1)))]\n",
    "    X_development[remove_development] = np.zeros((3 * embedding_size))\n",
    "\n",
    "    # Will train on data from the gap-test and gap-validation files, in total 2454 rows\n",
    "    X_train = np.concatenate((X_test, X_validation), axis = 0)\n",
    "    Y_train = np.concatenate((Y_test, Y_validation), axis = 0)\n",
    "\n",
    "    # Will predict probabilities for data from the gap-development file; initializing the predictions\n",
    "    prediction = np.zeros((len(X_development), 3)) # testing predictions\n",
    "\n",
    "    # Training and cross-validation\n",
    "    folds = KFold(n_splits=n_fold, shuffle=True, random_state=3)\n",
    "    scores = []\n",
    "    oof = np.zeros_like(Y_train)\n",
    "    \n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    \n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train)):\n",
    "        # split training and validation data\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_tr, X_val = X_train[train_index], X_train[valid_index]\n",
    "        meta_X_tr, meta_X_val = meta_features_train_valid[train_index], meta_features_train_valid[valid_index]\n",
    "        \n",
    "        Y_tr, Y_val = Y_train[train_index], Y_train[valid_index]\n",
    "        # Define the model, re-initializing for each fold\n",
    "        classif_model = build_mlp_model([X_train.shape[-1]], split_size=embedding_size, meta_features_shape=[meta_X_tr.shape[-1]])\n",
    "        classif_model.compile(optimizer=optimizers.Adam(lr=learning_rate), loss=\"categorical_crossentropy\")\n",
    "        \n",
    "        callbacks = [kc.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True),\n",
    "                     kc.ModelCheckpoint(os.path.join(checkpoint_path, model_tag + tag + str(fold_n) + '.pt'), monitor='val_loss', verbose=0, save_best_only=True, mode='min')]\n",
    "\n",
    "        # train the model\n",
    "        classif_model.fit(x=[X_tr, meta_X_tr], y=Y_tr, epochs=epochs, batch_size=batch_size, \n",
    "                          callbacks=callbacks, validation_data=([X_val, meta_X_val], Y_val), verbose=0)\n",
    "\n",
    "        # make predictions on validation and test data\n",
    "        pred_valid = classif_model.predict(x=[X_val, meta_X_val], verbose=0)\n",
    "        oof[valid_index] = pred_valid\n",
    "        pred = classif_model.predict(x=[X_development, meta_features_test], verbose=0)\n",
    "\n",
    "        # oof[valid_index] = pred_valid.reshape(-1,)\n",
    "        scores.append(log_loss(Y_val, pred_valid))\n",
    "        prediction += pred\n",
    "    \n",
    "    prediction /= n_fold\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"For the model\", tag)\n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    print(scores)\n",
    "    print(\"Test score:\", log_loss(Y_development, prediction)) # this line should be removed in the stage 2\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Write the prediction to file for submission\n",
    "    oof_df = pd.DataFrame(oof)\n",
    "    oof_df.to_csv(oof_folder + model_tag + tag + \".csv\", index=False)\n",
    "    \n",
    "    submission = pd.read_csv(\"data/sample_submission_stage_1.csv\", index_col = \"ID\")\n",
    "    submission[\"A\"] = prediction[:,0]\n",
    "    submission[\"B\"] = prediction[:,1]\n",
    "    submission[\"NEITHER\"] = prediction[:,2]\n",
    "    submission.to_csv(pred_folder + model_tag + tag + \".csv\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Base\n",
    "## Uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_base\"\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-base-uncased-seq512-\"\n",
    "pred_tag = \"Hnaive-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2454, 2304)\n",
      "Fold 0 started at Mon Apr 15 09:33:38 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Apr 15 09:34:26 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 started at Mon Apr 15 09:35:19 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 started at Mon Apr 15 09:35:59 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 started at Mon Apr 15 09:36:47 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "For the model bert-base-uncased-seq512-8\n",
      "CV mean score: 0.4674, std: 0.0311.\n",
      "[0.4331719821568293, 0.42636951553322805, 0.4933290876179759, 0.49951852219364923, 0.48437221295554284]\n",
      "Test score: 0.428411097599385\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_base_cased\"\n",
    "embedding_size = 768\n",
    "layer = 8\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-base-cased-seq512-\"\n",
    "pred_tag = \"Hnaive-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2454, 2304)\n",
      "Fold 0 started at Mon Apr 15 09:37:30 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Apr 15 09:38:15 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 started at Mon Apr 15 09:38:48 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 started at Mon Apr 15 09:39:41 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 started at Mon Apr 15 09:40:12 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "For the model bert-base-cased-seq512-8\n",
      "CV mean score: 0.5077, std: 0.0325.\n",
      "[0.48659037624921103, 0.46610562716394494, 0.5156144790323888, 0.5628816577328024, 0.5075072373941242]\n",
      "Test score: 0.4839722142997002\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Big\n",
    "## Uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_big\"\n",
    "embedding_size = 1024\n",
    "layer = 19\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-large-uncased-seq300-\"\n",
    "pred_tag = \"Hnaive-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2454, 3072)\n",
      "Fold 0 started at Mon Apr 15 09:41:08 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Apr 15 09:42:02 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 started at Mon Apr 15 09:42:57 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 started at Mon Apr 15 09:44:20 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 started at Mon Apr 15 09:44:57 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "For the model bert-large-uncased-seq300-19\n",
      "CV mean score: 0.4067, std: 0.0394.\n",
      "[0.35319187002941843, 0.3734994804380057, 0.4649440432662817, 0.4239922687333659, 0.41775769314690664]\n",
      "Test score: 0.36896653590320505\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='outputs/', model_tag=pred_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_folder_path = val_folder_path = test_folder_path = \"vector/bert_big_cased\"\n",
    "embedding_size = 1024\n",
    "layer = 18\n",
    "checkpoint_path = \"stage_1_checkpoints/\"\n",
    "n_fold = 5\n",
    "tag = \"bert-large-cased-seq300-\"\n",
    "pred_tag = \"Hnaive-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2454, 3072)\n",
      "Fold 0 started at Mon Apr 15 09:45:54 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Apr 15 09:46:57 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 started at Mon Apr 15 09:47:44 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 started at Mon Apr 15 09:48:23 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 started at Mon Apr 15 09:49:09 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zake7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "For the model bert-large-cased-seq300-18\n",
      "CV mean score: 0.4452, std: 0.0314.\n",
      "[0.4181910479437017, 0.4019797687281123, 0.4885887050195768, 0.4652644085081137, 0.45204981819403417]\n",
      "Test score: 0.4169767613248104\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_folds(tag, dev_folder_path, val_folder_path, test_folder_path, embedding_size, layer, checkpoint_path, n_fold,\n",
    "            dev_filename=dev_filename, val_filename=val_filename, test_filename=test_filename,\n",
    "            oof_folder='oof/', pred_folder='outputs/', model_tag=pred_tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
